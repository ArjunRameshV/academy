{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ray RLlib - Overview\n",
    "\n",
    "Â© 2019-2020, Anyscale. All Rights Reserved\n",
    "\n",
    "![Anyscale Academy](../images/AnyscaleAcademy_Logo_clearbanner_141x100.png)\n",
    "\n",
    "This tutorial, part of [Anyscale Academy](https://anyscale.com/academy), introduces the broad topic of _reinforcement learning_ (RL) and [RLlib](https://ray.readthedocs.io/en/latest/rllib.html), Ray's comprehensive RL library.\n",
    "\n",
    "The lessons in this tutorial use different _environments_ from [OpenAI Gym](https://gym.openai.com/) to illustrate how to train _policies_.\n",
    "\n",
    "See the instructions in the [README](../README.md) for setting up your environment to use this tutorial.\n",
    "\n",
    "Go [here](../Overview.ipynb) for an overview of all tutorials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial Sections\n",
    "\n",
    "Because of the breadth of RL this tutorial is divided into several segments. See below for a recommended _learning plan_.\n",
    "\n",
    "### Core Reinforcement Learning\n",
    "\n",
    "|    | Lesson | Description |\n",
    "| :- | :----- | :---------- |\n",
    "| 00 | [Ray RLlib Overview](00-Ray-RLlib-Overview.iypnb) | Overview of this tutorial. |\n",
    "| 01 | [Introduction to Reinforcement Learning](01-Introduction-to-Reinforcement-Learning.ipynb) | A quick introduction to the concepts of reinforcement learning. You can skim or skip this lesson if you already understand RL concepts. |\n",
    "| 02 | [Introduction to RLlib](02-Introduction-to-RLlib.ipynb) | An overview of RLlib, its goals and the capabilities it provides. |\n",
    "| 03 | [Application - Cart Pole](03-Application-Cart-Pole.ipynb) | The best starting place for learning how to use RL, in this case to train a moving car to balance a vertical pole. Based on the `CartPole-v0` environment from OpenAI Gym, combined with RLlib. |\n",
    "| 04 | [Application: Bipedal Walker](04-Bipedal-Walker.ipynb) | Train a two-legged robot simulator. This is an optional lesson, due to the longer compute times required, but fun to try. |\n",
    "| 05 | [Custom Environments and Reward Shaping](05-Custom-Environments-Reward-Shaping.ipynb) | How to customize environments and rewards for your applications. |\n",
    "| 06 | [Online Learning with DQN](06-Online-Learning-with-DQN.ipynb) | How to set up a server that simultaneously serves and learns a policy. |\n",
    "| 07 | [RL References](07-RL-References.ipynb) | References on reinforcement learning. |\n",
    "\n",
    "Some additional examples you might explore can be found in the `extras` folder:\n",
    "\n",
    "| Lesson | Description |\n",
    "| :----- | :---------- |\n",
    "| [Extra: Application - Mountain Car](extras/Extra-Application-Mountain-Car.ipynb) | Based on the `MountainCar-v0` environment from OpenAI Gym. |\n",
    "| [Extra: Application - Taxi](extras/Extra-Application-Taxi.ipynb) | Based on the `Taxi-v3` environment from OpenAI Gym. |\n",
    "| [Extra: Application - Frozen Lake](extras/Extra-Application-Frozen-Lake.ipynb) | Based on the `FrozenLake-v0` environment from OpenAI Gym. |\n",
    "\n",
    "In addition, exercise solutions for this tutorial can be found [here](solutions/Ray-RLlib-Solutions.ipynb).\n",
    "\n",
    "For earlier versions of some of these tutorials, see [`rllib_exercises`](https://github.com/ray-project/tutorial/blob/master/rllib_exercises/rllib_colab.ipynb) in the original [github.com/ray-project/tutorial](https://github.com/ray-project/tutorial) project.\n",
    "\n",
    "### Multi-Armed Bandits\n",
    "\n",
    "_Multi-Armed Bandits_ (MABs) are a special kind of RL problem that have broad and growing applications. The term is inspired by the slot machines in casinos, so called _one-armed bandits_, but where a machine might have more than one arm. \n",
    "\n",
    "|    | Lesson | Description |\n",
    "| :- | :----- | :---------- |\n",
    "| 00 | [Multi-Armed-Bandits Overview](multi-armed-bandits/00-Multi-Armed-Bandits-Overview.iypnb) | Overview of this set of lessons. |\n",
    "| 01 | [Introduction to Multi-Armed Bandits](multi-armed-bandits/01-Introduction-to-Multi-Armed-Bandits.ipynb) | A quick introduction to the concepts of multi-armeed bandits (MABs) and how they fit in the spectrum of RL problems. |\n",
    "| 02 | [Exploration vs. Exploitation Strategies](multi-armed-bandits/02-Exploration-vs-Exploitation-Strategies.iypnb) | A deeper look at algorithms that balance exploration vs. exploitation, the key challenge for efficient solutions. Much of this material is technical and can be skipped in a first reading, but skim the first part of this lesson at least. |\n",
    "| 03 | [Simple Multi-Armed Bandit](multi-armed-bandits/03-Simple-Multi-Armed-Bandit.ipynb) | A simple example of a multi-armed bandit to illustrate the core ideas. |\n",
    "| 04 | [Linear Upper Confidence Bound](multi-armed-bandits/04-Linear-Upper-Confidence-Bound.ipynb) | One popular algorithm for exploration vs. exploitation is _Upper Confidence Bound_. This lesson shows how to use a linear version in RLlib. |\n",
    "| 05 | [Linear Thompson Sampling](multi-armed-bandits/05-Linear-Thompson-Sampling.ipynb) | Another popular algorithm for exploration vs. exploitation is _Thompson Sampling_. This lesson shows how to use a linear version in RLlib. |\n",
    "| 06 | [Market Example](multi-armed-bandits/06-Market-Example.ipynb) | A simplified real-world example of MABs, finding the optimal stock and bond investment strategy. |\n",
    "\n",
    "In addition, exercise solutions for this segment of the tutorial can be found [here](multi-armed-bandits/solutions/Multi-Armed-Bandits-Solutions.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Plan\n",
    "\n",
    "We recommend the following _learning plan_ for working through the lessons:\n",
    "\n",
    "Start with the overview material for RL and RLlib:\n",
    "\n",
    "* [Ray RLlib Overview](00-Ray-RLlib-Overview.iypnb)\n",
    "* [Introduction to Reinforcement Learning](01-Introduction-to-Reinforcement-Learning.ipynb) \n",
    "* [Introduction to RLlib](02-Introduction-to-RLlib.ipynb)\n",
    "\n",
    "Then study several of the lessons for MABs, starting with these lessons:\n",
    "\n",
    "* [Multi-Armed-Bandits Overview](multi-armed-bandits/00-Multi-Armed-Bandits-Overview.iypnb)\n",
    "* [Introduction to Multi-Armed Bandits](multi-armed-bandits/01-Introduction-to-Multi-Armed-Bandits.ipynb)\n",
    "* [Exploration vs. Exploitation Strategies](multi-armed-bandits/02-Exploration-vs-Exploitation-Strategies.iypnb): Skim at least the first part of this lesson. \n",
    "* [Simple Multi-Armed Bandit](multi-armed-bandits/03-Simple-Multi-Armed-Bandit.ipynb)\n",
    "\n",
    "As time permits, study one or both of the following lessons:\n",
    "\n",
    "* [Linear Upper Confidence Bound](multi-armed-bandits/04-Linear-Upper-Confidence-Bound.ipynb)\n",
    "* [Linear Thompson Sampling](multi-armed-bandits/05-Linear-Thompson-Sampling.ipynb)\n",
    "\n",
    "Then finish with this more complete example:\n",
    "\n",
    "* [Market Example](multi-armed-bandits/06-Market-Example.ipynb)\n",
    "\n",
    "Next, return to the \"core\" RL lessons:\n",
    "\n",
    "Study the popular _CartPole_ example:\n",
    "\n",
    "* [Application: Cart Pole](03-Application-Cart-Pole.ipynb)\n",
    "\n",
    "If you working through the lessons at your own pace (i.e., not in a class), work through the _Bipedal Walker_ example:\n",
    "\n",
    "* [Application: Bipedal Walker](04-Bipedal-Walker.ipynb)\n",
    "\n",
    "Then finish the rest of the `ray-rllib` lessons:\n",
    "\n",
    "* [Custom Environments and Reward Shaping](05-Custom-Environments-Reward-Shaping.ipynb)\n",
    "* [Online Learning with DQN](06-Online-Learning-with-DQN.ipynb)\n",
    "\n",
    "Other examples are provided for your use. See the `extras` directory for examples that work with other OpenAI Gym environments:\n",
    "\n",
    "* [Extra: Application - Mountain Car](extras/Extra-Application-Mountain-Car.ipynb)\n",
    "* [Extra: Application - Taxi](extras/Extra-Application-Taxi.ipynb)\n",
    "* [Extra: Application - Frozen Lake](extras/Extra-Application-Frozen-Lake.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Help\n",
    "\n",
    "* The [#tutorial channel](https://ray-distributed.slack.com/archives/C011ML23W5B) on the [Ray Slack](https://ray-distributed.slack.com)\n",
    "* [Email](mailto:academy@anyscale.com)\n",
    "\n",
    "Find an issue? Please report it!\n",
    "\n",
    "* [GitHub issues](https://github.com/anyscale/academy/issues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
