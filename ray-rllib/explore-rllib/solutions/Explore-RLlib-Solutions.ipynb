{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ray RLlib Tutorial - Exercise Solutions\n",
    "\n",
    "Â© 2019-2020, Anyscale. All Rights Reserved\n",
    "\n",
    "![Anyscale Academy](../../../images/AnyscaleAcademy_Logo_clearbanner_141x100.png)\n",
    "\n",
    "This notebook contains the solutions for all the exercises in the RLlib tutorial.\n",
    "\n",
    "First, we have to setup everything needed from the other notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json, sys, os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01 Introduction to Reinforcement Learning\n",
    "\n",
    "### Exercise 1\n",
    "\n",
    "Finish implementing the `rollout_policy` function below, which should take an environment *and* a policy. Recall that the *policy* is a function that takes in a *state* and returns an *action*. The main difference is that instead of choosing a **random action**, like we just did (with poor results), the action should be chosen **with the policy** (as a function of the state)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created env: <TimeLimit<CartPoleEnv<CartPole-v0>>>\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "print('Created env:', env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first sample policy got an average reward of 9.47.\n",
      "The second sample policy got an average reward of 28.99.\n"
     ]
    }
   ],
   "source": [
    "def rollout_policy(env, policy):\n",
    "    state = env.reset()\n",
    "    \n",
    "    done = False\n",
    "    cumulative_reward = 0\n",
    "\n",
    "    # Keep looping as long as the simulation has not finished.\n",
    "    while not done:\n",
    "        # Choose a random action (either 0 or 1).\n",
    "        action = policy(state)\n",
    "        \n",
    "        # Take the action in the environment.\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        # Update the cumulative reward.\n",
    "        cumulative_reward += reward\n",
    "        \n",
    "    # Return the cumulative reward.\n",
    "    return cumulative_reward\n",
    "\n",
    "def sample_policy1(state):\n",
    "    return 0 if state[0] < 0 else 1\n",
    "\n",
    "def sample_policy2(state):\n",
    "    return 1 if state[0] < 0 else 0\n",
    "\n",
    "reward1 = np.mean([rollout_policy(env, sample_policy1) for _ in range(100)])\n",
    "reward2 = np.mean([rollout_policy(env, sample_policy2) for _ in range(100)])\n",
    "\n",
    "print('The first sample policy got an average reward of {}.'.format(reward1))\n",
    "print('The second sample policy got an average reward of {}.'.format(reward2))\n",
    "\n",
    "assert 5 < reward1 < 15, ('Make sure that rollout_policy computes the action '\n",
    "                          'by applying the policy to the state.')\n",
    "assert 25 < reward2 < 35, ('Make sure that rollout_policy computes the action '\n",
    "                           'by applying the policy to the state.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "The current network and training configuration are too large and heavy-duty for a simple problem like CartPole. Modify the configuration to use a smaller network and to speed up the optimization of the surrogate objective. (Fewer SGD iterations and a larger batch size should help.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray.rllib.agents.ppo import PPOTrainer, DEFAULT_CONFIG\n",
    "from ray.tune.logger import pretty_print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Ray is already running.\n"
     ]
    }
   ],
   "source": [
    "!../../../tools/start-ray.sh --check --verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '192.168.1.149',\n",
       " 'raylet_ip_address': '192.168.1.149',\n",
       " 'redis_address': '192.168.1.149:6379',\n",
       " 'object_store_address': '/tmp/ray/session_2020-06-26_05-43-51_233970_89279/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2020-06-26_05-43-51_233970_89279/sockets/raylet',\n",
       " 'webui_url': 'localhost:8265',\n",
       " 'session_dir': '/tmp/ray/session_2020-06-26_05-43-51_233970_89279'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init(address='auto', ignore_reinit_error=True, log_to_driver=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's one possible set. It takes longer for the max reward to reach 200, so I increased the number of episodes `N` to 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = DEFAULT_CONFIG.copy()\n",
    "config['num_workers'] = 3\n",
    "config['num_sgd_iter'] = 10                       # was 30\n",
    "config['sgd_minibatch_size'] = 256                # was 128\n",
    "config['model']['fcnet_hiddens'] = [20, 20]       # was [100, 100]\n",
    "config['num_cpus_per_worker'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-26 12:12:15,197\tINFO trainer.py:585 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "2020-06-26 12:12:15,198\tINFO trainer.py:612 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "2020-06-26 12:12:18,000\tWARNING util.py:37 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "agent = PPOTrainer(config, 'CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0: Min/Mean/Max reward:   8.0000/ 22.4324/122.0000\n",
      "  1: Min/Mean/Max reward:   8.0000/ 25.0898/ 96.0000\n",
      "  2: Min/Mean/Max reward:  10.0000/ 28.5608/112.0000\n",
      "  3: Min/Mean/Max reward:  10.0000/ 30.7687/107.0000\n",
      "  4: Min/Mean/Max reward:   9.0000/ 40.3846/113.0000\n",
      "  5: Min/Mean/Max reward:  11.0000/ 41.8235/125.0000\n",
      "  6: Min/Mean/Max reward:  11.0000/ 46.9400/144.0000\n",
      "  7: Min/Mean/Max reward:  14.0000/ 52.3300/149.0000\n",
      "  8: Min/Mean/Max reward:  14.0000/ 56.0000/138.0000\n",
      "  9: Min/Mean/Max reward:  15.0000/ 66.2300/162.0000\n",
      " 10: Min/Mean/Max reward:  13.0000/ 75.2900/193.0000\n",
      " 11: Min/Mean/Max reward:  13.0000/ 80.9700/193.0000\n",
      " 12: Min/Mean/Max reward:  20.0000/ 86.6300/178.0000\n",
      " 13: Min/Mean/Max reward:  20.0000/ 88.3700/164.0000\n",
      " 14: Min/Mean/Max reward:  20.0000/ 94.1800/200.0000\n",
      " 15: Min/Mean/Max reward:  22.0000/103.9200/200.0000\n",
      " 16: Min/Mean/Max reward:  28.0000/113.0500/200.0000\n",
      " 17: Min/Mean/Max reward:  28.0000/112.7100/200.0000\n",
      " 18: Min/Mean/Max reward:  24.0000/113.8200/200.0000\n",
      " 19: Min/Mean/Max reward:  24.0000/123.8900/200.0000\n"
     ]
    }
   ],
   "source": [
    "N=20                # was 10\n",
    "results = []\n",
    "episode_data = []\n",
    "episode_json = []\n",
    "for n in range(N):\n",
    "    result = agent.train()\n",
    "    results.append(result)\n",
    "    episode = {'n': n, \n",
    "               'episode_reward_min': result['episode_reward_min'],  \n",
    "               'episode_reward_mean': result['episode_reward_mean'], \n",
    "               'episode_reward_max': result['episode_reward_max'],  \n",
    "               'episode_len_mean': result['episode_len_mean']}    \n",
    "    episode_data.append(episode)\n",
    "    episode_json.append(json.dumps(episode))\n",
    "    print(f'{n:3d}: Min/Mean/Max reward: {result[\"episode_reward_min\"]:8.4f}/{result[\"episode_reward_mean\"]:8.4f}/{result[\"episode_reward_max\"]:8.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>episode_reward_min</th>\n",
       "      <th>episode_reward_mean</th>\n",
       "      <th>episode_reward_max</th>\n",
       "      <th>episode_len_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>22.432432</td>\n",
       "      <td>122.0</td>\n",
       "      <td>22.432432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>25.089820</td>\n",
       "      <td>96.0</td>\n",
       "      <td>25.089820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>10.0</td>\n",
       "      <td>28.560811</td>\n",
       "      <td>112.0</td>\n",
       "      <td>28.560811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>10.0</td>\n",
       "      <td>30.768657</td>\n",
       "      <td>107.0</td>\n",
       "      <td>30.768657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9.0</td>\n",
       "      <td>40.384615</td>\n",
       "      <td>113.0</td>\n",
       "      <td>40.384615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>11.0</td>\n",
       "      <td>41.823529</td>\n",
       "      <td>125.0</td>\n",
       "      <td>41.823529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>11.0</td>\n",
       "      <td>46.940000</td>\n",
       "      <td>144.0</td>\n",
       "      <td>46.940000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>14.0</td>\n",
       "      <td>52.330000</td>\n",
       "      <td>149.0</td>\n",
       "      <td>52.330000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>14.0</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>138.0</td>\n",
       "      <td>56.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>15.0</td>\n",
       "      <td>66.230000</td>\n",
       "      <td>162.0</td>\n",
       "      <td>66.230000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>13.0</td>\n",
       "      <td>75.290000</td>\n",
       "      <td>193.0</td>\n",
       "      <td>75.290000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>13.0</td>\n",
       "      <td>80.970000</td>\n",
       "      <td>193.0</td>\n",
       "      <td>80.970000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>20.0</td>\n",
       "      <td>86.630000</td>\n",
       "      <td>178.0</td>\n",
       "      <td>86.630000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>20.0</td>\n",
       "      <td>88.370000</td>\n",
       "      <td>164.0</td>\n",
       "      <td>88.370000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>20.0</td>\n",
       "      <td>94.180000</td>\n",
       "      <td>200.0</td>\n",
       "      <td>94.180000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>22.0</td>\n",
       "      <td>103.920000</td>\n",
       "      <td>200.0</td>\n",
       "      <td>103.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>28.0</td>\n",
       "      <td>113.050000</td>\n",
       "      <td>200.0</td>\n",
       "      <td>113.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>28.0</td>\n",
       "      <td>112.710000</td>\n",
       "      <td>200.0</td>\n",
       "      <td>112.710000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>24.0</td>\n",
       "      <td>113.820000</td>\n",
       "      <td>200.0</td>\n",
       "      <td>113.820000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>24.0</td>\n",
       "      <td>123.890000</td>\n",
       "      <td>200.0</td>\n",
       "      <td>123.890000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     n  episode_reward_min  episode_reward_mean  episode_reward_max  \\\n",
       "0    0                 8.0            22.432432               122.0   \n",
       "1    1                 8.0            25.089820                96.0   \n",
       "2    2                10.0            28.560811               112.0   \n",
       "3    3                10.0            30.768657               107.0   \n",
       "4    4                 9.0            40.384615               113.0   \n",
       "5    5                11.0            41.823529               125.0   \n",
       "6    6                11.0            46.940000               144.0   \n",
       "7    7                14.0            52.330000               149.0   \n",
       "8    8                14.0            56.000000               138.0   \n",
       "9    9                15.0            66.230000               162.0   \n",
       "10  10                13.0            75.290000               193.0   \n",
       "11  11                13.0            80.970000               193.0   \n",
       "12  12                20.0            86.630000               178.0   \n",
       "13  13                20.0            88.370000               164.0   \n",
       "14  14                20.0            94.180000               200.0   \n",
       "15  15                22.0           103.920000               200.0   \n",
       "16  16                28.0           113.050000               200.0   \n",
       "17  17                28.0           112.710000               200.0   \n",
       "18  18                24.0           113.820000               200.0   \n",
       "19  19                24.0           123.890000               200.0   \n",
       "\n",
       "    episode_len_mean  \n",
       "0          22.432432  \n",
       "1          25.089820  \n",
       "2          28.560811  \n",
       "3          30.768657  \n",
       "4          40.384615  \n",
       "5          41.823529  \n",
       "6          46.940000  \n",
       "7          52.330000  \n",
       "8          56.000000  \n",
       "9          66.230000  \n",
       "10         75.290000  \n",
       "11         80.970000  \n",
       "12         86.630000  \n",
       "13         88.370000  \n",
       "14         94.180000  \n",
       "15        103.920000  \n",
       "16        113.050000  \n",
       "17        112.710000  \n",
       "18        113.820000  \n",
       "19        123.890000  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data=episode_data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1001\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"1001\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error() {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < css_urls.length; i++) {\n",
       "      var url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.0.1.min.js\": \"JpP8FXbgAZLkfur7LiK3j9AGBhHNIvF742meBJrjO2ShJDhCG2I1uVvW+0DUtrmc\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.0.1.min.js\": \"xZlADit0Q04ISQEdKg2k3L4W9AwQBAuDs9nJL9fM/WwzL1tEU9VPNezOFX0nLEAz\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.0.1.min.js\": \"4BuPRZkdMKSnj3zoxiNrQ86XgNw0rYmBOxe7nshquXwwcauupgBF2DHLVG1WuZlV\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.0.1.min.js\": \"Dv1SQ87hmDqK6S5OhBf0bCuwAEvL5QYL0PuR/F1SPVhCS/r/abjkbpKDYL2zeM19\"};\n",
       "\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      if (url in hashes) {\n",
       "        element.crossOrigin = \"anonymous\";\n",
       "        element.integrity = \"sha384-\" + hashes[url];\n",
       "      }\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };var element = document.getElementById(\"1001\");\n",
       "  if (element == null) {\n",
       "    console.error(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  \n",
       "  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.0.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.0.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.0.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.0.1.min.js\"];\n",
       "  var css_urls = [];\n",
       "  \n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "    \n",
       "    \n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if (root.Bokeh !== undefined || force === true) {\n",
       "      \n",
       "    for (var i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i].call(root, root.Bokeh);\n",
       "    }\n",
       "    if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1001\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.0.1.min.js\": \"JpP8FXbgAZLkfur7LiK3j9AGBhHNIvF742meBJrjO2ShJDhCG2I1uVvW+0DUtrmc\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.0.1.min.js\": \"xZlADit0Q04ISQEdKg2k3L4W9AwQBAuDs9nJL9fM/WwzL1tEU9VPNezOFX0nLEAz\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.0.1.min.js\": \"4BuPRZkdMKSnj3zoxiNrQ86XgNw0rYmBOxe7nshquXwwcauupgBF2DHLVG1WuZlV\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.0.1.min.js\": \"Dv1SQ87hmDqK6S5OhBf0bCuwAEvL5QYL0PuR/F1SPVhCS/r/abjkbpKDYL2zeM19\"};\n\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      if (url in hashes) {\n        element.crossOrigin = \"anonymous\";\n        element.integrity = \"sha384-\" + hashes[url];\n      }\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };var element = document.getElementById(\"1001\");\n  if (element == null) {\n    console.error(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n    return false;\n  }\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  \n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.0.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.0.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.0.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.0.1.min.js\"];\n  var css_urls = [];\n  \n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    function(Bokeh) {\n    \n    \n    }\n  ];\n\n  function run_inline_js() {\n    \n    if (root.Bokeh !== undefined || force === true) {\n      \n    for (var i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n    if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../..\")\n",
    "from util.line_plots import plot_line, plot_line_with_min_max\n",
    "\n",
    "import bokeh.io\n",
    "# The next two lines prevent Bokeh from opening the graph in a new window.\n",
    "bokeh.io.reset_output()\n",
    "bokeh.io.output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "  <div class=\"bk-root\" id=\"42350769-463f-446f-92ac-5ad6e5ec1638\" data-root-id=\"1004\"></div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "(function(root) {\n",
       "  function embed_document(root) {\n",
       "    \n",
       "  var docs_json = {\"4315ad41-9fd6-48f2-84c7-e50058442301\":{\"roots\":{\"references\":[{\"attributes\":{\"below\":[{\"id\":\"1012\"}],\"center\":[{\"id\":\"1015\"},{\"id\":\"1019\"},{\"id\":\"1042\"}],\"left\":[{\"id\":\"1016\"}],\"plot_height\":500,\"plot_width\":800,\"renderers\":[{\"id\":\"1035\"},{\"id\":\"1040\"}],\"title\":{\"id\":\"1046\"},\"toolbar\":{\"id\":\"1026\"},\"x_range\":{\"id\":\"1005\"},\"x_scale\":{\"id\":\"1008\"},\"y_range\":{\"id\":\"1003\"},\"y_scale\":{\"id\":\"1010\"}},\"id\":\"1004\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{\"data\":{\"episode_len_mean\":{\"__ndarray__\":\"6wZT5LNuNkD1oJF3/hY5QPmsG0yRjzxAnd3Cr8bEPkAUO7ETOzFEQGlpaWlp6URAuB6F61F4R0AK16NwPSpKQAAAAAAAAExAH4XrUbiOUEDD9Shcj9JSQK5H4XoUPlRAuB6F61GoVUBI4XoUrhdWQOxRuB6Fi1dAexSuR+H6WUAzMzMzM0NcQD0K16NwLVxAFK5H4Xp0XEApXI/C9fheQA==\",\"dtype\":\"float64\",\"shape\":[20]},\"episode_reward_max\":{\"__ndarray__\":\"AAAAAACAXkAAAAAAAABYQAAAAAAAAFxAAAAAAADAWkAAAAAAAEBcQAAAAAAAQF9AAAAAAAAAYkAAAAAAAKBiQAAAAAAAQGFAAAAAAABAZEAAAAAAACBoQAAAAAAAIGhAAAAAAABAZkAAAAAAAIBkQAAAAAAAAGlAAAAAAAAAaUAAAAAAAABpQAAAAAAAAGlAAAAAAAAAaUAAAAAAAABpQA==\",\"dtype\":\"float64\",\"shape\":[20]},\"episode_reward_mean\":{\"__ndarray__\":\"6wZT5LNuNkD1oJF3/hY5QPmsG0yRjzxAnd3Cr8bEPkAUO7ETOzFEQGlpaWlp6URAuB6F61F4R0AK16NwPSpKQAAAAAAAAExAH4XrUbiOUEDD9Shcj9JSQK5H4XoUPlRAuB6F61GoVUBI4XoUrhdWQOxRuB6Fi1dAexSuR+H6WUAzMzMzM0NcQD0K16NwLVxAFK5H4Xp0XEApXI/C9fheQA==\",\"dtype\":\"float64\",\"shape\":[20]},\"episode_reward_min\":{\"__ndarray__\":\"AAAAAAAAIEAAAAAAAAAgQAAAAAAAACRAAAAAAAAAJEAAAAAAAAAiQAAAAAAAACZAAAAAAAAAJkAAAAAAAAAsQAAAAAAAACxAAAAAAAAALkAAAAAAAAAqQAAAAAAAACpAAAAAAAAANEAAAAAAAAA0QAAAAAAAADRAAAAAAAAANkAAAAAAAAA8QAAAAAAAADxAAAAAAAAAOEAAAAAAAAA4QA==\",\"dtype\":\"float64\",\"shape\":[20]},\"index\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19],\"level_0\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19],\"n\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19]},\"selected\":{\"id\":\"1054\"},\"selection_policy\":{\"id\":\"1053\"}},\"id\":\"1002\",\"type\":\"ColumnDataSource\"},{\"attributes\":{},\"id\":\"1053\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"axis_label\":\"reward\",\"formatter\":{\"id\":\"1049\"},\"ticker\":{\"id\":\"1017\"}},\"id\":\"1016\",\"type\":\"LinearAxis\"},{\"attributes\":{},\"id\":\"1005\",\"type\":\"DataRange1d\"},{\"attributes\":{},\"id\":\"1008\",\"type\":\"LinearScale\"},{\"attributes\":{\"axis_label\":\"n\",\"formatter\":{\"id\":\"1051\"},\"ticker\":{\"id\":\"1013\"}},\"id\":\"1012\",\"type\":\"LinearAxis\"},{\"attributes\":{\"x\":{\"field\":\"n\"},\"y\":{\"field\":\"episode_reward_mean\"}},\"id\":\"1038\",\"type\":\"Line\"},{\"attributes\":{},\"id\":\"1017\",\"type\":\"BasicTicker\"},{\"attributes\":{},\"id\":\"1013\",\"type\":\"BasicTicker\"},{\"attributes\":{\"bottom_units\":\"screen\",\"fill_alpha\":0.5,\"fill_color\":\"lightgrey\",\"left_units\":\"screen\",\"level\":\"overlay\",\"line_alpha\":1.0,\"line_color\":\"black\",\"line_dash\":[4,4],\"line_width\":2,\"render_mode\":\"css\",\"right_units\":\"screen\",\"top_units\":\"screen\"},\"id\":\"1025\",\"type\":\"BoxAnnotation\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_inspect\":\"auto\",\"active_multi\":null,\"active_scroll\":\"auto\",\"active_tap\":\"auto\",\"tools\":[{\"id\":\"1020\"},{\"id\":\"1021\"},{\"id\":\"1022\"},{\"id\":\"1023\"},{\"id\":\"1024\"},{\"id\":\"1044\"}]},\"id\":\"1026\",\"type\":\"Toolbar\"},{\"attributes\":{\"source\":{\"id\":\"1002\"}},\"id\":\"1041\",\"type\":\"CDSView\"},{\"attributes\":{},\"id\":\"1020\",\"type\":\"PanTool\"},{\"attributes\":{\"end\":203.84,\"start\":4.16},\"id\":\"1003\",\"type\":\"Range1d\"},{\"attributes\":{},\"id\":\"1010\",\"type\":\"LinearScale\"},{\"attributes\":{\"base\":{\"field\":\"n\",\"units\":\"data\"},\"fill_alpha\":0.3,\"level\":\"underlay\",\"line_color\":\"blue\",\"lower\":{\"field\":\"episode_reward_min\",\"units\":\"data\"},\"source\":{\"id\":\"1002\"},\"upper\":{\"field\":\"episode_reward_max\",\"units\":\"data\"}},\"id\":\"1042\",\"type\":\"Band\"},{\"attributes\":{},\"id\":\"1021\",\"type\":\"WheelZoomTool\"},{\"attributes\":{\"data_source\":{\"id\":\"1002\"},\"glyph\":{\"id\":\"1038\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"1039\"},\"selection_glyph\":null,\"view\":{\"id\":\"1041\"}},\"id\":\"1040\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"overlay\":{\"id\":\"1025\"}},\"id\":\"1022\",\"type\":\"BoxZoomTool\"},{\"attributes\":{},\"id\":\"1023\",\"type\":\"ResetTool\"},{\"attributes\":{\"line_alpha\":0.1,\"x\":{\"field\":\"n\"},\"y\":{\"field\":\"episode_reward_mean\"}},\"id\":\"1039\",\"type\":\"Line\"},{\"attributes\":{\"axis\":{\"id\":\"1016\"},\"dimension\":1,\"grid_line_alpha\":0.5,\"ticker\":null},\"id\":\"1019\",\"type\":\"Grid\"},{\"attributes\":{},\"id\":\"1024\",\"type\":\"SaveTool\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#1f77b4\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"blue\"},\"size\":{\"units\":\"screen\",\"value\":3},\"x\":{\"field\":\"n\"},\"y\":{\"field\":\"episode_reward_mean\"}},\"id\":\"1034\",\"type\":\"Scatter\"},{\"attributes\":{},\"id\":\"1051\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"callback\":null,\"tooltips\":[[\"n\",\"$x\"],[\"episode_reward_mean\",\"$y\"]]},\"id\":\"1044\",\"type\":\"HoverTool\"},{\"attributes\":{\"data_source\":{\"id\":\"1002\"},\"glyph\":{\"id\":\"1033\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"1034\"},\"selection_glyph\":null,\"view\":{\"id\":\"1036\"}},\"id\":\"1035\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"text\":\"Episode Rewards\"},\"id\":\"1046\",\"type\":\"Title\"},{\"attributes\":{\"axis\":{\"id\":\"1012\"},\"grid_line_alpha\":0.5,\"ticker\":null},\"id\":\"1015\",\"type\":\"Grid\"},{\"attributes\":{\"source\":{\"id\":\"1002\"}},\"id\":\"1036\",\"type\":\"CDSView\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.3},\"fill_color\":{\"value\":\"#1f77b4\"},\"line_color\":{\"value\":\"blue\"},\"size\":{\"units\":\"screen\",\"value\":3},\"x\":{\"field\":\"n\"},\"y\":{\"field\":\"episode_reward_mean\"}},\"id\":\"1033\",\"type\":\"Scatter\"},{\"attributes\":{},\"id\":\"1054\",\"type\":\"Selection\"},{\"attributes\":{},\"id\":\"1049\",\"type\":\"BasicTickFormatter\"}],\"root_ids\":[\"1004\"]},\"title\":\"Bokeh Application\",\"version\":\"2.0.1\"}};\n",
       "  var render_items = [{\"docid\":\"4315ad41-9fd6-48f2-84c7-e50058442301\",\"root_ids\":[\"1004\"],\"roots\":{\"1004\":\"42350769-463f-446f-92ac-5ad6e5ec1638\"}}];\n",
       "  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "\n",
       "  }\n",
       "  if (root.Bokeh !== undefined) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (root.Bokeh !== undefined) {\n",
       "        clearInterval(timer);\n",
       "        embed_document(root);\n",
       "      } else {\n",
       "        attempts++;\n",
       "        if (attempts > 100) {\n",
       "          clearInterval(timer);\n",
       "          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
       "        }\n",
       "      }\n",
       "    }, 10, root)\n",
       "  }\n",
       "})(window);"
      ],
      "application/vnd.bokehjs_exec.v0+json": ""
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "id": "1004"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_line_with_min_max(df, x_col='n', y_col='episode_reward_mean', min_col='episode_reward_min', max_col='episode_reward_max',\n",
    "                      title='Episode Rewards', x_axis_label='n', y_axis_label='reward')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "([image](../../../images/rllib/Cart-Pole-Episode-Rewards-Exercise.png))\n",
    "\n",
    "Compare this graph with the graph in the lesson, where we used a stronger network:\n",
    "\n",
    "![](../../../images/rllib/Cart-Pole-Episode-Rewards.png)\n",
    "\n",
    "Note that we only used 5 episodes before. If you compare the graphs at n=4, you see that this execise solution is training more slowly, but it after N=10, the mean reward grows quickly.\n",
    "\n",
    "Try it again with slightly larger and/or small neural network layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 05: Custom Environments and Reward Shaping\n",
    "\n",
    "### Exercise 1: A Custom Environment with Rewards\n",
    "\n",
    "Now we'll create an `n-Chain` environment, which represents moves along a linear chain of states, with two actions:\n",
    "\n",
    "     (0) **forward**: move along the chain but returns no reward\n",
    "     (1) **backward**: returns to the beginning and has a small reward\n",
    "\n",
    "The end of the chain, however, provides a large reward, and by moving **forward** at the end of the chain, this large reward can be repeated.\n",
    "\n",
    "#### Step 1: Implement `ChainEnv._setup_spaces`\n",
    "\n",
    "Use a `spaces.Discrete` action space and observation space. Implement `ChainEnv._setup_spaces` in `ChainEnv` so that `self.action_space` and `self.obseration_space` are proper gym spaces.\n",
    "  \n",
    "1. The observation space is an integer in the range `[0 to n-1]`.\n",
    "2. The action space is an integer in `[0, 1]`.\n",
    "\n",
    "For example:\n",
    "\n",
    "```python\n",
    "self.action_space = spaces.Discrete(2)\n",
    "self.observation_space = ...\n",
    "```\n",
    "\n",
    "You should see a message indicating tests passing when done correctly!\n",
    "\n",
    "#### Step 2: Implement a reward function.\n",
    "\n",
    "When `env.step` is called, it returns a tuple of `(state, reward, done, info)`. Right now, the reward is always 0. Modify `step()` so that the following rewards are returned for the given actions: \n",
    "\n",
    "1. `action == 1` will return `self.small_reward`.\n",
    "2. `action == 0` will return 0 if `self.state < self.n - 1`.\n",
    "3. `action == 0` will return `self.large_reward` if `self.state == self.n - 1`.\n",
    "\n",
    "You should see a message indicating tests passing when done correctly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('..')\n",
    "from test_exercises import test_chain_env_spaces, test_chain_env_reward, test_chain_env_behavior\n",
    "from gym import spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing if spaces have been setup correctly...\n",
      "Success! You've setup the spaces correctly.\n",
      "Testing if reward has been setup correctly...\n",
      "Success! You've setup the rewards correctly.\n"
     ]
    }
   ],
   "source": [
    "class ChainEnv(gym.Env):\n",
    "    \n",
    "    def __init__(self, env_config = None):\n",
    "        env_config = env_config or {}\n",
    "        self.n = env_config.get(\"n\", 20)\n",
    "        self.small_reward = env_config.get(\"small\", 2)  # payout for 'backwards' action\n",
    "        self.large_reward = env_config.get(\"large\", 10)  # payout at end of chain for 'forwards' action\n",
    "        self.state = 0  # Start at beginning of the chain\n",
    "        self._horizon = self.n\n",
    "        self._counter = 0  # For terminating the episode\n",
    "        self._setup_spaces()\n",
    "    \n",
    "    def _setup_spaces(self):\n",
    "        ##############\n",
    "        # TODO: Implement this so that it passes tests\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        self.observation_space = spaces.Discrete(self.n)\n",
    "        ##############\n",
    "\n",
    "    def step(self, action):\n",
    "        assert self.action_space.contains(action)\n",
    "        if action == 1:  # 'backwards': go back to the beginning, get small reward\n",
    "            ##############\n",
    "            # TODO 2: Implement this so that it passes tests\n",
    "            reward = self.small_reward\n",
    "            ##############\n",
    "            self.state = 0\n",
    "        elif self.state < self.n - 1:  # 'forwards': go up along the chain\n",
    "            ##############\n",
    "            # TODO 2: Implement this so that it passes tests\n",
    "            reward = 0\n",
    "            self.state += 1\n",
    "        else:  # 'forwards': stay at the end of the chain, collect large reward\n",
    "            ##############\n",
    "            # TODO 2: Implement this so that it passes tests\n",
    "            reward = self.large_reward\n",
    "            ##############\n",
    "        self._counter += 1\n",
    "        done = self._counter >= self._horizon\n",
    "        return self.state, reward, done, {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = 0\n",
    "        self._counter = 0\n",
    "        return self.state\n",
    "    \n",
    "# Tests here:\n",
    "test_chain_env_spaces(ChainEnv)\n",
    "test_chain_env_reward(ChainEnv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Improve the Policy\n",
    "\n",
    "Modify `ShapedChainEnv.step()` in the next cell to provide a reward that encourages the policy to traverse the chain (not just stick to 0). Do not change the behavior of the environment (the action -> state behavior should be the same).\n",
    "\n",
    "You can change the reward to be whatever you wish. We'll text it in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate `ShapedChainEnv` by Running the Cell(s) Below\n",
    "\n",
    "This trains PPO on the new env and counts the number of states seen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll set up things we need from the lesson notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.agents.ppo import PPOTrainer, DEFAULT_CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_config = DEFAULT_CONFIG.copy()\n",
    "trainer_config['num_workers'] = 1\n",
    "trainer_config[\"train_batch_size\"] = 400\n",
    "trainer_config[\"sgd_minibatch_size\"] = 64\n",
    "trainer_config[\"num_sgd_iter\"] = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now here's one solution, where the reward calculations are the only difference from the previous implementation of `step`. This problem is actually difficult to solve, because it's hard to encourage exploration with just the reward alone. \n",
    "\n",
    "The key is to penalize action 1 (go back to the beginning), because you always get a small reward if you stay there, so there's a temptation to exploit that action and keep accruing the small reward until you hit the goal. Hence, this solution sets the reward to zero and instead returns `self.small_reward * step.state` for the action 0 when the state is `self.state < self.n - 1` (i.e., not at the right-hand end). Hence, the reward grows as we move to the right. We tried many variations and most returned similar results that were not especially satisfactory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[17, 11, 18, 15, 18, 14, 10, 11, 11, 15]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.random.choice(range(10,20)) for _ in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing if behavior has been changed...\n",
      "Success! Behavior of environment is correct.\n"
     ]
    }
   ],
   "source": [
    "class ShapedChainEnv(ChainEnv):\n",
    "    def step(self, action):\n",
    "        assert self.action_space.contains(action)\n",
    "        if action == 1:  # 'backwards': go back to the beginning\n",
    "            reward = 0\n",
    "            self.state = 0\n",
    "        elif self.state < self.n - 1:  # 'forwards': go up along the chain\n",
    "            reward = np.random.choice(range(0, self.small_reward))  \n",
    "            self.state += 1\n",
    "        else:  # 'forwards': stay at the end of the chain\n",
    "            reward = self.large_reward\n",
    "        self._counter += 1\n",
    "        done = self._counter >= 2*self._horizon\n",
    "        return self.state, reward, done, {}\n",
    "\n",
    "test_chain_env_behavior(ShapedChainEnv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_training(chainEnvClass, iterations=20):\n",
    "    trainer = PPOTrainer(trainer_config, chainEnvClass)\n",
    "    print(f'Training iterations: ', end='')\n",
    "    for i in range(iterations):\n",
    "        print('.', end='')\n",
    "        trainer.train()\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/deanwampler/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "2020-06-26 14:46:56,433\tWARNING util.py:37 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iterations: ....................\n"
     ]
    }
   ],
   "source": [
    "do_training(ShapedChainEnv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_rollout(chainEnvClass):\n",
    "    env = chainEnvClass({})\n",
    "    max_states = []\n",
    "    cumulative_rewards = []\n",
    "    for i in range(5):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        max_states.append(-1)\n",
    "        cumulative_rewards.append(0)\n",
    "        while not done:\n",
    "            action = trainer.compute_action(state)\n",
    "            state, reward, done, results = env.step(action)\n",
    "            max_states[i] = max(max_states[i], state)\n",
    "            cumulative_rewards[i] += reward\n",
    "            # The \"results\" returned by env.step are empty.\n",
    "            #print(f'state = {state:3d}, reward = {reward:6.3f}, cumulative_reward = {cumulative_reward:6.3f}, done = {str(done):5s}, max_state = {max_state}') \n",
    "\n",
    "    print(f'Cumulative rewards: {cumulative_rewards}')\n",
    "    print(f'Max states you visited are: {max_states}. The mean of this list is {np.mean(max_states)}. (There are {env.n} states.)')\n",
    "    actual = np.mean(max_states) / env.n\n",
    "    desired = 0.7\n",
    "    print(f'This policy traversed on average {actual*100:4.1f}% of the available states.')\n",
    "    assert visited > 0.7, f\"{actual*100:4.1f}% is less than the desired percentage of {desired*100:4.1f}%.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulative rewards: [159, 107, 137, 141, 123]\n",
      "Max states you visited are: [8, 5, 9, 4, 5]. The mean of this list is 6.2. (There are 20 states.)\n",
      "This policy traversed on average 31.0% of the available states.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "31.0% is less than the desired percentage of 70.0%.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-181-57406e2882ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdo_rollout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mShapedChainEnv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-125-32bb599ce9ed>\u001b[0m in \u001b[0;36mdo_rollout\u001b[0;34m(chainEnvClass)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mdesired\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'This policy traversed on average {actual*100:4.1f}% of the available states.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mvisited\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{actual*100:4.1f}% is less than the desired percentage of {desired*100:4.1f}%.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: 31.0% is less than the desired percentage of 70.0%."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-27 11:29:03,585\tERROR worker.py:1049 -- listen_error_messages_raylet: Connection closed by server.\n",
      "2020-06-27 11:29:03,615\tERROR import_thread.py:93 -- ImportThread: Connection closed by server.\n"
     ]
    }
   ],
   "source": [
    "do_rollout(ShapedChainEnv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(If it fails to pass, rerun the previous cell.)\n",
    "\n",
    "This may have taken you several tries. The key is to penalize action 1 (go back to the beginning), because you always get a small reward if you stay there, so there's a temptation to exploit that action and keep accruing the small reward until you hit the goal. Hence, this solution sets the reward to zero and instead returns `self.small_reward` for the action 0 when the state is `self.state < self.n - 1` (i.e., not at the right-hand end)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does it take to visit all states? This variant only stops when it has successfully visited all states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing if behavior has been changed...\n",
      "Success! Behavior of environment is correct.\n"
     ]
    }
   ],
   "source": [
    "class ShapedChainEnvVisited(ChainEnv):\n",
    "\n",
    "    def __init__(self, env_config = None):\n",
    "        super().__init__(env_config)\n",
    "        self.visited = set()\n",
    "        \n",
    "    def step(self, action):\n",
    "        assert self.action_space.contains(action)\n",
    "        self.visited.add(self.state)\n",
    "        if action == 1:  # 'backwards': go back to the beginning\n",
    "            reward = 0\n",
    "            self.state = 0\n",
    "        elif self.state < self.n - 1:  # 'forwards': go up along the chain\n",
    "            reward = self.calc_reward_for_action_0()\n",
    "            self.state += 1\n",
    "        else:  # 'forwards': stay at the end of the chain\n",
    "            reward = self.large_reward\n",
    "        self._counter += 1\n",
    "        done = len(self.visited) == self.n\n",
    "        return self.state, reward, done, {}\n",
    "\n",
    "    def calc_reward_for_action_0(self):\n",
    "        return self.state * (self.large_reward - self.small_reward) / self.n\n",
    "\n",
    "\n",
    "test_chain_env_behavior(ShapedChainEnv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/deanwampler/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "2020-06-26 13:55:06,346\tWARNING util.py:37 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iterations: ....................\n"
     ]
    }
   ],
   "source": [
    "trainer = PPOTrainer(trainer_config, ShapedChainEnvVisited);\n",
    "print(f'Training iterations: ', end='')\n",
    "for i in range(20):\n",
    "    print('.', end='')\n",
    "    trainer.train()\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can take a while for the following to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulative reward: 14211.199999998242\n",
      "Step count:        45516\n",
      "Max state:         19\n"
     ]
    }
   ],
   "source": [
    "env = ShapedChainEnvVisited({})\n",
    "\n",
    "state = env.reset()\n",
    "max_state = -1\n",
    "cumulative_reward = 0\n",
    "step_count = 0\n",
    "done = False\n",
    "while not done:\n",
    "    step_count += 1\n",
    "    action = trainer.compute_action(state)\n",
    "    state, reward, done, results = env.step(action)\n",
    "    max_state = max(max_state, state)\n",
    "    cumulative_reward += reward\n",
    "    # The \"results\" returned by env.step are empty.\n",
    "    #print(f'state = {state:3d}, reward = {reward:6.3f}, cumulative_reward = {cumulative_reward:6.3f}, done = {str(done):5s}, max_state = {max_state}') \n",
    "\n",
    "print(f'Cumulative reward: {cumulative_reward}')\n",
    "print(f'Step count:        {step_count}')\n",
    "print(f'Max state:         {max_state}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A large number, but a bit misleading, due to the probabilistic nature of selecting actions, it's possible that some states \"avoid\" getting occupied for an unusually long time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
