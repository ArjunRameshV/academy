{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ray RLlib - Introduction to RLlib\n",
    "\n",
    "Â© 2019-2020, Anyscale. All Rights Reserved\n",
    "\n",
    "![Anyscale Academy](../images/AnyscaleAcademy_Logo_clearbanner_141x100.png)\n",
    "\n",
    "In the [previous lesson](01-Introduction-to-Reinforcement-Learning.ipynb), we learned the basic concepts of reinforcement learning, with a \"taste\" of [RLlib](https://rllib.io) and [OpenAI Gym](https://gym.openai.com). This lesson takes a step back to provide more information about RLlib and the features it provides. The subsequent lessons will continue our exploration of RL algorithms and tools.\n",
    "\n",
    "For more detailed information about RLlib and its open source community, see the following:\n",
    "\n",
    "* [rllib.io](http://rllib.io) (the documentation)\n",
    "* [GitHub repo](https://github.com/ray-project/ray/tree/master/rllib#rllib-scalable-reinforcement-learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RLlib is structured conceptually like this:\n",
    "\n",
    "![RLlib Stack](../images/rllib/RLlib-Stack-smaller.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The _(1) Application Support_ boxes are components used for particular RL algorithms. The _(2) Abstractions for RL_ provide building blocks used by the many algorithms that are implemented in RLlib (listed below). They also provide hooks for implementing your own algorithms. RLlib leverages Ray for efficient, cluster-wide, _(3) Distributed Execution_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start up Ray as in the previous lesson:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!../tools/start-ray.sh --check --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RLlib in 60 Seconds (plus some...)\n",
    "\n",
    "Here is a fast introduction to using RLlib from a command line, adapted from the [documentation](https://docs.ray.io/en/latest/rllib.html#rllib-in-60-seconds).\n",
    "\n",
    "First you would install [PyTorch](http://pytorch.org/) or [TensorFlow](https://www.tensorflow.org/), whichever you prefer.  Then install RLlib. **All of them** are already installed in this tutorial environment.\n",
    "\n",
    "```shell\n",
    "pip install ray[rllib]  # or consider using: ray[debug]\n",
    "```\n",
    "\n",
    "Then **train** _CartPole_ using _PPO_ with the `rllib` CLI, where we'll stop at 20 iterations, saving checkpoints every 10 iterations and at the end:\n",
    "\n",
    "```shell\n",
    "rllib train --run PPO --env CartPole-v0 --stop='{\"training_iteration\": 20}' --ray-address auto --checkpoint-freq 10 --checkpoint-at-end\n",
    "```\n",
    "\n",
    "We also specify using the Ray cluster that's already running and save a model checkpoint at the end.\n",
    "\n",
    "The `rllib` CLI has a `--help` flag that prints details about the supported options:\n",
    "\n",
    "```shell\n",
    "rllib --help          # general help\n",
    "rllib train --help    # specific help on the training options\n",
    "rllib rollout --help  # specific help on the rollout options\n",
    "```\n",
    "\n",
    "_Rollout_ means running an episode with the trained model, which you specify by passing a checkpoint directory to the command.\n",
    "\n",
    "You can execute the same training logic (but will different output) using the following Python code, which leverages [Ray Tune](http://tune.io), specifically the [tune.run](https://docs.ray.io/en/latest/tune/api_docs/execution.html#tune-run) method:\n",
    "\n",
    "```python\n",
    "from ray import tune\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "tune.run(PPOTrainer, \n",
    "    config={\"env\": \"CartPole-v0\"},\n",
    "    stop={\"training_iteration\": 20},\n",
    "    checkpoint_at_end=True,\n",
    "    verbose=2            # 2 for INFO; change to 1 or 0 to reduce the output.\n",
    "    )  \n",
    "```\n",
    "\n",
    "You can view the training results, which are written to the directory `$HOME/ray_results` using TensorBoard:\n",
    "\n",
    "```shell\n",
    "tensorboard --logdir=~/ray_results\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try the CLI if you like now. The following `ray train` command will take between and two minutes on a \"recent vintage\" laptop:\n",
    "\n",
    "You could also run this command in a separate terminal window.\n",
    "\n",
    "> **Tip:** The output will be long. In each cell, right click and select _Enable scrolling for outputs_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!rllib train --run PPO --env CartPole-v0 --stop='{\"training_iteration\": 20}' --ray-address auto --checkpoint-freq 10 --checkpoint-at-end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run TensorBoard, open a terminal (you can use the `+` item under the _Edit_ menu) and run this command:\n",
    "\n",
    "```shell\n",
    "!tensorboard --logdir=~/ray_results\n",
    "```\n",
    "\n",
    "(This command can't be run in a notebook cell...)\n",
    "\n",
    "TensorBoard lets you browse the training run data. Open the URL shown in the output.\n",
    "\n",
    "Here is a [screenshot](../images/rllib/TensorBoard-CartPole-PPO.png).\n",
    "\n",
    "Note that those results were written to the directory `~/ray_results`, by default, during training runs. In that directory you'll find results for all the RL training we'll do in this tutorial. You may wish to clean out old results periodically. For the run we just did, look for the results in `~/ray_results/default/PPO-CartPole-V0_0_*`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rollout\n",
    "\n",
    "> **WARNING:** The `rllib rollout` command shown below won't work in a cloud environment, because it attempts to pop up a window. If you are taking a live class, the instructor will demonstrate what you would see. You can also watch the next video of a single _episode_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"../images/rllib/Cart-Pole-Example-Video.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "cart_pole_sample_video='../images/rllib/Cart-Pole-Example-Video.mp4'\n",
    "Video(cart_pole_sample_video)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are working on a laptop, you can use `rllib rollout <checkpoint> --run PPO` to reply a session using the trained model in `<checkpoint>`, which in this case will be a directory with a name like this:\n",
    "\n",
    "```\n",
    "$HOME/ray_results/default/PPO_CartPole-v0_0_YYYY-MM-DD_HH-MM-SS.../checkpoint_20/checkpoint-20/\n",
    "```\n",
    "\n",
    "The following shell command should show you the directory for your run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/deanwampler/ray_results/default\n",
      "/Users/deanwampler/ray_results/default/experiment_state-2020-06-18_13-23-04.json\n",
      "/Users/deanwampler/ray_results/default/PPO_CartPole-v0_0_2020-06-18_13-23-04objm8pgg\n",
      "/Users/deanwampler/ray_results/default/PPO_CartPole-v0_0_2020-06-18_13-23-04objm8pgg/result.json\n",
      "/Users/deanwampler/ray_results/default/PPO_CartPole-v0_0_2020-06-18_13-23-04objm8pgg/params.pkl\n",
      "/Users/deanwampler/ray_results/default/PPO_CartPole-v0_0_2020-06-18_13-23-04objm8pgg/params.json\n",
      "/Users/deanwampler/ray_results/default/PPO_CartPole-v0_0_2020-06-18_13-23-04objm8pgg/events.out.tfevents.1592511784.DWAnyscaleMBP.local\n",
      "/Users/deanwampler/ray_results/default/PPO_CartPole-v0_0_2020-06-18_13-23-04objm8pgg/progress.csv\n",
      "/Users/deanwampler/ray_results/default/PPO_CartPole-v0_0_2020-06-18_13-23-04objm8pgg/checkpoint_20\n",
      "/Users/deanwampler/ray_results/default/PPO_CartPole-v0_0_2020-06-18_13-23-04objm8pgg/checkpoint_20/checkpoint-20\n",
      "/Users/deanwampler/ray_results/default/PPO_CartPole-v0_0_2020-06-18_13-23-04objm8pgg/checkpoint_20/checkpoint-20.tune_metadata\n",
      "/Users/deanwampler/ray_results/default/PPO_CartPole-v0_0_2020-06-18_13-23-04objm8pgg/checkpoint_20/.is_checkpoint\n",
      "/Users/deanwampler/ray_results/default/PPO_CartPole-v0_0_2020-06-18_13-23-04objm8pgg/checkpoint_10\n",
      "/Users/deanwampler/ray_results/default/PPO_CartPole-v0_0_2020-06-18_13-23-04objm8pgg/checkpoint_10/checkpoint-10\n",
      "/Users/deanwampler/ray_results/default/PPO_CartPole-v0_0_2020-06-18_13-23-04objm8pgg/checkpoint_10/.is_checkpoint\n",
      "/Users/deanwampler/ray_results/default/PPO_CartPole-v0_0_2020-06-18_13-23-04objm8pgg/checkpoint_10/checkpoint-10.tune_metadata\n"
     ]
    }
   ],
   "source": [
    "!find ~/ray_results/default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now substitute `checkpoint_dir` in the following command with the full directory path shown that matches `PPO_CartPole_.../checkpoint_20/checkpoint-20`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rllib rollout checkpoint_dir --run PPO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [this RLlib page on training policies](https://docs.ray.io/en/master/rllib-training.html) for more examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Concepts: Policies, Samples, and Trainers\n",
    "\n",
    "### Policies\n",
    "\n",
    "[Policies](https://docs.ray.io/en/latest/rllib-concepts.html#policies) are Python classes that define how an agent acts in an environment.\n",
    "\n",
    "[Rollout workers](https://docs.ray.io/en/latest/rllib-concepts.html#policy-evaluation) query the policy to determine agent actions. \n",
    "\n",
    "In a [gym](https://docs.ray.io/en/latest/rllib-env.html#openai-gym) environment, there is a single agent and policy. In [vector environments](https://docs.ray.io/en/latest/rllib-env.html#vectorized), policy inference is for multiple agents at once, and in [multi-agent and hierachical environments](https://docs.ray.io/en/latest/rllib-env.html#multi-agent-and-hierarchical), there may be multiple policies, each controlling one or more agents.\n",
    "\n",
    "![Environments and Policies in RLlib](../images/rllib/multi-flat.svg)\n",
    "\n",
    "Policies can be implemented using any framework ([code](https://github.com/ray-project/ray/blob/master/rllib/policy/policy.py)). However, for TensorFlow and PyTorch, RLlib has [build_tf_policy](https://docs.ray.io/en/latest/rllib-concepts.html#building-policies-in-tensorflow) and [build_torch_policy](https://docs.ray.io/en/latest/rllib-concepts.html#building-policies-in-pytorch) helper functions, respectively, that let you define a trainable policy with a functional-style API. An example is in the [documentation](https://docs.ray.io/en/latest/rllib.html#policies)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Batches\n",
    "\n",
    "From single processes to [large clusters](https://docs.ray.io/en/latest/rllib-training.html#specifying-resources>), all data interchange in RLlib uses [sample batches](https://github.com/ray-project/ray/blob/master/rllib/policy/sample_batch.py). Sample batches encode one or more fragments of a trajectory. Typically, RLlib collects batches of size `rollout_fragment_length` from rollout workers, and concatenates one or more of these batches into a batch of size `train_batch_size` that is the input to SGD.\n",
    "\n",
    "A typical sample batch looks something like the following when summarized. Since all values are kept in arrays, this allows for efficient encoding and transmission across the network:\n",
    "\n",
    "```python\n",
    " { 'action_logp': np.ndarray((200,), dtype=float32, min=-0.701, max=-0.685, mean=-0.694),\n",
    "   'actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.495),\n",
    "   'dones': np.ndarray((200,), dtype=bool, min=0.0, max=1.0, mean=0.055),\n",
    "   'infos': np.ndarray((200,), dtype=object, head={}),\n",
    "   'new_obs': np.ndarray((200, 4), dtype=float32, min=-2.46, max=2.259, mean=0.018),\n",
    "   'obs': np.ndarray((200, 4), dtype=float32, min=-2.46, max=2.259, mean=0.016),\n",
    "   'rewards': np.ndarray((200,), dtype=float32, min=1.0, max=1.0, mean=1.0),\n",
    "   't': np.ndarray((200,), dtype=int64, min=0.0, max=34.0, mean=9.14)}\n",
    "```\n",
    "\n",
    "In [multi-agent mode](https://docs.ray.io/en/latest/rllib-concepts.html#policies-in-multi-agent), sample batches are collected separately for each individual policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Policies each define a `learn_on_batch()` method that improves the policy given a sample batch of input. For TensorFlow and PyTorch policies, this is implemented using a _loss function_ that takes as input sample batch tensors and outputs a scalar loss value. Here are a few example loss functions:\n",
    "\n",
    "* Simple [policy gradient loss](https://github.com/ray-project/ray/blob/master/rllib/agents/pg/pg_tf_policy.py).\n",
    "* Simple [Q-function loss](https://github.com/ray-project/ray/blob/a1d2e1762325cd34e14dc411666d63bb15d6eaf0/rllib/agents/dqn/simple_q_policy.py#L136)\n",
    "* Importance-weighted [APPO surrogate loss](https://github.com/ray-project/ray/blob/master/rllib/agents/ppo/appo_policy.py)\n",
    "\n",
    "RLlib [trainer classes](https://docs.ray.io/en/latest/rllib-concepts.html#trainers) coordinate the distributed workflow of running rollouts and optimizing policies. They do this by leveraging Ray [parallel iterators](https://docs.ray.io/en/latest/iter.html) to implement the desired computation pattern. The following figure shows *synchronous sampling*, the simplest of [these patterns](https://docs.ray.io/en/latest/rllib-algorithms.html):\n",
    "\n",
    "![Synchronous Sampling](../images/rllib/a2c-arch.svg)\n",
    "\n",
    "    Synchronous Sampling (e.g., A2C, PG, PPO)\n",
    "\n",
    "RLlib uses [Ray actors](https://docs.ray.io/en/latest/actors.html) to scale training from a single core to many thousands of cores in a cluster. You can [configure the parallelism](https://docs.ray.io/en/latest/rllib-training.html#specifying-resources) used for training by changing the `num_workers` parameter. Check out our [scaling guide](https://docs.ray.io/en/latest/rllib-training.html#scaling-guide) for more details here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application Support\n",
    "\n",
    "Beyond environments defined in Python, [RLlib supports]((https://docs.ray.io/en/latest/rllib.html#application-support)) batch training on [offline datasets](https://docs.ray.io/en/latest/rllib-offline.html), and also provides a variety of integration strategies for [external applications](https://docs.ray.io/en/latest/rllib-env.html#external-agents-and-applications).\n",
    "\n",
    "### Customization\n",
    "\n",
    "RLlib provides ways to customize almost all aspects of training, including the [environmen](https://docs.ray.io/en/latest/rllib-env.html#configuring-environments), [neural network model](https://docs.ray.io/en/latest/rllib-models.html#tensorflow-models), [action distribution](https://docs.ray.io/en/latest/rllib-models.html#custom-action-distributions), and [policy definitions](https://docs.ray.io/en/latest/rllib-concepts.html#policies>).\n",
    "\n",
    "![RLlib components](../images/rllib/RLlib-components.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms Implemented in RLlib\n",
    "\n",
    "Here is the current list of supported algorithms in RLlib. The links go to the corresponding RLlib documentation, which includes links to the original papers and other references.\n",
    "\n",
    "See also the documentation's [Feature Compatibility Matrix](https://docs.ray.io/en/latest/rllib-algorithms.html#feature-compatibility-matrix), which lists the algorithms and useful properties for them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High-throughput Architectures\n",
    "\n",
    "* [Distributed Prioritized Experience Replay (Ape-X)](https://ray.readthedocs.io/en/latest/rllib-algorithms.html#distributed-prioritized-experience-replay-ape-x)\n",
    "* [Importance Weighted Actor-Learner Architecture (IMPALA)](https://ray.readthedocs.io/en/latest/rllib-algorithms.html#importance-weighted-actor-learner-architecture-impala)\n",
    "* [Asynchronous Proximal Policy Optimization (APPO)](https://ray.readthedocs.io/en/latest/rllib-algorithms.html#asynchronous-proximal-policy-optimization-appo)\n",
    "* [Decentralized Distributed Proximal Policy Optimization (DD-PPO)](https://docs.ray.io/en/latest/rllib-algorithms.html#decentralized-distributed-proximal-policy-optimization-dd-ppo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient-based\n",
    "\n",
    "* [Advantage Actor-Critic (A2C, A3C)](https://ray.readthedocs.io/en/latest/rllib-algorithms.html#advantage-actor-critic-a2c-a3c)\n",
    "* [Deep Deterministic Policy Gradients (DDPG, TD3)](https://ray.readthedocs.io/en/latest/rllib-algorithms.html#deep-deterministic-policy-gradients-ddpg-td3)\n",
    "* [Deep Q Networks (DQN, Rainbow, Parametric DQN)](https://ray.readthedocs.io/en/latest/rllib-algorithms.html#deep-q-networks-dqn-rainbow-parametric-dqn)\n",
    "* [Policy Gradients](https://ray.readthedocs.io/en/latest/rllib-algorithms.html#policy-gradients)\n",
    "* [Proximal Policy Optimization (PPO)](https://docs.ray.io/en/latest/rllib-algorithms.html#proximal-policy-optimization-ppo)\n",
    "* [Soft Actor-Critic (SAC)](https://ray.readthedocs.io/en/latest/rllib-algorithms.html#soft-actor-critic-sac)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient-free\n",
    "\n",
    "* [Augmented Random Search (ARS)](https://ray.readthedocs.io/en/latest/rllib-algorithms.html#augmented-random-search-ars)\n",
    "* [Evolution Strategies](https://ray.readthedocs.io/en/latest/rllib-algorithms.html#evolution-strategies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-agent Specific\n",
    "\n",
    "* [QMIX Monotonic Value Factorisation (QMIX, VDN, IQN)](https://ray.readthedocs.io/en/latest/rllib-algorithms.html#qmix-monotonic-value-factorisation-qmix-vdn-iqn)\n",
    "* [Multi-Agent Deep Deterministic Policy Gradient (contrib/MADDPG)](https://docs.ray.io/en/latest/rllib-algorithms.html#multi-agent-deep-deterministic-policy-gradient-contrib-maddpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Offline\n",
    "\n",
    "* [Advantage Re-Weighted Imitation Learning (MARWIL)](https://ray.readthedocs.io/en/latest/rllib-algorithms.html#advantage-re-weighted-imitation-learning-marwil)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contextual Bandits (contrib/bandits)\n",
    "\n",
    "* [Linear Upper Confidence Bound (contrib/LinUCB)](https://docs.ray.io/en/latest/rllib-algorithms.html#linear-upper-confidence-bound-contrib-linucb)\n",
    "* [Linear Thompson Sampling (contrib/LinTS)](https://docs.ray.io/en/latest/rllib-algorithms.html#linear-thompson-sampling-contrib-lints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other\n",
    "\n",
    "* [Single-Player Alpha Zero (contrib/AlphaZero)](https://docs.ray.io/en/latest/rllib-algorithms.html#single-player-alpha-zero-contrib-alphazero)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the [Overview](00-Ray-RLlib-Overview.ipynb) for recommendations on which lessons to study next."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Copy of RLlib Tutorial",
   "provenance": []
  },
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
