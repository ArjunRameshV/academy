{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ray RLlib - Introduction to RLlib\n",
    "\n",
    "Â© 2019-2020, Anyscale. All Rights Reserved\n",
    "\n",
    "![Anyscale Academy](../images/AnyscaleAcademy_Logo_clearbanner_141x100.png)\n",
    "\n",
    "In the [previous lesson](01-Introduction-to-Reinforcement-Learning.ipynb), we learned the basic concepts of reinforcement learning, with a \"taste\" of [RLlib](https://rllib.io) and [OpenAI Gym](https://gym.openai.com). This lesson takes a step back to provide more information about RLlib and the features it provides. The subsequent lessons will continue our exploration of RL algorithms and tools.\n",
    "\n",
    "For more detailed information about RLlib and its open source community, see the following:\n",
    "\n",
    "* [rllib.io](http::/rllib.io) (the documentation)\n",
    "* [GitHub repo](https://github.com/ray-project/ray/tree/master/rllib#rllib-scalable-reinforcement-learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RLlib is structured conceptually like this:\n",
    "\n",
    "![RLlib Stack](../images/rllib/RLlib-stack.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The _(1) Application Support_ boxes are components used for particular RL algorithms. The _(2) Abstractions for RL_ provide building blocks used by the many algorithms that are implemented in RLlib (listed below). They also provide hooks for implementing your own algorithms. RLlib leverages Ray for efficient, cluster-wide, _(3) Distributed Execution_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RLlib in 60 Seconds\n",
    "\n",
    "Here is a fast introduction to using RLlib from a command line, adapted from the [documentation](https://docs.ray.io/en/latest/rllib.html#rllib-in-60-seconds).\n",
    "\n",
    "First install [PyTorch](http://pytorch.org/) or [TensorFlow](https://www.tensorflow.org/), whichever you prefer. Then install RLlib:\n",
    "\n",
    "```shell\n",
    "pip install ray[rllib]  # or consider using: ray[debug]\n",
    "```\n",
    "\n",
    "Then try training _CartPole_ using the `rllib` CLI:\n",
    "\n",
    "```shell\n",
    "rllib train --run=PPO --env=CartPole-v0  # -v [-vv] for verbose,\n",
    "                                         # --eager [--trace] for eager execution,\n",
    "                                         # --torch to use PyTorch\n",
    "```\n",
    "\n",
    "The `rllib` CLI has a `--help` flag that prints details about the supported options.\n",
    "\n",
    "You can execute the same logic (but will different output) using the following Python code:\n",
    "\n",
    "```python\n",
    "from ray import tune\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "tune.run(PPOTrainer, config={\"env\": \"CartPole-v0\"})  # \"log_level\": \"INFO\" for verbose,\n",
    "                                                     # \"eager\": True for eager execution,\n",
    "                                                     # \"torch\": True for PyTorch\n",
    "```\n",
    "\n",
    "You can view the training results, which are written to the directory `$HOME/ray_results` using TensorBoard:\n",
    "\n",
    "```shell\n",
    "tensorboard --logdir=~/ray_results\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try them if you like.\n",
    "\n",
    "> **WARNING:** The next two cells will run for **a very long time**, especially on small machines or VMs.\n",
    "\n",
    "> **Tip:** The output will be long. In each cell, right click and select _Enable scrolling for outputs_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!rllib train --run=PPO --env=CartPole-v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import tune\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "tune.run(PPOTrainer, config={\"env\": \"CartPole-v0\"})  # \"log_level\": \"INFO\" for verbose,\n",
    "                                                     # \"eager\": True for eager execution,\n",
    "                                                     # \"torch\": True for PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir=~/ray_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Concepts: Policies, Samples, and Trainers\n",
    "\n",
    "### Policies\n",
    "\n",
    "[Policies](https://docs.ray.io/en/latest/rllib-concepts.html#policies) are Python classes that define how an agent acts in an environment.\n",
    "\n",
    "[Rollout workers](https://docs.ray.io/en/latest/rllib-concepts.html#policy-evaluation) query the policy to determine agent actions. \n",
    "\n",
    "In a [gym](https://docs.ray.io/en/latest/rllib-env.html#openai-gym) environment, there is a single agent and policy. In [vector environments](https://docs.ray.io/en/latest/rllib-env.html#vectorized), policy inference is for multiple agents at once, and in [multi-agent and hierachical environments](https://docs.ray.io/en/latest/rllib-env.html#multi-agent-and-hierarchical), there may be multiple policies, each controlling one or more agents.\n",
    "\n",
    "![Environments and Policies in RLlib](../images/rllib/multi-flat.svg)\n",
    "\n",
    "Policies can be implemented using any framework ([code](https://github.com/ray-project/ray/blob/master/rllib/policy/policy.py)). However, for TensorFlow and PyTorch, RLlib has [build_tf_policy](https://docs.ray.io/en/latest/rllib-concepts.html#building-policies-in-tensorflow) and [build_torch_policy](https://docs.ray.io/en/latest/rllib-concepts.html#building-policies-in-pytorch) helper functions, respectively, that let you define a trainable policy with a functional-style API. An example is in the [documentation](https://docs.ray.io/en/latest/rllib.html#policies)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Batches\n",
    "\n",
    "From single processes to [large clusters](https://docs.ray.io/en/latest/rllib-training.html#specifying-resources>), all data interchange in RLlib uses [sample batches](https://github.com/ray-project/ray/blob/master/rllib/policy/sample_batch.py). Sample batches encode one or more fragments of a trajectory. Typically, RLlib collects batches of size `rollout_fragment_length` from rollout workers, and concatenates one or more of these batches into a batch of size `train_batch_size` that is the input to SGD.\n",
    "\n",
    "A typical sample batch looks something like the following when summarized. Since all values are kept in arrays, this allows for efficient encoding and transmission across the network:\n",
    "\n",
    "```python\n",
    " { 'action_logp': np.ndarray((200,), dtype=float32, min=-0.701, max=-0.685, mean=-0.694),\n",
    "   'actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.495),\n",
    "   'dones': np.ndarray((200,), dtype=bool, min=0.0, max=1.0, mean=0.055),\n",
    "   'infos': np.ndarray((200,), dtype=object, head={}),\n",
    "   'new_obs': np.ndarray((200, 4), dtype=float32, min=-2.46, max=2.259, mean=0.018),\n",
    "   'obs': np.ndarray((200, 4), dtype=float32, min=-2.46, max=2.259, mean=0.016),\n",
    "   'rewards': np.ndarray((200,), dtype=float32, min=1.0, max=1.0, mean=1.0),\n",
    "   't': np.ndarray((200,), dtype=int64, min=0.0, max=34.0, mean=9.14)}\n",
    "```\n",
    "\n",
    "In [multi-agent mode](https://docs.ray.io/en/latest/rllib-concepts.html#policies-in-multi-agent), sample batches are collected separately for each individual policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Policies each define a `learn_on_batch()` method that improves the policy given a sample batch of input. For TensorFlow and PyTorch policies, this is implemented using a _loss function_ that takes as input sample batch tensors and outputs a scalar loss value. Here are a few example loss functions:\n",
    "\n",
    "* Simple [policy gradient loss](https://github.com/ray-project/ray/blob/master/rllib/agents/pg/pg_tf_policy.py).\n",
    "* Simple [Q-function loss](https://github.com/ray-project/ray/blob/a1d2e1762325cd34e14dc411666d63bb15d6eaf0/rllib/agents/dqn/simple_q_policy.py#L136)\n",
    "* Importance-weighted [APPO surrogate loss](https://github.com/ray-project/ray/blob/master/rllib/agents/ppo/appo_policy.py)\n",
    "\n",
    "RLlib [trainer classes](https://docs.ray.io/en/latest/rllib-concepts.html#trainers) coordinate the distributed workflow of running rollouts and optimizing policies. They do this by leveraging Ray [parallel iterators](https://docs.ray.io/en/latest/iter.html) to implement the desired computation pattern. The following figure shows *synchronous sampling*, the simplest of [these patterns](https://docs.ray.io/en/latest/rllib-algorithms.html):\n",
    "\n",
    "![Synchronous Sampling](../images/rllib/a2c-arch.svg)\n",
    "\n",
    "    Synchronous Sampling (e.g., A2C, PG, PPO)\n",
    "\n",
    "RLlib uses [Ray actors](https://docs.ray.io/en/latest/actors.html) to scale training from a single core to many thousands of cores in a cluster. You can [configure the parallelism](https://docs.ray.io/en/latest/rllib-training.html#specifying-resources) used for training by changing the `num_workers` parameter. Check out our [scaling guide](https://docs.ray.io/en/latest/rllib-training.html#scaling-guide) for more details here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application Support\n",
    "\n",
    "Beyond environments defined in Python, [RLlib supports]((https://docs.ray.io/en/latest/rllib.html#application-support)) batch training on [offline datasets](https://docs.ray.io/en/latest/rllib-offline.html), and also provides a variety of integration strategies for [external applications](https://docs.ray.io/en/latest/rllib-env.html#external-agents-and-applications).\n",
    "\n",
    "### Customization\n",
    "\n",
    "RLlib provides ways to customize almost all aspects of training, including the [environmen](https://docs.ray.io/en/latest/rllib-env.html#configuring-environments), [neural network model](https://docs.ray.io/en/latest/rllib-models.html#tensorflow-models), [action distribution](https://docs.ray.io/en/latest/rllib-models.html#custom-action-distributions), and [policy definitions](https://docs.ray.io/en/latest/rllib-concepts.html#policies>).\n",
    "\n",
    "![RLlib components](../images/rllib/RLlib-components.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms Implemented in RLlib\n",
    "\n",
    "Here is the current list of supported algorithms in RLlib. The links go to the corresponding RLlib documentation, which includes links to the original papers and other references.\n",
    "\n",
    "See also the documentation's [Feature Compatibility Matrix](https://docs.ray.io/en/latest/rllib-algorithms.html#feature-compatibility-matrix), which lists the algorithms and useful properties for them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High-throughput Architectures\n",
    "\n",
    "* [Distributed Prioritized Experience Replay (Ape-X)](https://ray.readthedocs.io/en/latest/rllib-algorithms.html#distributed-prioritized-experience-replay-ape-x)\n",
    "* [Importance Weighted Actor-Learner Architecture (IMPALA)](https://ray.readthedocs.io/en/latest/rllib-algorithms.html#importance-weighted-actor-learner-architecture-impala)\n",
    "* [Asynchronous Proximal Policy Optimization (APPO)](https://ray.readthedocs.io/en/latest/rllib-algorithms.html#asynchronous-proximal-policy-optimization-appo)\n",
    "* [Decentralized Distributed Proximal Policy Optimization (DD-PPO)](https://docs.ray.io/en/latest/rllib-algorithms.html#decentralized-distributed-proximal-policy-optimization-dd-ppo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient-based\n",
    "\n",
    "* [Advantage Actor-Critic (A2C, A3C)](https://ray.readthedocs.io/en/latest/rllib-algorithms.html#advantage-actor-critic-a2c-a3c)\n",
    "* [Deep Deterministic Policy Gradients (DDPG, TD3)](https://ray.readthedocs.io/en/latest/rllib-algorithms.html#deep-deterministic-policy-gradients-ddpg-td3)\n",
    "* [Deep Q Networks (DQN, Rainbow, Parametric DQN)](https://ray.readthedocs.io/en/latest/rllib-algorithms.html#deep-q-networks-dqn-rainbow-parametric-dqn)\n",
    "* [Policy Gradients](https://ray.readthedocs.io/en/latest/rllib-algorithms.html#policy-gradients)\n",
    "* [Proximal Policy Optimization (PPO)](https://docs.ray.io/en/latest/rllib-algorithms.html#proximal-policy-optimization-ppo)\n",
    "* [Soft Actor-Critic (SAC)](https://ray.readthedocs.io/en/latest/rllib-algorithms.html#soft-actor-critic-sac)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient-free\n",
    "\n",
    "* [Augmented Random Search (ARS)](https://ray.readthedocs.io/en/latest/rllib-algorithms.html#augmented-random-search-ars)\n",
    "* [Evolution Strategies](https://ray.readthedocs.io/en/latest/rllib-algorithms.html#evolution-strategies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-agent Specific\n",
    "\n",
    "* [QMIX Monotonic Value Factorisation (QMIX, VDN, IQN)](https://ray.readthedocs.io/en/latest/rllib-algorithms.html#qmix-monotonic-value-factorisation-qmix-vdn-iqn)\n",
    "* [Multi-Agent Deep Deterministic Policy Gradient (contrib/MADDPG)](https://docs.ray.io/en/latest/rllib-algorithms.html#multi-agent-deep-deterministic-policy-gradient-contrib-maddpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Offline\n",
    "\n",
    "* [Advantage Re-Weighted Imitation Learning (MARWIL)](https://ray.readthedocs.io/en/latest/rllib-algorithms.html#advantage-re-weighted-imitation-learning-marwil)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contextual Bandits (contrib/bandits)\n",
    "\n",
    "* [Linear Upper Confidence Bound (contrib/LinUCB)](https://docs.ray.io/en/latest/rllib-algorithms.html#linear-upper-confidence-bound-contrib-linucb)\n",
    "* [Linear Thompson Sampling (contrib/LinTS)](https://docs.ray.io/en/latest/rllib-algorithms.html#linear-thompson-sampling-contrib-lints)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other\n",
    "\n",
    "* [Single-Player Alpha Zero (contrib/AlphaZero)](https://docs.ray.io/en/latest/rllib-algorithms.html#single-player-alpha-zero-contrib-alphazero)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next lesson, [03: Application: Cart Pole](03-Application-Cart-Pole.ipynb) returns to the _cart pole_ example, where we train a moving car to balance a vertical pole. Based on the `CartPole-v0` environment from OpenAI Gym."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Copy of RLlib Tutorial",
   "provenance": []
  },
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
