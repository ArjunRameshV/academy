{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ray RLlib - Multi-Armed Bandits - Exercise Solutions\n",
    "\n",
    "Â© 2019-2020, Anyscale. All Rights Reserved\n",
    "\n",
    "![Anyscale Academy](../../../images/AnyscaleAcademy_Logo_clearbanner_141x100.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore a very simple contextual bandit example with 3 arms. We'll run trials using RLlib and [Tune](http://tune.io), Ray's hyperparameter tuning library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.spaces import Discrete, Box\n",
    "import numpy as np\n",
    "import random\n",
    "from ray import tune\n",
    "from ray.tune.progress_reporter import JupyterNotebookReporter\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03: Simple Multi-Armed Bandits - Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, set up a function to generate the rewards for n arms. To keep it somewhat simple, just use the original rewards for -1 in `SimpleBandit`, `[-10,0,10]` and repeat it as much as necessary, and optionally offset the start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleContextualBandit2 (gym.Env):\n",
    "    def __init__ (self, config=None):\n",
    "        self.action_space = Discrete(3)     # 3 arms\n",
    "        self.observation_space = Box(low=-1., high=1., shape=(2, ), dtype=np.float64)  # Random (x,y), where x,y from -1 to 1\n",
    "        self.current_context = None\n",
    "        self.rewards_for_context = {\n",
    "            -1.: [-10, 0, 10],\n",
    "            1.: [10, 0, -10],\n",
    "        }\n",
    "\n",
    "    def reset (self):\n",
    "        self.current_context = random.choice([-1., 1.])\n",
    "        return np.array([-self.current_context, self.current_context])\n",
    "\n",
    "    def step (self, action):\n",
    "        reward = self.rewards_for_context[self.current_context][action]\n",
    "        self.current_context = random.choice([-1.,1.])\n",
    "        return (np.array([-self.current_context, self.current_context]), reward, True,\n",
    "                {\n",
    "                    \"regret\": 10 - reward\n",
    "                })\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'SimpleContextualBandit2(action_space={self.action_space}, observation_space={self.observation_space}, current_context={self.current_context}, rewards per context={self.rewards_for_context})'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Initial observation = [-1.  1.], bandit = SimpleContextualBandit2(action_space=Discrete(3), observation_space=Box(2,), current_context=1.0, rewards per context={-1.0: [-10, 0, 10], 1.0: [10, 0, -10]})'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bandit = SimpleContextualBandit2()\n",
    "observation = bandit.reset()\n",
    "f'Initial observation = {observation}, bandit = {repr(bandit)}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = {\n",
    "    \"training_iteration\": 200,\n",
    "    \"timesteps_total\": 100000,\n",
    "    \"episode_reward_mean\": 10.0,\n",
    "}\n",
    "\n",
    "config = {\n",
    "    \"env\": SimpleContextualBandit2,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-08 13:58:52,018\tINFO resource_spec.py:212 -- Starting Ray with 4.44 GiB memory available for workers and up to 2.22 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n",
      "2020-06-08 13:58:52,344\tINFO services.py:1170 -- View the Ray dashboard at \u001b[1m\u001b[32mlocalhost:8266\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/8 CPUs, 0/0 GPUs, 0.0/4.44 GiB heap, 0.0/1.51 GiB objects<br>Result logdir: /Users/deanwampler/ray_results/contrib/LinUCB<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                                  </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>contrib_LinUCB_SimpleContextualBandit2_00000</td><td>RUNNING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=13285)\u001b[0m 2020-06-08 13:59:00,475\tINFO trainer.py:421 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=13285)\u001b[0m 2020-06-08 13:59:00,478\tINFO trainer.py:580 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=13285)\u001b[0m 2020-06-08 13:59:00,486\tINFO trainable.py:217 -- Getting current IP.\n",
      "\u001b[2m\u001b[36m(pid=13285)\u001b[0m 2020-06-08 13:59:00,486\tWARNING util.py:37 -- Install gputil for GPU system monitoring.\n",
      "Result for contrib_LinUCB_SimpleContextualBandit2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-06-08_13-59-00\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.9\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 100\n",
      "  episodes_total: 100\n",
      "  experiment_id: 91d770ae39234ca1bfd77582362df8dc\n",
      "  experiment_tag: '0'\n",
      "  grad_time_ms: 0.246\n",
      "  hostname: DWAnyscaleMBP.local\n",
      "  info:\n",
      "    grad_time_ms: 0.246\n",
      "    learner:\n",
      "      cumulative_regret: 10.0\n",
      "      update_latency: 0.0001289844512939453\n",
      "    num_steps_sampled: 100\n",
      "    num_steps_trained: 100\n",
      "    opt_peak_throughput: 4070.954\n",
      "    opt_samples: 1.0\n",
      "    sample_peak_throughput: 1463.827\n",
      "    sample_time_ms: 0.683\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 1\n",
      "  learner:\n",
      "    cumulative_regret: 10.0\n",
      "    update_latency: 0.0001289844512939453\n",
      "  node_ip: 192.168.1.149\n",
      "  num_healthy_workers: 0\n",
      "  num_steps_sampled: 100\n",
      "  num_steps_trained: 100\n",
      "  off_policy_estimator: {}\n",
      "  opt_peak_throughput: 4070.954\n",
      "  opt_samples: 1.0\n",
      "  optimizer_steps_this_iter: 100\n",
      "  perf:\n",
      "    cpu_util_percent: 33.7\n",
      "    ram_util_percent: 58.1\n",
      "  pid: 13285\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sample_peak_throughput: 1463.827\n",
      "  sample_time_ms: 0.683\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.026233125441145186\n",
      "    mean_inference_ms: 0.39734698758266956\n",
      "    mean_processing_ms: 0.2800993400044961\n",
      "  time_since_restore: 0.10699796676635742\n",
      "  time_this_iter_s: 0.10699796676635742\n",
      "  time_total_s: 0.10699796676635742\n",
      "  timestamp: 1591649940\n",
      "  timesteps_since_restore: 100\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 100\n",
      "  training_iteration: 1\n",
      "  trial_id: '00000'\n",
      "  update_time_ms: 0.001\n",
      "  \n",
      "Result for contrib_LinUCB_SimpleContextualBandit2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-06-08_13-59-00\n",
      "  done: true\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 10.0\n",
      "  episode_reward_min: 10.0\n",
      "  episodes_this_iter: 100\n",
      "  episodes_total: 200\n",
      "  experiment_id: 91d770ae39234ca1bfd77582362df8dc\n",
      "  experiment_tag: '0'\n",
      "  grad_time_ms: 0.285\n",
      "  hostname: DWAnyscaleMBP.local\n",
      "  info:\n",
      "    grad_time_ms: 0.285\n",
      "    learner:\n",
      "      cumulative_regret: 10.0\n",
      "      update_latency: 0.0002300739288330078\n",
      "    num_steps_sampled: 200\n",
      "    num_steps_trained: 200\n",
      "    opt_peak_throughput: 3514.289\n",
      "    opt_samples: 1.0\n",
      "    sample_peak_throughput: 1118.72\n",
      "    sample_time_ms: 0.894\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 2\n",
      "  learner:\n",
      "    cumulative_regret: 10.0\n",
      "    update_latency: 0.0002300739288330078\n",
      "  node_ip: 192.168.1.149\n",
      "  num_healthy_workers: 0\n",
      "  num_steps_sampled: 200\n",
      "  num_steps_trained: 200\n",
      "  off_policy_estimator: {}\n",
      "  opt_peak_throughput: 3514.289\n",
      "  opt_samples: 1.0\n",
      "  optimizer_steps_this_iter: 100\n",
      "  perf: {}\n",
      "  pid: 13285\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sample_peak_throughput: 1118.72\n",
      "  sample_time_ms: 0.894\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.028240146921641788\n",
      "    mean_inference_ms: 0.4231502760702105\n",
      "    mean_processing_ms: 0.28765142260499255\n",
      "  time_since_restore: 0.2399148941040039\n",
      "  time_this_iter_s: 0.13291692733764648\n",
      "  time_total_s: 0.2399148941040039\n",
      "  timestamp: 1591649940\n",
      "  timesteps_since_restore: 200\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 200\n",
      "  training_iteration: 2\n",
      "  trial_id: '00000'\n",
      "  update_time_ms: 0.001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/0 GPUs, 0.0/4.44 GiB heap, 0.0/1.51 GiB objects<br>Result logdir: /Users/deanwampler/ray_results/contrib/LinUCB<br>Number of trials: 1 (1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                                  </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>contrib_LinUCB_SimpleContextualBandit2_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        0.239915</td><td style=\"text-align: right;\"> 200</td><td style=\"text-align: right;\">      10</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The trials took 8.765048027038574 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "analysis = tune.run(\"contrib/LinUCB\", config=config, stop=stop, \n",
    "                    progress_reporter=JupyterNotebookReporter(overwrite=False),  # This is the default, actually.\n",
    "                    verbose=2)  # Change to 0 or 1 to reduce the output.\n",
    "\n",
    "print(\"The trials took\", time.time() - start_time, \"seconds\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_reward_max</th>\n",
       "      <th>episode_reward_min</th>\n",
       "      <th>episode_reward_mean</th>\n",
       "      <th>episode_len_mean</th>\n",
       "      <th>episodes_this_iter</th>\n",
       "      <th>num_steps_trained</th>\n",
       "      <th>num_steps_sampled</th>\n",
       "      <th>sample_time_ms</th>\n",
       "      <th>grad_time_ms</th>\n",
       "      <th>update_time_ms</th>\n",
       "      <th>...</th>\n",
       "      <th>info/sample_peak_throughput</th>\n",
       "      <th>info/opt_samples</th>\n",
       "      <th>learner/cumulative_regret</th>\n",
       "      <th>learner/update_latency</th>\n",
       "      <th>perf/cpu_util_percent</th>\n",
       "      <th>perf/ram_util_percent</th>\n",
       "      <th>info/learner/cumulative_regret</th>\n",
       "      <th>info/learner/update_latency</th>\n",
       "      <th>config/env</th>\n",
       "      <th>logdir</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>0.894</td>\n",
       "      <td>0.285</td>\n",
       "      <td>0.001</td>\n",
       "      <td>...</td>\n",
       "      <td>1118.72</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.00023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.00023</td>\n",
       "      <td>&lt;class '__main__.SimpleContextualBandit2'&gt;</td>\n",
       "      <td>/Users/deanwampler/ray_results/contrib/LinUCB/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   episode_reward_max  episode_reward_min  episode_reward_mean  \\\n",
       "0                10.0                10.0                 10.0   \n",
       "\n",
       "   episode_len_mean  episodes_this_iter  num_steps_trained  num_steps_sampled  \\\n",
       "0               1.0                 100                200                200   \n",
       "\n",
       "   sample_time_ms  grad_time_ms  update_time_ms  ...  \\\n",
       "0           0.894         0.285           0.001  ...   \n",
       "\n",
       "   info/sample_peak_throughput  info/opt_samples  learner/cumulative_regret  \\\n",
       "0                      1118.72               1.0                       10.0   \n",
       "\n",
       "   learner/update_latency  perf/cpu_util_percent  perf/ram_util_percent  \\\n",
       "0                 0.00023                    NaN                    NaN   \n",
       "\n",
       "   info/learner/cumulative_regret  info/learner/update_latency  \\\n",
       "0                            10.0                      0.00023   \n",
       "\n",
       "                                   config/env  \\\n",
       "0  <class '__main__.SimpleContextualBandit2'>   \n",
       "\n",
       "                                              logdir  \n",
       "0  /Users/deanwampler/ray_results/contrib/LinUCB/...  \n",
       "\n",
       "[1 rows x 54 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = analysis.dataframe()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It trains just as easily as the original implementation that didn't switch contexts between steps. Is this surprising? Probably not, because the relationship between the reward and the context remains linear, so what LinUCB learns for one context is correct for the second context, too. Also, _Tune_ runs many episodes, so it studies both contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03: Simple Multi-Armed Bandits - Exercise 2\n",
    "\n",
    "Recall the `rewards_for_context` we used:\n",
    "\n",
    "```python\n",
    "self.rewards_for_context = {\n",
    "    -1.: [-10, 0, 10],\n",
    "    1.: [10, 0, -10],\n",
    "}\n",
    "```\n",
    "\n",
    "We said that Linear Upper Confidence Bound assumes a linear dependency between the expected reward of an action and its context. It models the representation space using a set of linear predictors.\n",
    "\n",
    "Change the values for the rewards as follows, so they no longer have the same simple linear relationship:\n",
    "\n",
    "```python\n",
    "self.rewards_for_context = {\n",
    "    -1.: [-10, 10, 0],\n",
    "    1.: [0, 10, -10],\n",
    "}\n",
    "```\n",
    "\n",
    "Also remove the change made for exercise 1, the line `self.current_context = random.choice([-1.,1.])` in the `step` method.\n",
    "\n",
    "Run the training again and look at the results for the reward mean in TensorBoard. How successful was the training? How smooth is the plot for `episode_reward_mean`? How many steps were taken in the training?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleContextualBanditNonlinear (gym.Env):\n",
    "    def __init__ (self, config=None):\n",
    "        self.action_space = Discrete(3)     # 3 arms\n",
    "        self.observation_space = Box(low=-1., high=1., shape=(2, ), dtype=np.float64)  # Random (x,y), where x,y from -1 to 1\n",
    "        self.current_context = None\n",
    "        self.rewards_for_context = {   # Changed here:\n",
    "            -1.: [-10, 10, 0],\n",
    "            1.: [0, 10, -10],\n",
    "        }\n",
    "\n",
    "    def reset (self):\n",
    "        self.current_context = random.choice([-1., 1.])\n",
    "        return np.array([-self.current_context, self.current_context])\n",
    "\n",
    "    def step (self, action):\n",
    "        reward = self.rewards_for_context[self.current_context][action]\n",
    "        return (np.array([-self.current_context, self.current_context]), reward, True,\n",
    "                {\n",
    "                    \"regret\": 10 - reward\n",
    "                })\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'SimpleContextualBanditNonlinear(action_space={self.action_space}, observation_space={self.observation_space}, current_context={self.current_context}, rewards per context={self.rewards_for_context})'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Initial observation = [ 1. -1.], bandit = SimpleContextualBanditNonlinear(action_space=Discrete(3), observation_space=Box(2,), current_context=-1.0, rewards per context={-1.0: [-10, 10, 0], 1.0: [0, 10, -10]})'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bandit = SimpleContextualBanditNonlinear()\n",
    "observation = bandit.reset()\n",
    "f'Initial observation = {observation}, bandit = {repr(bandit)}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation = [ 1. -1.], reward =    0, done = True , info = {'regret': 10}\n",
      "observation = [ 1. -1.], reward =   10, done = True , info = {'regret': 0}\n",
      "observation = [ 1. -1.], reward =   10, done = True , info = {'regret': 0}\n",
      "observation = [ 1. -1.], reward =   10, done = True , info = {'regret': 0}\n",
      "observation = [ 1. -1.], reward =  -10, done = True , info = {'regret': 20}\n",
      "observation = [ 1. -1.], reward =    0, done = True , info = {'regret': 10}\n",
      "observation = [ 1. -1.], reward =  -10, done = True , info = {'regret': 20}\n",
      "observation = [ 1. -1.], reward =    0, done = True , info = {'regret': 10}\n",
      "observation = [ 1. -1.], reward =    0, done = True , info = {'regret': 10}\n",
      "observation = [ 1. -1.], reward =   10, done = True , info = {'regret': 0}\n"
     ]
    }
   ],
   "source": [
    "print(f'current_context = {bandit.current_context}')\n",
    "for i in range(10):\n",
    "    action = bandit.action_space.sample()\n",
    "    observation, reward, done, info = bandit.step(action)\n",
    "    print(f'observation = {observation}, action = {action}, reward = {reward:4d}, done = {str(done):5s}, info = {info}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `stop` defined above is unchanged.\n",
    "\n",
    "config = {\n",
    "    \"env\": SimpleContextualBanditNonlinear,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/8 CPUs, 0/0 GPUs, 0.0/4.44 GiB heap, 0.0/1.51 GiB objects<br>Result logdir: /Users/deanwampler/ray_results/contrib/LinUCB<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                                          </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>contrib_LinUCB_SimpleContextualBanditNonlinear_00000</td><td>RUNNING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=13289)\u001b[0m 2020-06-08 14:01:10,502\tINFO trainer.py:421 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=13289)\u001b[0m 2020-06-08 14:01:10,505\tINFO trainer.py:580 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=13289)\u001b[0m 2020-06-08 14:01:10,512\tINFO trainable.py:217 -- Getting current IP.\n",
      "\u001b[2m\u001b[36m(pid=13289)\u001b[0m 2020-06-08 14:01:10,512\tWARNING util.py:37 -- Install gputil for GPU system monitoring.\n",
      "Result for contrib_LinUCB_SimpleContextualBanditNonlinear_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-06-08_14-01-10\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 5.4\n",
      "  episode_reward_min: -10.0\n",
      "  episodes_this_iter: 100\n",
      "  episodes_total: 100\n",
      "  experiment_id: 1ff2fc0067bd458aaaabe106df9751e1\n",
      "  experiment_tag: '0'\n",
      "  grad_time_ms: 0.255\n",
      "  hostname: DWAnyscaleMBP.local\n",
      "  info:\n",
      "    grad_time_ms: 0.255\n",
      "    learner:\n",
      "      cumulative_regret: 460.0\n",
      "      update_latency: 0.00013589859008789062\n",
      "    num_steps_sampled: 100\n",
      "    num_steps_trained: 100\n",
      "    opt_peak_throughput: 3925.781\n",
      "    opt_samples: 1.0\n",
      "    sample_peak_throughput: 1370.419\n",
      "    sample_time_ms: 0.73\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 1\n",
      "  learner:\n",
      "    cumulative_regret: 460.0\n",
      "    update_latency: 0.00013589859008789062\n",
      "  node_ip: 192.168.1.149\n",
      "  num_healthy_workers: 0\n",
      "  num_steps_sampled: 100\n",
      "  num_steps_trained: 100\n",
      "  off_policy_estimator: {}\n",
      "  opt_peak_throughput: 3925.781\n",
      "  opt_samples: 1.0\n",
      "  optimizer_steps_this_iter: 100\n",
      "  perf:\n",
      "    cpu_util_percent: 21.2\n",
      "    ram_util_percent: 66.1\n",
      "  pid: 13289\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sample_peak_throughput: 1370.419\n",
      "  sample_time_ms: 0.73\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.026556524899926506\n",
      "    mean_inference_ms: 0.40512509865335905\n",
      "    mean_processing_ms: 0.3051167667502224\n",
      "  time_since_restore: 0.11366605758666992\n",
      "  time_this_iter_s: 0.11366605758666992\n",
      "  time_total_s: 0.11366605758666992\n",
      "  timestamp: 1591650070\n",
      "  timesteps_since_restore: 100\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 100\n",
      "  training_iteration: 1\n",
      "  trial_id: '00000'\n",
      "  update_time_ms: 0.001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/8 CPUs, 0/0 GPUs, 0.0/4.44 GiB heap, 0.0/1.51 GiB objects<br>Result logdir: /Users/deanwampler/ray_results/contrib/LinUCB<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                                          </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>contrib_LinUCB_SimpleContextualBanditNonlinear_00000</td><td>RUNNING </td><td>192.168.1.149:13289</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         1.88235</td><td style=\"text-align: right;\">1700</td><td style=\"text-align: right;\">     5.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for contrib_LinUCB_SimpleContextualBanditNonlinear_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-06-08_14-01-15\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 5.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 100\n",
      "  episodes_total: 4400\n",
      "  experiment_id: 1ff2fc0067bd458aaaabe106df9751e1\n",
      "  experiment_tag: '0'\n",
      "  grad_time_ms: 0.306\n",
      "  hostname: DWAnyscaleMBP.local\n",
      "  info:\n",
      "    grad_time_ms: 0.306\n",
      "    learner:\n",
      "      cumulative_regret: 21780.0\n",
      "      update_latency: 0.0001728534698486328\n",
      "    num_steps_sampled: 4400\n",
      "    num_steps_trained: 4400\n",
      "    opt_peak_throughput: 3264.304\n",
      "    opt_samples: 1.0\n",
      "    sample_peak_throughput: 1297.462\n",
      "    sample_time_ms: 0.771\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 44\n",
      "  learner:\n",
      "    cumulative_regret: 21780.0\n",
      "    update_latency: 0.0001728534698486328\n",
      "  node_ip: 192.168.1.149\n",
      "  num_healthy_workers: 0\n",
      "  num_steps_sampled: 4400\n",
      "  num_steps_trained: 4400\n",
      "  off_policy_estimator: {}\n",
      "  opt_peak_throughput: 3264.304\n",
      "  opt_samples: 1.0\n",
      "  optimizer_steps_this_iter: 100\n",
      "  perf: {}\n",
      "  pid: 13289\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sample_peak_throughput: 1297.462\n",
      "  sample_time_ms: 0.771\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.026051820990335556\n",
      "    mean_inference_ms: 0.3962962092716188\n",
      "    mean_processing_ms: 0.27339584690363555\n",
      "  time_since_restore: 4.819295883178711\n",
      "  time_this_iter_s: 0.11878013610839844\n",
      "  time_total_s: 4.819295883178711\n",
      "  timestamp: 1591650075\n",
      "  timesteps_since_restore: 4400\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 4400\n",
      "  training_iteration: 44\n",
      "  trial_id: '00000'\n",
      "  update_time_ms: 0.001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/8 CPUs, 0/0 GPUs, 0.0/4.44 GiB heap, 0.0/1.51 GiB objects<br>Result logdir: /Users/deanwampler/ray_results/contrib/LinUCB<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                                          </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>contrib_LinUCB_SimpleContextualBanditNonlinear_00000</td><td>RUNNING </td><td>192.168.1.149:13289</td><td style=\"text-align: right;\">    60</td><td style=\"text-align: right;\">          6.5581</td><td style=\"text-align: right;\">6000</td><td style=\"text-align: right;\">     4.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for contrib_LinUCB_SimpleContextualBanditNonlinear_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-06-08_14-01-20\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 4.8\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 100\n",
      "  episodes_total: 8700\n",
      "  experiment_id: 1ff2fc0067bd458aaaabe106df9751e1\n",
      "  experiment_tag: '0'\n",
      "  grad_time_ms: 0.321\n",
      "  hostname: DWAnyscaleMBP.local\n",
      "  info:\n",
      "    grad_time_ms: 0.321\n",
      "    learner:\n",
      "      cumulative_regret: 42800.0\n",
      "      update_latency: 0.000209808349609375\n",
      "    num_steps_sampled: 8700\n",
      "    num_steps_trained: 8700\n",
      "    opt_peak_throughput: 3111.501\n",
      "    opt_samples: 1.0\n",
      "    sample_peak_throughput: 1437.045\n",
      "    sample_time_ms: 0.696\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 87\n",
      "  learner:\n",
      "    cumulative_regret: 42800.0\n",
      "    update_latency: 0.000209808349609375\n",
      "  node_ip: 192.168.1.149\n",
      "  num_healthy_workers: 0\n",
      "  num_steps_sampled: 8700\n",
      "  num_steps_trained: 8700\n",
      "  off_policy_estimator: {}\n",
      "  opt_peak_throughput: 3111.501\n",
      "  opt_samples: 1.0\n",
      "  optimizer_steps_this_iter: 100\n",
      "  perf: {}\n",
      "  pid: 13289\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sample_peak_throughput: 1437.045\n",
      "  sample_time_ms: 0.696\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.025820860902737816\n",
      "    mean_inference_ms: 0.3948773670273257\n",
      "    mean_processing_ms: 0.27368307360259037\n",
      "  time_since_restore: 9.56510329246521\n",
      "  time_this_iter_s: 0.11979913711547852\n",
      "  time_total_s: 9.56510329246521\n",
      "  timestamp: 1591650080\n",
      "  timesteps_since_restore: 8700\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 8700\n",
      "  training_iteration: 87\n",
      "  trial_id: '00000'\n",
      "  update_time_ms: 0.001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/8 CPUs, 0/0 GPUs, 0.0/4.44 GiB heap, 0.0/1.51 GiB objects<br>Result logdir: /Users/deanwampler/ray_results/contrib/LinUCB<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                                          </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>contrib_LinUCB_SimpleContextualBanditNonlinear_00000</td><td>RUNNING </td><td>192.168.1.149:13289</td><td style=\"text-align: right;\">   102</td><td style=\"text-align: right;\">         11.2425</td><td style=\"text-align: right;\">10200</td><td style=\"text-align: right;\">     5.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for contrib_LinUCB_SimpleContextualBanditNonlinear_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-06-08_14-01-25\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 5.4\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 100\n",
      "  episodes_total: 12800\n",
      "  experiment_id: 1ff2fc0067bd458aaaabe106df9751e1\n",
      "  experiment_tag: '0'\n",
      "  grad_time_ms: 0.406\n",
      "  hostname: DWAnyscaleMBP.local\n",
      "  info:\n",
      "    grad_time_ms: 0.406\n",
      "    learner:\n",
      "      cumulative_regret: 63530.0\n",
      "      update_latency: 0.00024175643920898438\n",
      "    num_steps_sampled: 12800\n",
      "    num_steps_trained: 12800\n",
      "    opt_peak_throughput: 2461.59\n",
      "    opt_samples: 1.0\n",
      "    sample_peak_throughput: 1320.625\n",
      "    sample_time_ms: 0.757\n",
      "    update_time_ms: 0.002\n",
      "  iterations_since_restore: 128\n",
      "  learner:\n",
      "    cumulative_regret: 63530.0\n",
      "    update_latency: 0.00024175643920898438\n",
      "  node_ip: 192.168.1.149\n",
      "  num_healthy_workers: 0\n",
      "  num_steps_sampled: 12800\n",
      "  num_steps_trained: 12800\n",
      "  off_policy_estimator: {}\n",
      "  opt_peak_throughput: 2461.59\n",
      "  opt_samples: 1.0\n",
      "  optimizer_steps_this_iter: 100\n",
      "  perf: {}\n",
      "  pid: 13289\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sample_peak_throughput: 1320.625\n",
      "  sample_time_ms: 0.757\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.025643193063377624\n",
      "    mean_inference_ms: 0.39395564478381084\n",
      "    mean_processing_ms: 0.2722754887087011\n",
      "  time_since_restore: 14.228108882904053\n",
      "  time_this_iter_s: 0.11670994758605957\n",
      "  time_total_s: 14.228108882904053\n",
      "  timestamp: 1591650085\n",
      "  timesteps_since_restore: 12800\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 12800\n",
      "  training_iteration: 128\n",
      "  trial_id: '00000'\n",
      "  update_time_ms: 0.002\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/8 CPUs, 0/0 GPUs, 0.0/4.44 GiB heap, 0.0/1.51 GiB objects<br>Result logdir: /Users/deanwampler/ray_results/contrib/LinUCB<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                                          </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>contrib_LinUCB_SimpleContextualBanditNonlinear_00000</td><td>RUNNING </td><td>192.168.1.149:13289</td><td style=\"text-align: right;\">   142</td><td style=\"text-align: right;\">          15.898</td><td style=\"text-align: right;\">14200</td><td style=\"text-align: right;\">     5.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for contrib_LinUCB_SimpleContextualBanditNonlinear_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-06-08_14-01-30\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 4.9\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 100\n",
      "  episodes_total: 16700\n",
      "  experiment_id: 1ff2fc0067bd458aaaabe106df9751e1\n",
      "  experiment_tag: '0'\n",
      "  grad_time_ms: 0.38\n",
      "  hostname: DWAnyscaleMBP.local\n",
      "  info:\n",
      "    grad_time_ms: 0.38\n",
      "    learner:\n",
      "      cumulative_regret: 83060.0\n",
      "      update_latency: 0.00027298927307128906\n",
      "    num_steps_sampled: 16700\n",
      "    num_steps_trained: 16700\n",
      "    opt_peak_throughput: 2634.613\n",
      "    opt_samples: 1.0\n",
      "    sample_peak_throughput: 1390.085\n",
      "    sample_time_ms: 0.719\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 167\n",
      "  learner:\n",
      "    cumulative_regret: 83060.0\n",
      "    update_latency: 0.00027298927307128906\n",
      "  node_ip: 192.168.1.149\n",
      "  num_healthy_workers: 0\n",
      "  num_steps_sampled: 16700\n",
      "  num_steps_trained: 16700\n",
      "  off_policy_estimator: {}\n",
      "  opt_peak_throughput: 2634.613\n",
      "  opt_samples: 1.0\n",
      "  optimizer_steps_this_iter: 100\n",
      "  perf: {}\n",
      "  pid: 13289\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sample_peak_throughput: 1390.085\n",
      "  sample_time_ms: 0.719\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.025714716435363848\n",
      "    mean_inference_ms: 0.39547637910673294\n",
      "    mean_processing_ms: 0.2726013592906816\n",
      "  time_since_restore: 18.9075710773468\n",
      "  time_this_iter_s: 0.1164698600769043\n",
      "  time_total_s: 18.9075710773468\n",
      "  timestamp: 1591650090\n",
      "  timesteps_since_restore: 16700\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 16700\n",
      "  training_iteration: 167\n",
      "  trial_id: '00000'\n",
      "  update_time_ms: 0.001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/8 CPUs, 0/0 GPUs, 0.0/4.44 GiB heap, 0.0/1.51 GiB objects<br>Result logdir: /Users/deanwampler/ray_results/contrib/LinUCB<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                                          </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>contrib_LinUCB_SimpleContextualBanditNonlinear_00000</td><td>RUNNING </td><td>192.168.1.149:13289</td><td style=\"text-align: right;\">   181</td><td style=\"text-align: right;\">         20.6512</td><td style=\"text-align: right;\">18100</td><td style=\"text-align: right;\">     5.2</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for contrib_LinUCB_SimpleContextualBanditNonlinear_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-06-08_14-01-35\n",
      "  done: true\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 5.7\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 100\n",
      "  episodes_total: 20000\n",
      "  experiment_id: 1ff2fc0067bd458aaaabe106df9751e1\n",
      "  experiment_tag: '0'\n",
      "  grad_time_ms: 0.445\n",
      "  hostname: DWAnyscaleMBP.local\n",
      "  info:\n",
      "    grad_time_ms: 0.445\n",
      "    learner:\n",
      "      cumulative_regret: 99230.0\n",
      "      update_latency: 0.0003299713134765625\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "    opt_peak_throughput: 2245.946\n",
      "    opt_samples: 1.0\n",
      "    sample_peak_throughput: 1244.97\n",
      "    sample_time_ms: 0.803\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 200\n",
      "  learner:\n",
      "    cumulative_regret: 99230.0\n",
      "    update_latency: 0.0003299713134765625\n",
      "  node_ip: 192.168.1.149\n",
      "  num_healthy_workers: 0\n",
      "  num_steps_sampled: 20000\n",
      "  num_steps_trained: 20000\n",
      "  off_policy_estimator: {}\n",
      "  opt_peak_throughput: 2245.946\n",
      "  opt_samples: 1.0\n",
      "  optimizer_steps_this_iter: 100\n",
      "  perf:\n",
      "    cpu_util_percent: 22.1\n",
      "    ram_util_percent: 66.1\n",
      "  pid: 13289\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sample_peak_throughput: 1244.97\n",
      "  sample_time_ms: 0.803\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.02568786450966758\n",
      "    mean_inference_ms: 0.3963143674977202\n",
      "    mean_processing_ms: 0.2727537749737574\n",
      "  time_since_restore: 22.98465323448181\n",
      "  time_this_iter_s: 0.12446212768554688\n",
      "  time_total_s: 22.98465323448181\n",
      "  timestamp: 1591650095\n",
      "  timesteps_since_restore: 20000\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 200\n",
      "  trial_id: '00000'\n",
      "  update_time_ms: 0.001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/0 GPUs, 0.0/4.44 GiB heap, 0.0/1.51 GiB objects<br>Result logdir: /Users/deanwampler/ray_results/contrib/LinUCB<br>Number of trials: 1 (1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                                          </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>contrib_LinUCB_SimpleContextualBanditNonlinear_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">   200</td><td style=\"text-align: right;\">         22.9847</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">     5.7</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The trials took 27.73778510093689 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "analysis = tune.run(\"contrib/LinUCB\", config=config, stop=stop, \n",
    "                    progress_reporter=JupyterNotebookReporter(overwrite=False),  # This is the default, actually.\n",
    "                    verbose=2)  # Change to 0 or 1 to reduce the output.\n",
    "\n",
    "print(\"The trials took\", time.time() - start_time, \"seconds\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_reward_max</th>\n",
       "      <th>episode_reward_min</th>\n",
       "      <th>episode_reward_mean</th>\n",
       "      <th>episode_len_mean</th>\n",
       "      <th>episodes_this_iter</th>\n",
       "      <th>num_steps_trained</th>\n",
       "      <th>num_steps_sampled</th>\n",
       "      <th>sample_time_ms</th>\n",
       "      <th>grad_time_ms</th>\n",
       "      <th>update_time_ms</th>\n",
       "      <th>...</th>\n",
       "      <th>info/sample_peak_throughput</th>\n",
       "      <th>info/opt_samples</th>\n",
       "      <th>learner/cumulative_regret</th>\n",
       "      <th>learner/update_latency</th>\n",
       "      <th>perf/cpu_util_percent</th>\n",
       "      <th>perf/ram_util_percent</th>\n",
       "      <th>info/learner/cumulative_regret</th>\n",
       "      <th>info/learner/update_latency</th>\n",
       "      <th>config/env</th>\n",
       "      <th>logdir</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100</td>\n",
       "      <td>20000</td>\n",
       "      <td>20000</td>\n",
       "      <td>0.803</td>\n",
       "      <td>0.445</td>\n",
       "      <td>0.001</td>\n",
       "      <td>...</td>\n",
       "      <td>1244.97</td>\n",
       "      <td>1.0</td>\n",
       "      <td>99230.0</td>\n",
       "      <td>0.00033</td>\n",
       "      <td>22.1</td>\n",
       "      <td>66.1</td>\n",
       "      <td>99230.0</td>\n",
       "      <td>0.00033</td>\n",
       "      <td>&lt;class '__main__.SimpleContextualBanditNonline...</td>\n",
       "      <td>/Users/deanwampler/ray_results/contrib/LinUCB/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   episode_reward_max  episode_reward_min  episode_reward_mean  \\\n",
       "0                10.0                 0.0                  5.7   \n",
       "\n",
       "   episode_len_mean  episodes_this_iter  num_steps_trained  num_steps_sampled  \\\n",
       "0               1.0                 100              20000              20000   \n",
       "\n",
       "   sample_time_ms  grad_time_ms  update_time_ms  ...  \\\n",
       "0           0.803         0.445           0.001  ...   \n",
       "\n",
       "   info/sample_peak_throughput  info/opt_samples  learner/cumulative_regret  \\\n",
       "0                      1244.97               1.0                    99230.0   \n",
       "\n",
       "   learner/update_latency  perf/cpu_util_percent  perf/ram_util_percent  \\\n",
       "0                 0.00033                   22.1                   66.1   \n",
       "\n",
       "   info/learner/cumulative_regret  info/learner/update_latency  \\\n",
       "0                         99230.0                      0.00033   \n",
       "\n",
       "                                          config/env  \\\n",
       "0  <class '__main__.SimpleContextualBanditNonline...   \n",
       "\n",
       "                                              logdir  \n",
       "0  /Users/deanwampler/ray_results/contrib/LinUCB/...  \n",
       "\n",
       "[1 rows x 54 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = analysis.dataframe()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It ran the maximum of 20,000 steps and the best it does is about 4.8, not 10.0. the `episode_reward_mean` is chaotic:\n",
    "\n",
    "![Nonlinear model with LinUCB](../../../images/rllib/TensorBoard2.png).\n",
    "\n",
    "Because LinUCB expcts a linear relationship between the context and each reward, it's not surprising that it fails to converge to the desired reward mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03: Simple Multi-Armed Bandits - Exercise 3\n",
    "\n",
    "We briefly discussed another algorithm for selecting the next action, _Thompson Sampling_, in the [previous lesson](../02-Exploration-vs-Exploitation-Strategies.ipynb). Repeat exercises 1 and 2 using linear version, called _Linear Thompson Sampling_ ([RLlib documentation](https://docs.ray.io/en/latest/rllib-algorithms.html?highlight=greedy#linear-thompson-sampling-contrib-lints)). To make this change, look at this code we used above:\n",
    "\n",
    "```python\n",
    "analysis = tune.run(\"contrib/LinUCB\", config=config, stop=stop, \n",
    "                    progress_reporter=JupyterNotebookReporter(overwrite=False),  # This is the default, actually.\n",
    "                    verbose=2)  # Change to 0 or 1 to reduce the output.\n",
    "```\n",
    "\n",
    "Change `contrib/LinUCB` to `contrib/LinTS`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bandit = SimpleContextualBandit2()\n",
    "observation = bandit.reset()\n",
    "\n",
    "# `stop` defined above is unchanged.\n",
    "\n",
    "config = {\n",
    "    \"env\": SimpleContextualBandit2,\n",
    "}\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "analysis = tune.run(\"contrib/LinTS\", config=config, stop=stop, \n",
    "                    progress_reporter=JupyterNotebookReporter(overwrite=False),  # This is the default, actually.\n",
    "                    verbose=2)  # Change to 0 or 1 to reduce the output.\n",
    "\n",
    "print(\"The trials took\", time.time() - start_time, \"seconds\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/8 CPUs, 0/0 GPUs, 0.0/4.44 GiB heap, 0.0/1.51 GiB objects<br>Result logdir: /Users/deanwampler/ray_results/contrib/LinTS<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                                 </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>contrib_LinTS_SimpleContextualBandit2_00000</td><td>RUNNING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=13291)\u001b[0m 2020-06-08 14:02:51,052\tINFO trainer.py:421 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=13291)\u001b[0m 2020-06-08 14:02:51,056\tINFO trainer.py:580 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=13291)\u001b[0m 2020-06-08 14:02:51,063\tINFO trainable.py:217 -- Getting current IP.\n",
      "\u001b[2m\u001b[36m(pid=13291)\u001b[0m 2020-06-08 14:02:51,063\tWARNING util.py:37 -- Install gputil for GPU system monitoring.\n",
      "Result for contrib_LinTS_SimpleContextualBandit2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-06-08_14-02-51\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.9\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 100\n",
      "  episodes_total: 100\n",
      "  experiment_id: 481647293a464969a3d73baab12f468b\n",
      "  experiment_tag: '0'\n",
      "  grad_time_ms: 0.25\n",
      "  hostname: DWAnyscaleMBP.local\n",
      "  info:\n",
      "    grad_time_ms: 0.25\n",
      "    learner:\n",
      "      cumulative_regret: 10.0\n",
      "      update_latency: 0.00013208389282226562\n",
      "    num_steps_sampled: 100\n",
      "    num_steps_trained: 100\n",
      "    opt_peak_throughput: 3997.24\n",
      "    opt_samples: 1.0\n",
      "    sample_peak_throughput: 1419.632\n",
      "    sample_time_ms: 0.704\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 1\n",
      "  learner:\n",
      "    cumulative_regret: 10.0\n",
      "    update_latency: 0.00013208389282226562\n",
      "  node_ip: 192.168.1.149\n",
      "  num_healthy_workers: 0\n",
      "  num_steps_sampled: 100\n",
      "  num_steps_trained: 100\n",
      "  off_policy_estimator: {}\n",
      "  opt_peak_throughput: 3997.24\n",
      "  opt_samples: 1.0\n",
      "  optimizer_steps_this_iter: 100\n",
      "  perf:\n",
      "    cpu_util_percent: 20.7\n",
      "    ram_util_percent: 66.2\n",
      "  pid: 13291\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sample_peak_throughput: 1419.632\n",
      "  sample_time_ms: 0.704\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.027198602657506954\n",
      "    mean_inference_ms: 0.392460587001083\n",
      "    mean_processing_ms: 0.2814212647995147\n",
      "  time_since_restore: 0.10828185081481934\n",
      "  time_this_iter_s: 0.10828185081481934\n",
      "  time_total_s: 0.10828185081481934\n",
      "  timestamp: 1591650171\n",
      "  timesteps_since_restore: 100\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 100\n",
      "  training_iteration: 1\n",
      "  trial_id: '00000'\n",
      "  update_time_ms: 0.001\n",
      "  \n",
      "Result for contrib_LinTS_SimpleContextualBandit2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-06-08_14-02-51\n",
      "  done: true\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 10.0\n",
      "  episode_reward_min: 10.0\n",
      "  episodes_this_iter: 100\n",
      "  episodes_total: 200\n",
      "  experiment_id: 481647293a464969a3d73baab12f468b\n",
      "  experiment_tag: '0'\n",
      "  grad_time_ms: 0.232\n",
      "  hostname: DWAnyscaleMBP.local\n",
      "  info:\n",
      "    grad_time_ms: 0.232\n",
      "    learner:\n",
      "      cumulative_regret: 10.0\n",
      "      update_latency: 0.00013113021850585938\n",
      "    num_steps_sampled: 200\n",
      "    num_steps_trained: 200\n",
      "    opt_peak_throughput: 4304.057\n",
      "    opt_samples: 1.0\n",
      "    sample_peak_throughput: 1366.312\n",
      "    sample_time_ms: 0.732\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 2\n",
      "  learner:\n",
      "    cumulative_regret: 10.0\n",
      "    update_latency: 0.00013113021850585938\n",
      "  node_ip: 192.168.1.149\n",
      "  num_healthy_workers: 0\n",
      "  num_steps_sampled: 200\n",
      "  num_steps_trained: 200\n",
      "  off_policy_estimator: {}\n",
      "  opt_peak_throughput: 4304.057\n",
      "  opt_samples: 1.0\n",
      "  optimizer_steps_this_iter: 100\n",
      "  perf: {}\n",
      "  pid: 13291\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sample_peak_throughput: 1366.312\n",
      "  sample_time_ms: 0.732\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.027585385450676302\n",
      "    mean_inference_ms: 0.3913362227862153\n",
      "    mean_processing_ms: 0.2777339214116187\n",
      "  time_since_restore: 0.21352815628051758\n",
      "  time_this_iter_s: 0.10524630546569824\n",
      "  time_total_s: 0.21352815628051758\n",
      "  timestamp: 1591650171\n",
      "  timesteps_since_restore: 200\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 200\n",
      "  training_iteration: 2\n",
      "  trial_id: '00000'\n",
      "  update_time_ms: 0.001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/0 GPUs, 0.0/4.44 GiB heap, 0.0/1.51 GiB objects<br>Result logdir: /Users/deanwampler/ray_results/contrib/LinTS<br>Number of trials: 1 (1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                                 </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>contrib_LinTS_SimpleContextualBandit2_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        0.213528</td><td style=\"text-align: right;\"> 200</td><td style=\"text-align: right;\">      10</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The trials took 3.132218837738037 seconds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_reward_max</th>\n",
       "      <th>episode_reward_min</th>\n",
       "      <th>episode_reward_mean</th>\n",
       "      <th>episode_len_mean</th>\n",
       "      <th>episodes_this_iter</th>\n",
       "      <th>num_steps_trained</th>\n",
       "      <th>num_steps_sampled</th>\n",
       "      <th>sample_time_ms</th>\n",
       "      <th>grad_time_ms</th>\n",
       "      <th>update_time_ms</th>\n",
       "      <th>...</th>\n",
       "      <th>info/sample_peak_throughput</th>\n",
       "      <th>info/opt_samples</th>\n",
       "      <th>learner/cumulative_regret</th>\n",
       "      <th>learner/update_latency</th>\n",
       "      <th>perf/cpu_util_percent</th>\n",
       "      <th>perf/ram_util_percent</th>\n",
       "      <th>info/learner/cumulative_regret</th>\n",
       "      <th>info/learner/update_latency</th>\n",
       "      <th>config/env</th>\n",
       "      <th>logdir</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>0.732</td>\n",
       "      <td>0.232</td>\n",
       "      <td>0.001</td>\n",
       "      <td>...</td>\n",
       "      <td>1366.312</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>&lt;class '__main__.SimpleContextualBandit2'&gt;</td>\n",
       "      <td>/Users/deanwampler/ray_results/contrib/LinTS/c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   episode_reward_max  episode_reward_min  episode_reward_mean  \\\n",
       "0                10.0                10.0                 10.0   \n",
       "\n",
       "   episode_len_mean  episodes_this_iter  num_steps_trained  num_steps_sampled  \\\n",
       "0               1.0                 100                200                200   \n",
       "\n",
       "   sample_time_ms  grad_time_ms  update_time_ms  ...  \\\n",
       "0           0.732         0.232           0.001  ...   \n",
       "\n",
       "   info/sample_peak_throughput  info/opt_samples  learner/cumulative_regret  \\\n",
       "0                     1366.312               1.0                       10.0   \n",
       "\n",
       "   learner/update_latency  perf/cpu_util_percent  perf/ram_util_percent  \\\n",
       "0                0.000131                    NaN                    NaN   \n",
       "\n",
       "   info/learner/cumulative_regret  info/learner/update_latency  \\\n",
       "0                            10.0                     0.000131   \n",
       "\n",
       "                                   config/env  \\\n",
       "0  <class '__main__.SimpleContextualBandit2'>   \n",
       "\n",
       "                                              logdir  \n",
       "0  /Users/deanwampler/ray_results/contrib/LinTS/c...  \n",
       "\n",
       "[1 rows x 54 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = analysis.dataframe()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, the training only takes 200 steps and converge to the desired reward mean of `10.0`.\n",
    "\n",
    "Now let's try the nonlinear bandit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandit = SimpleContextualBanditNonlinear()\n",
    "observation = bandit.reset()\n",
    "\n",
    "# `stop` defined above is unchanged.\n",
    "\n",
    "config = {\n",
    "    \"env\": SimpleContextualBanditNonlinear,\n",
    "}\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "analysis = tune.run(\"contrib/LinTS\", config=config, stop=stop, \n",
    "                    progress_reporter=JupyterNotebookReporter(overwrite=False),  # This is the default, actually.\n",
    "                    verbose=2)  # Change to 0 or 1 to reduce the output.\n",
    "\n",
    "print(\"The trials took\", time.time() - start_time, \"seconds\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_reward_max</th>\n",
       "      <th>episode_reward_min</th>\n",
       "      <th>episode_reward_mean</th>\n",
       "      <th>episode_len_mean</th>\n",
       "      <th>episodes_this_iter</th>\n",
       "      <th>num_steps_trained</th>\n",
       "      <th>num_steps_sampled</th>\n",
       "      <th>sample_time_ms</th>\n",
       "      <th>grad_time_ms</th>\n",
       "      <th>update_time_ms</th>\n",
       "      <th>...</th>\n",
       "      <th>info/sample_peak_throughput</th>\n",
       "      <th>info/opt_samples</th>\n",
       "      <th>learner/cumulative_regret</th>\n",
       "      <th>learner/update_latency</th>\n",
       "      <th>perf/cpu_util_percent</th>\n",
       "      <th>perf/ram_util_percent</th>\n",
       "      <th>info/learner/cumulative_regret</th>\n",
       "      <th>info/learner/update_latency</th>\n",
       "      <th>config/env</th>\n",
       "      <th>logdir</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.0</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100</td>\n",
       "      <td>20000</td>\n",
       "      <td>20000</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.446</td>\n",
       "      <td>0.003</td>\n",
       "      <td>...</td>\n",
       "      <td>1350.692</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100750.0</td>\n",
       "      <td>0.00042</td>\n",
       "      <td>16.9</td>\n",
       "      <td>68.7</td>\n",
       "      <td>100750.0</td>\n",
       "      <td>0.00042</td>\n",
       "      <td>&lt;class '__main__.SimpleContextualBanditNonline...</td>\n",
       "      <td>/Users/deanwampler/ray_results/contrib/LinTS/c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   episode_reward_max  episode_reward_min  episode_reward_mean  \\\n",
       "0                10.0               -10.0                  4.5   \n",
       "\n",
       "   episode_len_mean  episodes_this_iter  num_steps_trained  num_steps_sampled  \\\n",
       "0               1.0                 100              20000              20000   \n",
       "\n",
       "   sample_time_ms  grad_time_ms  update_time_ms  ...  \\\n",
       "0            0.74         0.446           0.003  ...   \n",
       "\n",
       "   info/sample_peak_throughput  info/opt_samples  learner/cumulative_regret  \\\n",
       "0                     1350.692               1.0                   100750.0   \n",
       "\n",
       "   learner/update_latency  perf/cpu_util_percent  perf/ram_util_percent  \\\n",
       "0                 0.00042                   16.9                   68.7   \n",
       "\n",
       "   info/learner/cumulative_regret  info/learner/update_latency  \\\n",
       "0                        100750.0                      0.00042   \n",
       "\n",
       "                                          config/env  \\\n",
       "0  <class '__main__.SimpleContextualBanditNonline...   \n",
       "\n",
       "                                              logdir  \n",
       "0  /Users/deanwampler/ray_results/contrib/LinTS/c...  \n",
       "\n",
       "[1 rows x 54 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = analysis.dataframe()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This run with Thompson sampling yields similar results with the reward mean about 4.5 and failure chaotic results over 20000 steps as shown in the TensorBoard graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
