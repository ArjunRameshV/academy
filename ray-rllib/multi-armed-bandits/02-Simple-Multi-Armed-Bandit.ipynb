{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ray RLlib - Simple Multi-Armed Bandits Example\n",
    "\n",
    "Â© 2019-2020, Anyscale. All Rights Reserved\n",
    "\n",
    "![Anyscale Academy](../../images/AnyscaleAcademy_Logo_clearbanner_141x100.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore a very simple contextual bandit example with 3 arms. We'll run trials using RLlib and [Tune](http://tune.io), Ray's hyperparameter tuning library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.spaces import Discrete, Box\n",
    "import numpy as np\n",
    "import random\n",
    "from ray import tune\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the bandit as a subclass of an OpenAI Gym environment. We set the action space to have three discrete variables, one action for each arm, and an observation space (the context) in the range[-1.0, 1.0].\n",
    "\n",
    "Note that we'll randomly pick the context when `reset` is called, but it stays fixed (static) throughout the episode (the set of steps between calls to `reset`). Hence, this is not a context bandit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleBandit (gym.Env):\n",
    "    def __init__ (self, config=None):\n",
    "        self.action_space = Discrete(3)     # 3 arms\n",
    "        self.observation_space = Box(low=-1., high=1., shape=(2, ), dtype=np.float64)  # Random (x,y), where x,y from -1 to 1\n",
    "        self.current_context = None\n",
    "\n",
    "    def reset (self):\n",
    "        self.current_context = random.choice([-1., 1.])\n",
    "        return np.array([-self.current_context, self.current_context])\n",
    "\n",
    "    def step (self, action):\n",
    "        rewards_for_context = {\n",
    "            -1.: [-10, 0, 10],\n",
    "            1.: [10, 0, -10],\n",
    "        }\n",
    "        reward = rewards_for_context[self.current_context][action]\n",
    "        return (np.array([-self.current_context, self.current_context]), reward, True,\n",
    "                {\n",
    "                    \"regret\": 10 - reward\n",
    "                })\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'SimpleBandit(action_space={self.action_space}, observation_space={self.observation_space}, current_context={self.current_context})'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try repeating the next two code cells enough times to see the current context set to `1.0` and `-1.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Initial observation = [ 1. -1.], current context = -1.0'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bandit = SimpleBandit()\n",
    "observation = bandit.reset()\n",
    "f'Initial observation = {observation}, bandit = {repr(bandit)}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `bandit.current_context` and the observation of the current environment will remain fixed through the episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation = [ 1. -1.], reward =  -10, done = True , info = {'regret': 20}\n",
      "observation = [ 1. -1.], reward =    0, done = True , info = {'regret': 10}\n",
      "observation = [ 1. -1.], reward =   10, done = True , info = {'regret': 0}\n",
      "observation = [ 1. -1.], reward =    0, done = True , info = {'regret': 10}\n",
      "observation = [ 1. -1.], reward =  -10, done = True , info = {'regret': 20}\n",
      "observation = [ 1. -1.], reward =   10, done = True , info = {'regret': 0}\n",
      "observation = [ 1. -1.], reward =  -10, done = True , info = {'regret': 20}\n",
      "observation = [ 1. -1.], reward =   10, done = True , info = {'regret': 0}\n",
      "observation = [ 1. -1.], reward =  -10, done = True , info = {'regret': 20}\n",
      "observation = [ 1. -1.], reward =   10, done = True , info = {'regret': 0}\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    observation, reward, done, info = bandit.step(bandit.action_space.sample())\n",
    "    print(f'observation = {observation}, reward = {reward:4d}, done = {str(done):5s}, info = {info}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use Tune to train the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = {\n",
    "    \"training_iteration\": 200,\n",
    "    \"timesteps_total\": 100000,\n",
    "    \"episode_reward_mean\": 10.0,\n",
    "}\n",
    "\n",
    "config = {\n",
    "    \"env\": SimpleBandit,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.progress_reporter import JupyterNotebookReporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-04 14:14:30,481\tINFO resource_spec.py:212 -- Starting Ray with 4.39 GiB memory available for workers and up to 2.21 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n",
      "2020-06-04 14:14:30,801\tINFO services.py:1170 -- View the Ray dashboard at \u001b[1m\u001b[32mlocalhost:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/8 CPUs, 0/0 GPUs, 0.0/4.39 GiB heap, 0.0/1.51 GiB objects<br>Result logdir: /Users/deanwampler/ray_results/contrib/LinUCB<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>contrib_LinUCB_SimpleBandit_00000</td><td>RUNNING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=57907)\u001b[0m 2020-06-04 14:14:38,676\tINFO trainer.py:421 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=57907)\u001b[0m 2020-06-04 14:14:38,679\tINFO trainer.py:580 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=57907)\u001b[0m 2020-06-04 14:14:38,694\tINFO trainable.py:217 -- Getting current IP.\n",
      "\u001b[2m\u001b[36m(pid=57907)\u001b[0m 2020-06-04 14:14:38,694\tWARNING util.py:37 -- Install gputil for GPU system monitoring.\n",
      "Result for contrib_LinUCB_SimpleBandit_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-06-04_14-14-38\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.7\n",
      "  episode_reward_min: -10.0\n",
      "  episodes_this_iter: 100\n",
      "  episodes_total: 100\n",
      "  experiment_id: 2b8c7c1def084ab0ae5094cab7b30c3f\n",
      "  experiment_tag: '0'\n",
      "  grad_time_ms: 0.277\n",
      "  hostname: DWAnyscaleMBP.local\n",
      "  info:\n",
      "    grad_time_ms: 0.277\n",
      "    learner:\n",
      "      cumulative_regret: 30.0\n",
      "      update_latency: 0.00015020370483398438\n",
      "    num_steps_sampled: 100\n",
      "    num_steps_trained: 100\n",
      "    opt_peak_throughput: 3608.314\n",
      "    opt_samples: 1.0\n",
      "    sample_peak_throughput: 1317.969\n",
      "    sample_time_ms: 0.759\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 1\n",
      "  learner:\n",
      "    cumulative_regret: 30.0\n",
      "    update_latency: 0.00015020370483398438\n",
      "  node_ip: 192.168.1.149\n",
      "  num_healthy_workers: 0\n",
      "  num_steps_sampled: 100\n",
      "  num_steps_trained: 100\n",
      "  off_policy_estimator: {}\n",
      "  opt_peak_throughput: 3608.314\n",
      "  opt_samples: 1.0\n",
      "  optimizer_steps_this_iter: 100\n",
      "  perf:\n",
      "    cpu_util_percent: 30.8\n",
      "    ram_util_percent: 64.7\n",
      "  pid: 57907\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sample_peak_throughput: 1317.969\n",
      "  sample_time_ms: 0.759\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.026754813619179293\n",
      "    mean_inference_ms: 0.42633254929344244\n",
      "    mean_processing_ms: 0.2920297112795387\n",
      "  time_since_restore: 0.11420917510986328\n",
      "  time_this_iter_s: 0.11420917510986328\n",
      "  time_total_s: 0.11420917510986328\n",
      "  timestamp: 1591305278\n",
      "  timesteps_since_restore: 100\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 100\n",
      "  training_iteration: 1\n",
      "  trial_id: '00000'\n",
      "  update_time_ms: 0.001\n",
      "  \n",
      "Result for contrib_LinUCB_SimpleBandit_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-06-04_14-14-38\n",
      "  done: true\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 10.0\n",
      "  episode_reward_min: 10.0\n",
      "  episodes_this_iter: 100\n",
      "  episodes_total: 200\n",
      "  experiment_id: 2b8c7c1def084ab0ae5094cab7b30c3f\n",
      "  experiment_tag: '0'\n",
      "  grad_time_ms: 0.26\n",
      "  hostname: DWAnyscaleMBP.local\n",
      "  info:\n",
      "    grad_time_ms: 0.26\n",
      "    learner:\n",
      "      cumulative_regret: 30.0\n",
      "      update_latency: 0.00015211105346679688\n",
      "    num_steps_sampled: 200\n",
      "    num_steps_trained: 200\n",
      "    opt_peak_throughput: 3851.873\n",
      "    opt_samples: 1.0\n",
      "    sample_peak_throughput: 1341.276\n",
      "    sample_time_ms: 0.746\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 2\n",
      "  learner:\n",
      "    cumulative_regret: 30.0\n",
      "    update_latency: 0.00015211105346679688\n",
      "  node_ip: 192.168.1.149\n",
      "  num_healthy_workers: 0\n",
      "  num_steps_sampled: 200\n",
      "  num_steps_trained: 200\n",
      "  off_policy_estimator: {}\n",
      "  opt_peak_throughput: 3851.873\n",
      "  opt_samples: 1.0\n",
      "  optimizer_steps_this_iter: 100\n",
      "  perf: {}\n",
      "  pid: 57907\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sample_peak_throughput: 1341.276\n",
      "  sample_time_ms: 0.746\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.025382682458678297\n",
      "    mean_inference_ms: 0.40484898125947416\n",
      "    mean_processing_ms: 0.27937319741320266\n",
      "  time_since_restore: 0.21674323081970215\n",
      "  time_this_iter_s: 0.10253405570983887\n",
      "  time_total_s: 0.21674323081970215\n",
      "  timestamp: 1591305278\n",
      "  timesteps_since_restore: 200\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 200\n",
      "  training_iteration: 2\n",
      "  trial_id: '00000'\n",
      "  update_time_ms: 0.001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/0 GPUs, 0.0/4.39 GiB heap, 0.0/1.51 GiB objects<br>Result logdir: /Users/deanwampler/ray_results/contrib/LinUCB<br>Number of trials: 1 (1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>contrib_LinUCB_SimpleBandit_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        0.216743</td><td style=\"text-align: right;\"> 200</td><td style=\"text-align: right;\">      10</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The trials took 8.493189096450806 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "analysis = tune.run(\"contrib/LinUCB\", config=config, stop=stop, \n",
    "                    progress_reporter=JupyterNotebookReporter(overwrite=False),  # this is the default, actually.\n",
    "                    verbose=2)  # Change to 0 or 1 to reduce the output.\n",
    "\n",
    "print(\"The trials took\", time.time() - start_time, \"seconds\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see some of the final data as a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_reward_max</th>\n",
       "      <th>episode_reward_min</th>\n",
       "      <th>episode_reward_mean</th>\n",
       "      <th>episode_len_mean</th>\n",
       "      <th>episodes_this_iter</th>\n",
       "      <th>num_steps_trained</th>\n",
       "      <th>num_steps_sampled</th>\n",
       "      <th>sample_time_ms</th>\n",
       "      <th>grad_time_ms</th>\n",
       "      <th>update_time_ms</th>\n",
       "      <th>...</th>\n",
       "      <th>info/sample_peak_throughput</th>\n",
       "      <th>info/opt_samples</th>\n",
       "      <th>learner/cumulative_regret</th>\n",
       "      <th>learner/update_latency</th>\n",
       "      <th>perf/cpu_util_percent</th>\n",
       "      <th>perf/ram_util_percent</th>\n",
       "      <th>info/learner/cumulative_regret</th>\n",
       "      <th>info/learner/update_latency</th>\n",
       "      <th>config/env</th>\n",
       "      <th>logdir</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>0.746</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.001</td>\n",
       "      <td>...</td>\n",
       "      <td>1341.276</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>&lt;class '__main__.SimpleBandit'&gt;</td>\n",
       "      <td>/Users/deanwampler/ray_results/contrib/LinUCB/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   episode_reward_max  episode_reward_min  episode_reward_mean  \\\n",
       "0                10.0                10.0                 10.0   \n",
       "\n",
       "   episode_len_mean  episodes_this_iter  num_steps_trained  num_steps_sampled  \\\n",
       "0               1.0                 100                200                200   \n",
       "\n",
       "   sample_time_ms  grad_time_ms  update_time_ms  ...  \\\n",
       "0           0.746          0.26           0.001  ...   \n",
       "\n",
       "   info/sample_peak_throughput  info/opt_samples  learner/cumulative_regret  \\\n",
       "0                     1341.276               1.0                       30.0   \n",
       "\n",
       "   learner/update_latency  perf/cpu_util_percent  perf/ram_util_percent  \\\n",
       "0                0.000152                    NaN                    NaN   \n",
       "\n",
       "   info/learner/cumulative_regret  info/learner/update_latency  \\\n",
       "0                            30.0                     0.000152   \n",
       "\n",
       "                        config/env  \\\n",
       "0  <class '__main__.SimpleBandit'>   \n",
       "\n",
       "                                              logdir  \n",
       "0  /Users/deanwampler/ray_results/contrib/LinUCB/...  \n",
       "\n",
       "[1 rows x 54 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = analysis.dataframe()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easiest way to inspect the progression of training is to use TensorBoard.\n",
    "\n",
    "1. If you are runnng on the Anyscale Platform, click the _Tensorboard_ link. \n",
    "2. If you running this notebook on a laptop, open a terminal window using the `+` under the _Edit_ menu, run the following command, then open the URL shown.\n",
    "\n",
    "```\n",
    "tensorboard --logdir ~/ray_results \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have many data sets plotted from previous tutorial lessons. In the _Runs_ on the left, look for one named something like this:\n",
    "\n",
    "```\n",
    "contrib/LinUCB/contrib_LinUCB_SimpleBandit_0_YYYY-MM-DD_HH-MM-SSxxxxxxxx  \n",
    "```\n",
    "\n",
    "If you have several of them, you want the one with the latest timestamp. To select just that one, click _toggler all runs_ below the list of runs, then select the one you want. You should see something like the following image:\n",
    "\n",
    "![TensorBoard for SimpleBandit](../../images/rllib/TensorBoard-for-SimpleBandit.png)\n",
    "\n",
    "The graph for the metric we were optimizing, the mean reward, is shown with a rectangle surrounding it. It improved steadily during the training runs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Context Bandit\n",
    "\n",
    "`SimpleBandit` had a fixed context through entire episodes. What if we made it contextual, and allowed the `current_context` to randomly change at each step? To do that, we can simply subclass `SimpleBandit` and override `step` to change the `current_context` after calling `SimpleBandit.step()`. (We do this afterwards to set up the _next_ step, but whether we do this before or after doesn't affect the results.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleContextBandit(SimpleBandit):\n",
    "    def __init__ (self, config=None):\n",
    "        super().__init__(config)\n",
    "        \n",
    "    def step(self, action):\n",
    "        result = super().step(action)\n",
    "        self.current_context = random.choice([-1.,1.])\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandit2 = SimpleContextBandit()\n",
    "observation2 = bandit2.reset()\n",
    "f'Initial observation = {observation2}, bandit = {repr(bandit2)}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the `bandit.current_context` and the observation of the current environment will _change_ through the episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation = [-1.  1.], reward =    0, done = True , info = {'regret': 10}\n",
      "observation = [ 1. -1.], reward =    0, done = True , info = {'regret': 10}\n",
      "observation = [-1.  1.], reward =    0, done = True , info = {'regret': 10}\n",
      "observation = [-1.  1.], reward =   10, done = True , info = {'regret': 0}\n",
      "observation = [ 1. -1.], reward =   10, done = True , info = {'regret': 0}\n",
      "observation = [-1.  1.], reward =  -10, done = True , info = {'regret': 20}\n",
      "observation = [-1.  1.], reward =    0, done = True , info = {'regret': 10}\n",
      "observation = [ 1. -1.], reward =   10, done = True , info = {'regret': 0}\n",
      "observation = [ 1. -1.], reward =    0, done = True , info = {'regret': 10}\n",
      "observation = [ 1. -1.], reward =    0, done = True , info = {'regret': 10}\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    observation, reward, done, info = bandit2.step(bandit2.action_space.sample())\n",
    "    print(f'observation = {observation}, reward = {reward:4d}, done = {str(done):5s}, info = {info}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train with Tune again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop2 = {\n",
    "    \"training_iteration\": 200,\n",
    "    \"timesteps_total\": 100000,\n",
    "    \"episode_reward_mean\": 10.0,\n",
    "}\n",
    "\n",
    "config2 = {\n",
    "    \"env\": SimpleContextBandit,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/8 CPUs, 0/0 GPUs, 0.0/4.39 GiB heap, 0.0/1.51 GiB objects<br>Result logdir: /Users/deanwampler/ray_results/contrib/LinUCB<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                              </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>contrib_LinUCB_SimpleContextBandit_00000</td><td>RUNNING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=57910)\u001b[0m 2020-06-04 14:30:36,705\tINFO trainer.py:421 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=57910)\u001b[0m 2020-06-04 14:30:36,708\tINFO trainer.py:580 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=57910)\u001b[0m 2020-06-04 14:30:36,716\tINFO trainable.py:217 -- Getting current IP.\n",
      "\u001b[2m\u001b[36m(pid=57910)\u001b[0m 2020-06-04 14:30:36,717\tWARNING util.py:37 -- Install gputil for GPU system monitoring.\n",
      "Result for contrib_LinUCB_SimpleContextBandit_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-06-04_14-30-36\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.7\n",
      "  episode_reward_min: -10.0\n",
      "  episodes_this_iter: 100\n",
      "  episodes_total: 100\n",
      "  experiment_id: 256312a17af2442bb2a44add4858c552\n",
      "  experiment_tag: '0'\n",
      "  grad_time_ms: 0.238\n",
      "  hostname: DWAnyscaleMBP.local\n",
      "  info:\n",
      "    grad_time_ms: 0.238\n",
      "    learner:\n",
      "      cumulative_regret: 30.0\n",
      "      update_latency: 0.000125885009765625\n",
      "    num_steps_sampled: 100\n",
      "    num_steps_trained: 100\n",
      "    opt_peak_throughput: 4193.465\n",
      "    opt_samples: 1.0\n",
      "    sample_peak_throughput: 1496.416\n",
      "    sample_time_ms: 0.668\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 1\n",
      "  learner:\n",
      "    cumulative_regret: 30.0\n",
      "    update_latency: 0.000125885009765625\n",
      "  node_ip: 192.168.1.149\n",
      "  num_healthy_workers: 0\n",
      "  num_steps_sampled: 100\n",
      "  num_steps_trained: 100\n",
      "  off_policy_estimator: {}\n",
      "  opt_peak_throughput: 4193.465\n",
      "  opt_samples: 1.0\n",
      "  optimizer_steps_this_iter: 100\n",
      "  perf:\n",
      "    cpu_util_percent: 12.9\n",
      "    ram_util_percent: 65.6\n",
      "  pid: 57910\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sample_peak_throughput: 1496.416\n",
      "  sample_time_ms: 0.668\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.02761406473594136\n",
      "    mean_inference_ms: 0.36491025792490156\n",
      "    mean_processing_ms: 0.2685773490679146\n",
      "  time_since_restore: 0.10178422927856445\n",
      "  time_this_iter_s: 0.10178422927856445\n",
      "  time_total_s: 0.10178422927856445\n",
      "  timestamp: 1591306236\n",
      "  timesteps_since_restore: 100\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 100\n",
      "  training_iteration: 1\n",
      "  trial_id: '00000'\n",
      "  update_time_ms: 0.001\n",
      "  \n",
      "Result for contrib_LinUCB_SimpleContextBandit_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-06-04_14-30-36\n",
      "  done: true\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 10.0\n",
      "  episode_reward_min: 10.0\n",
      "  episodes_this_iter: 100\n",
      "  episodes_total: 200\n",
      "  experiment_id: 256312a17af2442bb2a44add4858c552\n",
      "  experiment_tag: '0'\n",
      "  grad_time_ms: 0.212\n",
      "  hostname: DWAnyscaleMBP.local\n",
      "  info:\n",
      "    grad_time_ms: 0.212\n",
      "    learner:\n",
      "      cumulative_regret: 30.0\n",
      "      update_latency: 0.0001239776611328125\n",
      "    num_steps_sampled: 200\n",
      "    num_steps_trained: 200\n",
      "    opt_peak_throughput: 4715.88\n",
      "    opt_samples: 1.0\n",
      "    sample_peak_throughput: 1560.904\n",
      "    sample_time_ms: 0.641\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 2\n",
      "  learner:\n",
      "    cumulative_regret: 30.0\n",
      "    update_latency: 0.0001239776611328125\n",
      "  node_ip: 192.168.1.149\n",
      "  num_healthy_workers: 0\n",
      "  num_steps_sampled: 200\n",
      "  num_steps_trained: 200\n",
      "  off_policy_estimator: {}\n",
      "  opt_peak_throughput: 4715.88\n",
      "  opt_samples: 1.0\n",
      "  optimizer_steps_this_iter: 100\n",
      "  perf: {}\n",
      "  pid: 57910\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sample_peak_throughput: 1560.904\n",
      "  sample_time_ms: 0.641\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.02687487436171194\n",
      "    mean_inference_ms: 0.35571696153327587\n",
      "    mean_processing_ms: 0.25522768200926527\n",
      "  time_since_restore: 0.19376397132873535\n",
      "  time_this_iter_s: 0.0919797420501709\n",
      "  time_total_s: 0.19376397132873535\n",
      "  timestamp: 1591306236\n",
      "  timesteps_since_restore: 200\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 200\n",
      "  training_iteration: 2\n",
      "  trial_id: '00000'\n",
      "  update_time_ms: 0.001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/0 GPUs, 0.0/4.39 GiB heap, 0.0/1.51 GiB objects<br>Result logdir: /Users/deanwampler/ray_results/contrib/LinUCB<br>Number of trials: 1 (1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                              </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>contrib_LinUCB_SimpleContextBandit_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        0.193764</td><td style=\"text-align: right;\"> 200</td><td style=\"text-align: right;\">      10</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The trials took 4.615687847137451 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "analysis2 = tune.run(\"contrib/LinUCB\", config=config2, stop=stop2, \n",
    "                    progress_reporter=JupyterNotebookReporter(overwrite=False),  # this is the default, actually.\n",
    "                    verbose=2)  # Change to 0 or 1 to reduce the output.\n",
    "\n",
    "print(\"The trials took\", time.time() - start_time, \"seconds\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the analysis dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_reward_max</th>\n",
       "      <th>episode_reward_min</th>\n",
       "      <th>episode_reward_mean</th>\n",
       "      <th>episode_len_mean</th>\n",
       "      <th>episodes_this_iter</th>\n",
       "      <th>num_steps_trained</th>\n",
       "      <th>num_steps_sampled</th>\n",
       "      <th>sample_time_ms</th>\n",
       "      <th>grad_time_ms</th>\n",
       "      <th>update_time_ms</th>\n",
       "      <th>...</th>\n",
       "      <th>info/sample_peak_throughput</th>\n",
       "      <th>info/opt_samples</th>\n",
       "      <th>learner/cumulative_regret</th>\n",
       "      <th>learner/update_latency</th>\n",
       "      <th>perf/cpu_util_percent</th>\n",
       "      <th>perf/ram_util_percent</th>\n",
       "      <th>info/learner/cumulative_regret</th>\n",
       "      <th>info/learner/update_latency</th>\n",
       "      <th>config/env</th>\n",
       "      <th>logdir</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>0.641</td>\n",
       "      <td>0.212</td>\n",
       "      <td>0.001</td>\n",
       "      <td>...</td>\n",
       "      <td>1560.904</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.000124</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.000124</td>\n",
       "      <td>&lt;class '__main__.SimpleContextBandit'&gt;</td>\n",
       "      <td>/Users/deanwampler/ray_results/contrib/LinUCB/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   episode_reward_max  episode_reward_min  episode_reward_mean  \\\n",
       "0                10.0                10.0                 10.0   \n",
       "\n",
       "   episode_len_mean  episodes_this_iter  num_steps_trained  num_steps_sampled  \\\n",
       "0               1.0                 100                200                200   \n",
       "\n",
       "   sample_time_ms  grad_time_ms  update_time_ms  ...  \\\n",
       "0           0.641         0.212           0.001  ...   \n",
       "\n",
       "   info/sample_peak_throughput  info/opt_samples  learner/cumulative_regret  \\\n",
       "0                     1560.904               1.0                       30.0   \n",
       "\n",
       "   learner/update_latency  perf/cpu_util_percent  perf/ram_util_percent  \\\n",
       "0                0.000124                    NaN                    NaN   \n",
       "\n",
       "   info/learner/cumulative_regret  info/learner/update_latency  \\\n",
       "0                            30.0                     0.000124   \n",
       "\n",
       "                               config/env  \\\n",
       "0  <class '__main__.SimpleContextBandit'>   \n",
       "\n",
       "                                              logdir  \n",
       "0  /Users/deanwampler/ray_results/contrib/LinUCB/...  \n",
       "\n",
       "[1 rows x 54 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis2.dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's similar to what we saw before. Switch to your TensorBoard window and look at the plots for this run. The name will be similar to the following:\n",
    "\n",
    "```\n",
    "contrib/LinUCB/contrib_LinUCB_SimpleContextBandit_0_YYYY-MM-DD_HH-MM-SSxxxxxxxx  \n",
    "```\n",
    "\n",
    "Toggle this one on by itself, then also turn on (check) a run for `SimpleBandit`. The plots for `tune/episode_reward_mean` are probably identical for both, while this won't be true for some of the other graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
