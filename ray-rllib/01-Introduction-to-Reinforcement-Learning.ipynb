{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ray RLlib - Introduction to Reinforcement Learning\n",
    "\n",
    "Â© 2019-2020, Anyscale. All Rights Reserved\n",
    "\n",
    "![Anyscale Academy](../images/AnyscaleAcademy_Logo_clearbanner_141x100.png)\n",
    "\n",
    "_Reinforcement Learning_ is the category of machine learning that focuses on training one or more _agents_ to achieve maximal _rewards_ while operating in an environment. This lesson discusses the core concepts of RL, while subsequent lessons explore RLlib in depth. We'll use two examples with exercises to give you a taste of RL. If you already understand RL concepts, you can either skim this lesson or skip to the [next lesson](02-About-RLlib.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Is Reinforcement Learning?\n",
    "\n",
    "Let's explore the basic concepts of RL, specifically the _Markov Decision Process_ abstraction, and to show its use in Python.\n",
    "\n",
    "Consider the following image:\n",
    "\n",
    "![RL Concepts](../images/rllib/RL-concepts.png)\n",
    "\n",
    "In RL, one or more **agents** interact with an **environment** to maximize a **reward**. The agents make **observations** about the **state** of the environment and take **actions** that are believed will maximize the long-term reward. However, at any particular moment, the agents can only observe the immediate reward. So, the training process usually involves lots and lot of replay of the game, the robot simulator traversing a virtual space, etc., so the agents can learn from repeated trials what decisions/actions work best to maximize the long-term, cummulative reward.\n",
    "\n",
    "The trail and error search and delayed reward are the distinguishing characterists of RL vs. other ML methods ([Sutton 2018](06-RL-References.ipynb#Books)).\n",
    "\n",
    "The way to formalize trial and error is the **exploitation vs. exploration tradeoff**. When an agent finds what appears to be a \"rewarding\" sequence of actions, the agent may naturally want to continue to **exploit** these actions. However, even better actions may exist. An agent won't know whether alternatives are better or not unless some percentage of actions taken **explore** the alternatives. So, all RL algorithms include a strategy for exploitation and exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL Applications\n",
    "\n",
    "RL has many potential applications. RL became \"famous\" due to these successes, including achieving expert game play, training robots, autonomous vehicles, and other simulated agents:\n",
    "\n",
    "![AlphaGo](../images/rllib/alpha-go.jpg)\n",
    "![Game](../images/rllib/breakout.png)\n",
    "\n",
    "![Stacking Legos with Sawyer](../images/rllib/stacking-legos-with-sawyer.gif)\n",
    "![Walking Man](../images/rllib/walking-man.gif)\n",
    "\n",
    "![Autonomous Vehicle](../images/rllib/daimler-autonomous-car.jpg)\n",
    "![Four-legged Robot](../images/rllib/four-legged-robot.jpeg)\n",
    "\n",
    "Credits:\n",
    "* [AlphaGo](https://www.youtube.com/watch?v=l7ngy56GY6k)\n",
    "* [Breakout](https://towardsdatascience.com/tutorial-double-deep-q-learning-with-dueling-network-architectures-4c1b3fb7f756) ([paper](https://arxiv.org/abs/1312.5602))\n",
    "* [Stacking Legos with Sawyer](https://robohub.org/soft-actor-critic-deep-reinforcement-learning-with-real-world-robots/)\n",
    "* [Walking Man](https://openai.com/blog/openai-baselines-ppo/)\n",
    "* [Autonomous Vehicle](https://www.daimler.com/innovation/case/autonomous/intelligent-drive-2.html)\n",
    "* [Four-legged Robot](https://robohub.org/four-legged-robot-that-efficiently-handles-challenging-terrain/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recently other industry applications have emerged, include the following:\n",
    "\n",
    "* **Process optimization:** industrial processes (factories, pipelines) and other business processes, routing problems, cluster optimization.\n",
    "* **Ad serving and recommendations:** Some of the traditional methods, including _collaborative filtering_, are hard to scale for very large data sets. RL systems are being developed to do an effective job more efficiently than traditional methods.\n",
    "* **Finance:** Markets are time-oriented _environments_ where automated trading systems are the _agents_. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hideCode": false,
    "hidePrompt": false,
    "id": "bAptEafKNlhm"
   },
   "source": [
    "## Markov Decision Processes\n",
    "\n",
    "At its core, Reinforcement learning builds on the concepts of [Markov Decision Process (MDP)](https://en.wikipedia.org/wiki/Markov_decision_process), where the current state, the possible actions that can be taken, and overall goal are the building blocks.\n",
    "\n",
    "An MDP models sequential interactions with an external environment. It consists of the following:\n",
    "\n",
    "- a **state space** where the current state of the system is sometimes called the **context**.\n",
    "- a set of **actions** that can be taken at a particular state $s$ (or sometimes the same set for all states).\n",
    "- a **transition function** that describes the probability of being in a state $s'$ at time $t+1$ given that the MDP was in state $s$ at time $t$ and action $a$ was taken. The next action is selected stocastically based on these probabilities.\n",
    "- a **reward function**, which determines the reward received at time $t$ following action $a$, based on the decision of **policy** $\\pi$.\n",
    "\n",
    "The goal of MDP is to develop a **policy** $\\pi$ that specifies what action $a$ should be chosen for a given state $s$ so that the cummulative reward is maximized. Since the policy fixes a single action for each state, the transition probabilities reduce to the probability of transitioning to state $s'$ given the current state is $s$, independent of actions. Various algorithms can be used to compute this policy. \n",
    "\n",
    "Often this cummulative reward is computed using the **discounted sum** over all rewards observed:\n",
    "\n",
    "\\begin{equation}\n",
    "\\arg\\max_{\\pi} \\sum_{t=1}^T \\gamma^t R_t(\\pi),\n",
    "\\end{equation}\n",
    "\n",
    "where $T$ is the number of steps taken in the MDP (this is a random variable and may depend on $\\pi$), $R_t$ is the reward received at time $t$ (also a random variable which depends on $\\pi$), and $\\gamma$ is the **discount factor**. The value of $\\gamma$ is between 0 and 1, meaning it has the effect of \"discounting\" earlier rewards vs. more recent rewards. \n",
    "\n",
    "More details about MDP are available [here](https://en.wikipedia.org/wiki/Markov_decision_process). Note what we said in the third bullet, that the new state only depends on the previous state and the action taken. The assumption is that we can simplify our effort by ignoring all the previous states except the last one and still achieve good results. This is known as the [Markov property](https://en.wikipedia.org/wiki/Markov_property)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hideCode": false,
    "hidePrompt": false,
    "id": "bAptEafKNlhm"
   },
   "source": [
    "## The Elements of RL\n",
    "\n",
    "Here are the elements of RL that expand on MDP concepts ([Sutton 2018](http://incompleteideas.net/book/bookdraft2018jan1.pdf)):\n",
    "\n",
    "#### Policies\n",
    "\n",
    "Unlike MDP, the **transition function** probabilities are often not known in advance, but must be learned. Learning is done through repeated \"play\", where the agent interacts with the environment.\n",
    "\n",
    "This makes the **policy** $\\pi$ harder to determine. Also, specifying one action per state, as in MDP, is usually relaxed, making the choice of action stochastic.\n",
    "\n",
    "#### Reward Signal\n",
    "\n",
    "The idea of a **reward signal** encapsulates the desired goal for the system and provides feedback for updating the policy based on how well particular events or actions contribute rewards towards the goal.\n",
    "\n",
    "#### Value Function\n",
    "\n",
    "The **value function** encapsulates the maximum commulative reward likely to be achieved starting from a given state. This is harder to determine than the simple reward returned after taking an action. In fact, much of the research in RL over the decades has focused on finding better and more efficient implementations of value functions. To illustrate the challenge, repeatedly taking one sequence of actions may yield low rewards for a while, but eventually provide large rewards, or vice-versa. \n",
    "\n",
    "#### Model\n",
    "\n",
    "An optional feature, some RL algorithms develop a **model** of the environment to anticipate the resulting states and rewards for future actions. Hence, they are useful for _planning_ scenarios. Methods for solving RL problems that use models are called _model-based methods_, while methods that learn by trial and error are called _model-free methods_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hideCode": false,
    "hidePrompt": false,
    "id": "bAptEafKNlhm"
   },
   "source": [
    "## Reinforcement Learning Examples\n",
    "\n",
    "Let's finish this introduction with two examples. One is a popular \"hello world\" (1) example for RL, balancing a pole vertically on a moving cart, called _CartPole_. The second example provides a \"taste\" of RLlib and a popular RL algorithm, _Proximal Policy Optimization_, again using _CartPole_.\n",
    "\n",
    "(1) In books and tutorials on particular programming languages, it is a tradition that the very first program shown prints the message \"Hello World!\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hideCode": false,
    "hidePrompt": false,
    "id": "bAptEafKNlhm"
   },
   "source": [
    "### CartPole and OpenAI\n",
    "\n",
    "The [OpenAI \"gym\" environment](https://gym.openai.com/) provides MDP interfaces to a variety of simulators. For example, the _CartPole_ environment interfaces with a simple simulator that simulates the physics of balancing a pole on a cart. The CartPole problem is described at https://gym.openai.com/envs/CartPole-v0. Here is an image from that website:\n",
    "\n",
    "![Cart Pole](../images/rllib/Cart-Pole.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hideCode": false,
    "hidePrompt": false,
    "id": "bAptEafKNlhm"
   },
   "source": [
    "This example fits into the MDP framework as follows.\n",
    "- The **state** consists of the position and velocity of the cart (moving in one dimension from left to right) as well as the angle and angular velocity of the pole that is balancing on the cart.\n",
    "- The **actions** are to decrease or increase the cart's velocity by one unit.\n",
    "- The **transition function** is deterministic and is determined by simulating physical laws.\n",
    "- The **reward function** is a constant 1 as long as the pole is upright, and 0 once the pole has fallen over. Therefore, maximizing the reward means balancing the pole for as long as possible.\n",
    "- The **discount factor** in this case can be taken to be 1.\n",
    "\n",
    "More information about the `gym` Python module is available at https://gym.openai.com/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a9Kwo5ZfNlhn"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WPpofaxQNlhp"
   },
   "source": [
    "The code below illustrates how to create and manipulate MDPs in Python. An MDP can be created by calling `gym.make`. Gym environments are identified by names like `CartPole-v0`. A **catalog of built-in environments** can be found at https://gym.openai.com/envs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "hideCode": false,
    "hidePrompt": false,
    "id": "6DZ68SG9Nlhp",
    "outputId": "293be60b-8107-42f2-c54a-58f3eaf295f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created env: <TimeLimit<CartPoleEnv<CartPole-v0>>>\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "print('Created env:', env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xn5PqgDzNlhr"
   },
   "source": [
    "Reset the state of the MDP by calling `env.reset()`. This call returns the initial state of the MDP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "hideCode": false,
    "hidePrompt": false,
    "id": "zRA58dOFNlhs",
    "outputId": "7aba4eac-fb0f-4654-eb49-0fbd6d664f28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The starting state is: [ 0.04810923 -0.01633585 -0.00723093  0.03941663]\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "print('The starting state is:', state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the state is the position of the cart, its velocity, the angle of the pole, and the angular velocity of the pole."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8MuXXesWNlhu"
   },
   "source": [
    "The `env.step` method takes an action. In the case of the CartPole environment, the appropriate actions are 0 or 1, for pushing the cart to the left or right, respectively. `env.step()` returns a tuple of four things:\n",
    "1. the new state of the environment\n",
    "2. a reward\n",
    "3. a boolean indicating whether the simulation has finished\n",
    "4. a dictionary of miscellaneous extra information\n",
    "\n",
    "Let's show what happens if we take one step with an action of 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "hideCode": false,
    "hidePrompt": false,
    "id": "TufVaMz_Nlhu",
    "outputId": "920b9758-7d85-49e8-f8ef-4586b6947dea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.04778251 -0.21135336 -0.0064426   0.32980938] 1.0 False {}\n"
     ]
    }
   ],
   "source": [
    "action = 0\n",
    "state, reward, done, info = env.step(action)\n",
    "print(state, reward, done, info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uBIoIuWYNlhw"
   },
   "source": [
    "A **rollout** is a simulation of a policy (trained or \"hand-coded\") in an environment. It alternates between choosing actions using some policy and taking those actions in the environment.\n",
    "\n",
    "The code below performs a rollout in a given environment. It takes **random actions** until the simulation has finished and returns the cumulative reward. Try rerunning this cell a few times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "Zp00mr88Nlhw",
    "outputId": "f0d01977-00c9-4ad2-931a-7f9730b5b005"
   },
   "outputs": [],
   "source": [
    "def random_rollout(env):\n",
    "    state = env.reset()\n",
    "    \n",
    "    done = False\n",
    "    cumulative_reward = 0\n",
    "\n",
    "    # Keep looping as long as the simulation has not finished.\n",
    "    while not done:\n",
    "        # Choose a random action (either 0 or 1).\n",
    "        action = np.random.choice([0, 1])\n",
    "        \n",
    "        # Take the action in the environment.\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        # Update the cumulative reward.\n",
    "        cumulative_reward += reward\n",
    "    \n",
    "    # Return the cumulative reward.\n",
    "    return cumulative_reward    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try rerunning the following cell a few times. How much do the answers change? Note that the maximum possible reward for _CartPole_ is 200. You'll probably get numbers under 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "Zp00mr88Nlhw",
    "outputId": "f0d01977-00c9-4ad2-931a-7f9730b5b005"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43.0\n",
      "14.0\n"
     ]
    }
   ],
   "source": [
    "reward = random_rollout(env)\n",
    "print(reward)\n",
    "reward = random_rollout(env)\n",
    "print(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i3FVvEJRNlhy"
   },
   "source": [
    "### Exercise 1\n",
    "\n",
    "Choosing actions at random in `random_rollout` is not a very effective policy, as the previous results showed. Finish implementing the `rollout_policy` function below, which takes an environment *and* a policy. Recall that the *policy* is a function that takes in a *state* and returns an *action*. The main difference is that instead of choosing a **random action**, like we just did (with poor results), the action should be chosen **with the policy** (as a function of the state).\n",
    "\n",
    "> **Note:** Exercise solutions for this tutorial can be found [here](solutions/Ray-RLlib-Solutions.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 375
    },
    "colab_type": "code",
    "id": "9PgmkROqNlhy",
    "outputId": "91445278-aeb7-4e86-e1b7-c92e5ba830a2"
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-56bb3be881bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mreward1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrollout_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_policy1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mreward2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrollout_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_policy2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-56bb3be881bf>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mreward1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrollout_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_policy1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mreward2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrollout_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_policy2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-56bb3be881bf>\u001b[0m in \u001b[0;36mrollout_policy\u001b[0;34m(env, policy)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# EXERCISE: Fill out this function by copying the appropriate part of 'random_rollout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# and modifying it to choose the action using the policy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Return the cumulative reward.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def rollout_policy(env, policy):\n",
    "    state = env.reset()\n",
    "    \n",
    "    done = False\n",
    "    cumulative_reward = 0\n",
    "\n",
    "    # EXERCISE: Fill out this function by copying the appropriate part of 'random_rollout'\n",
    "    # and modifying it to choose the action using the policy.\n",
    "    raise NotImplementedError\n",
    "\n",
    "    # Return the cumulative reward.\n",
    "    return cumulative_reward\n",
    "\n",
    "def sample_policy1(state):\n",
    "    return 0 if state[0] < 0 else 1\n",
    "\n",
    "def sample_policy2(state):\n",
    "    return 1 if state[0] < 0 else 0\n",
    "\n",
    "reward1 = np.mean([rollout_policy(env, sample_policy1) for _ in range(100)])\n",
    "reward2 = np.mean([rollout_policy(env, sample_policy2) for _ in range(100)])\n",
    "\n",
    "print('The first sample policy got an average reward of {}.'.format(reward1))\n",
    "print('The second sample policy got an average reward of {}.'.format(reward2))\n",
    "\n",
    "assert 5 < reward1 < 15, ('Make sure that rollout_policy computes the action '\n",
    "                          'by applying the policy to the state.')\n",
    "assert 25 < reward2 < 35, ('Make sure that rollout_policy computes the action '\n",
    "                           'by applying the policy to the state.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll return to _CartPole_ in lesson [03: Application Cart Pole](03-Applications-Cart-Pole.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qXQ8hIB9Nlh0"
   },
   "source": [
    "### Reinforcement Learning Example: Cart Pole with Proximal Policy Optimization\n",
    "\n",
    "This section demonstrates how to use the _proximal policy optimization_ (PPO) algorithm implemented by [RLlib](http://rllib.io). PPO is a popular way to develop a policy. RLlib also uses [Ray Tune](http://tune.io), the Ray Hyperparameter Tuning framework, which is covered in the [Ray Tune Tutorial](../ray-tune/00-Ray-Tune-Overview.ipynb).\n",
    "\n",
    "We'll provide relatively little explanation of **RLlib** concepts for now, but explore them in greater depth in subsequent lessons. For more on RLlib, see the documentation at http://rllib.io."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qXQ8hIB9Nlh0"
   },
   "source": [
    "PPO is described in detail in [this paper](https://arxiv.org/abs/1707.06347). It is a variant of _Trust Region Policy Optimization_ (TRPO) described in [this earlier paper](https://arxiv.org/abs/1502.05477). [This OpenAI post](https://openai.com/blog/openai-baselines-ppo/) provides a more accessible introduction to PPO.\n",
    "\n",
    "PPO works in two phases. In the first phase, a large number of rollouts are performed in parallel. The rollouts are then aggregated on the driver and a surrogate optimization objective is defined based on those rollouts. In the second phase, we use SGD (_stochastic gradient descent_) to find the policy that maximizes that objective with a penalty term for diverging too much from the current policy.\n",
    "\n",
    "![PPO](../images/rllib/ppo.png)\n",
    "\n",
    "([source](https://raw.githubusercontent.com/ucbrise/risecamp/risecamp2018/ray/tutorial/rllib_exercises/))\n",
    "\n",
    "> **NOTE:** The SGD optimization step is best performed in a data-parallel manner over multiple GPUs. This is exposed through the `num_gpus` field of the `config` dictionary. Hence, for normal usage, one or more GPUs is recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XwPnR2ibNlh2"
   },
   "outputs": [],
   "source": [
    "# import gym  # imported above already, but listed here for completeness\n",
    "import ray\n",
    "from ray.rllib.agents.ppo import PPOTrainer, DEFAULT_CONFIG\n",
    "from ray.tune.logger import pretty_print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following script checks if the Ray cluster is already running. If not, it tells you what to do to start Ray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Ray is already running.\n"
     ]
    }
   ],
   "source": [
    "!../tools/start-ray.sh --check --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tQFzEX2BNlh3"
   },
   "source": [
    "Now start Ray in this \"driver\" process. This must be done before we instantiate any RL agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tQFzEX2BNlh3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-12 09:00:13,341\tWARNING worker.py:809 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '192.168.1.149',\n",
       " 'raylet_ip_address': '192.168.1.149',\n",
       " 'redis_address': '192.168.1.149:15832',\n",
       " 'object_store_address': '/tmp/ray/session_2020-06-12_08-58-38_626987_40764/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2020-06-12_08-58-38_626987_40764/sockets/raylet',\n",
       " 'webui_url': 'localhost:8265',\n",
       " 'session_dir': '/tmp/ray/session_2020-06-12_08-58-38_626987_40764'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init(address='auto', ignore_reinit_error=True, log_to_driver=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Tip:** Having trouble starting Ray? See the [Troubleshooting](../reference/Troubleshooting-Tips-Tricks.ipynb) tips."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell prints the URL for the Ray Dashboard. **This is only correct if you are running this tutorial on a laptop.** Click the link to open the dashboard.\n",
    "\n",
    "If you are running on the Anyscale platform, use the URL provided by your instructor to open the Dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dashboard URL: http://localhost:8265\n"
     ]
    }
   ],
   "source": [
    "print(f'Dashboard URL: http://{ray.get_webui_url()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f9yhpJZVNlh5"
   },
   "source": [
    "Instantiate a PPOTrainer object. We pass in a config object that specifies how the network and training procedure should be configured. Some of the parameters are the following.\n",
    "\n",
    "- `num_workers` is the number of actors that the agent will create. This determines the degree of parallelism that will be used.\n",
    "- `num_sgd_iter` is the number of epochs of SGD (stochastic gradient descent, i.e., passes through the data) that will be used to optimize the PPO surrogate objective at each iteration of PPO.\n",
    "- `sgd_minibatch_size` is the SGD batch size (batches of data) that will be used to optimize the PPO surrogate objective.\n",
    "- `model` contains a dictionary of parameters describing the neural net used to parameterize the policy. The `fcnet_hiddens` parameter is a list of the sizes of the hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ok210MCfNlh5"
   },
   "outputs": [],
   "source": [
    "config = DEFAULT_CONFIG.copy()\n",
    "config['num_workers'] = 1\n",
    "config['num_sgd_iter'] = 30\n",
    "config['sgd_minibatch_size'] = 128\n",
    "config['model']['fcnet_hiddens'] = [100, 100]\n",
    "config['num_cpus_per_worker'] = 0  # This avoids running out of resources in the notebook environment when this cell is re-executed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ok210MCfNlh5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-12 09:37:37,729\tINFO trainer.py:421 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution\n",
      "2020-06-12 09:37:37,944\tINFO trainer.py:580 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "2020-06-12 09:37:40,118\tINFO trainable.py:217 -- Getting current IP.\n",
      "2020-06-12 09:37:40,119\tWARNING util.py:37 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "agent = PPOTrainer(config, 'CartPole-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ty1a6AWVNlh7"
   },
   "source": [
    "Now let's train the policy on the `CartPole-v0` environment for `N` steps. The JSON object returned by each call to `agent.train()` contains a lot of information we'll inspect below. For now, we'll extract three fields that help us gauge improvement, where typical early values are shown:\n",
    "\n",
    "```\n",
    "episode_reward_max: 71.0\n",
    "episode_reward_mean: 22.262569832402235\n",
    "episode_len_mean: 22.262569832402235\n",
    "\n",
    "```\n",
    "\n",
    "The _mean_ values are more useful for determining successful training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max reward: 54.0\n",
      "Max reward: 129.0\n",
      "Max reward: 185.0\n",
      "Max reward: 200.0\n",
      "Max reward: 200.0\n"
     ]
    }
   ],
   "source": [
    "N=5\n",
    "results = []\n",
    "episode_data = []\n",
    "episode_json = []\n",
    "for n in range(N):\n",
    "    result = agent.train()\n",
    "    results.append(result)\n",
    "    episode = {'n': n, \n",
    "               'episode_reward_mean': result['episode_reward_mean'], \n",
    "               'episode_reward_max': result['episode_reward_max'],  \n",
    "               'episode_len_mean': result['episode_len_mean']}    \n",
    "    episode_data.append(episode)\n",
    "    episode_json.append(json.dumps(episode))\n",
    "    print(f'Max reward: {episode[\"episode_reward_max\"]}')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's convert the episode data to a Pandas `DataFrame` for easy manipulation. The results indicate how much reward the policy is receiving (`episode_reward_*`) and how many time steps of the environment the policy ran (`episode_len_mean`). The maximum possible reward for this problem is `200`. The reward mean and trajectory length are very close because the agent receives a reward of one for every time step that it survives. However, this is specific to this environment and not true in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>episode_reward_mean</th>\n",
       "      <th>episode_reward_max</th>\n",
       "      <th>episode_len_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>20.963158</td>\n",
       "      <td>54.0</td>\n",
       "      <td>20.963158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>38.394231</td>\n",
       "      <td>129.0</td>\n",
       "      <td>38.394231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>59.880000</td>\n",
       "      <td>185.0</td>\n",
       "      <td>59.880000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>86.510000</td>\n",
       "      <td>200.0</td>\n",
       "      <td>86.510000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>115.340000</td>\n",
       "      <td>200.0</td>\n",
       "      <td>115.340000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n  episode_reward_mean  episode_reward_max  episode_len_mean\n",
       "0  0            20.963158                54.0         20.963158\n",
       "1  1            38.394231               129.0         38.394231\n",
       "2  2            59.880000               185.0         59.880000\n",
       "3  3            86.510000               200.0         86.510000\n",
       "4  4           115.340000               200.0        115.340000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data=episode_data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['n', 'episode_reward_mean', 'episode_reward_max', 'episode_len_mean']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1001\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"1001\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error() {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < css_urls.length; i++) {\n",
       "      var url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.0.1.min.js\": \"JpP8FXbgAZLkfur7LiK3j9AGBhHNIvF742meBJrjO2ShJDhCG2I1uVvW+0DUtrmc\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.0.1.min.js\": \"xZlADit0Q04ISQEdKg2k3L4W9AwQBAuDs9nJL9fM/WwzL1tEU9VPNezOFX0nLEAz\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.0.1.min.js\": \"4BuPRZkdMKSnj3zoxiNrQ86XgNw0rYmBOxe7nshquXwwcauupgBF2DHLVG1WuZlV\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.0.1.min.js\": \"Dv1SQ87hmDqK6S5OhBf0bCuwAEvL5QYL0PuR/F1SPVhCS/r/abjkbpKDYL2zeM19\"};\n",
       "\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      if (url in hashes) {\n",
       "        element.crossOrigin = \"anonymous\";\n",
       "        element.integrity = \"sha384-\" + hashes[url];\n",
       "      }\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };var element = document.getElementById(\"1001\");\n",
       "  if (element == null) {\n",
       "    console.error(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  \n",
       "  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.0.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.0.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.0.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.0.1.min.js\"];\n",
       "  var css_urls = [];\n",
       "  \n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "    \n",
       "    \n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if (root.Bokeh !== undefined || force === true) {\n",
       "      \n",
       "    for (var i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i].call(root, root.Bokeh);\n",
       "    }\n",
       "    if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1001\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.0.1.min.js\": \"JpP8FXbgAZLkfur7LiK3j9AGBhHNIvF742meBJrjO2ShJDhCG2I1uVvW+0DUtrmc\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.0.1.min.js\": \"xZlADit0Q04ISQEdKg2k3L4W9AwQBAuDs9nJL9fM/WwzL1tEU9VPNezOFX0nLEAz\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.0.1.min.js\": \"4BuPRZkdMKSnj3zoxiNrQ86XgNw0rYmBOxe7nshquXwwcauupgBF2DHLVG1WuZlV\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.0.1.min.js\": \"Dv1SQ87hmDqK6S5OhBf0bCuwAEvL5QYL0PuR/F1SPVhCS/r/abjkbpKDYL2zeM19\"};\n\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      if (url in hashes) {\n        element.crossOrigin = \"anonymous\";\n        element.integrity = \"sha384-\" + hashes[url];\n      }\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };var element = document.getElementById(\"1001\");\n  if (element == null) {\n    console.error(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n    return false;\n  }\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  \n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.0.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.0.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.0.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.0.1.min.js\"];\n  var css_urls = [];\n  \n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    function(Bokeh) {\n    \n    \n    }\n  ];\n\n  function run_inline_js() {\n    \n    if (root.Bokeh !== undefined || force === true) {\n      \n    for (var i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n    if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from bokeh.plotting import figure, output_file, show\n",
    "from bokeh.models import ColumnDataSource\n",
    "from bokeh.models.tools import HoverTool\n",
    "import bokeh.io\n",
    "# The next two lines prevent Bokeh from opening the graph in a new window.\n",
    "bokeh.io.reset_output()\n",
    "bokeh.io.output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the length and reward means are equal, we'll only plot one line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "  <div class=\"bk-root\" id=\"05991210-6881-4778-bfc9-b9f9e3751e95\" data-root-id=\"1003\"></div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "(function(root) {\n",
       "  function embed_document(root) {\n",
       "    \n",
       "  var docs_json = {\"1f968ff0-aea2-4fdf-a642-a7df7b129d30\":{\"roots\":{\"references\":[{\"attributes\":{\"below\":[{\"id\":\"1014\"}],\"center\":[{\"id\":\"1017\"},{\"id\":\"1021\"},{\"id\":\"1047\"}],\"left\":[{\"id\":\"1018\"}],\"renderers\":[{\"id\":\"1039\"},{\"id\":\"1052\"},{\"id\":\"1057\"},{\"id\":\"1071\"}],\"title\":{\"id\":\"1004\"},\"toolbar\":{\"id\":\"1029\"},\"x_range\":{\"id\":\"1006\"},\"x_scale\":{\"id\":\"1010\"},\"y_range\":{\"id\":\"1008\"},\"y_scale\":{\"id\":\"1012\"}},\"id\":\"1003\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{\"data_source\":{\"id\":\"1002\"},\"glyph\":{\"id\":\"1055\"},\"hover_glyph\":null,\"muted_glyph\":null,\"name\":\"Episode reward max\",\"nonselection_glyph\":{\"id\":\"1056\"},\"selection_glyph\":null,\"view\":{\"id\":\"1058\"}},\"id\":\"1057\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"callback\":null,\"tooltips\":[[\"n\",\"$x\"],[\"mean\",\"$y\"]]},\"id\":\"1073\",\"type\":\"HoverTool\"},{\"attributes\":{},\"id\":\"1019\",\"type\":\"BasicTicker\"},{\"attributes\":{\"axis\":{\"id\":\"1018\"},\"dimension\":1,\"grid_line_alpha\":0.2,\"ticker\":null},\"id\":\"1021\",\"type\":\"Grid\"},{\"attributes\":{\"source\":{\"id\":\"1002\"}},\"id\":\"1058\",\"type\":\"CDSView\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_inspect\":\"auto\",\"active_multi\":null,\"active_scroll\":\"auto\",\"active_tap\":\"auto\",\"tools\":[{\"id\":\"1022\"},{\"id\":\"1023\"},{\"id\":\"1024\"},{\"id\":\"1025\"},{\"id\":\"1026\"},{\"id\":\"1027\"},{\"id\":\"1073\"}]},\"id\":\"1029\",\"type\":\"Toolbar\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"green\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"green\"},\"size\":{\"units\":\"screen\",\"value\":8},\"x\":{\"field\":\"n\"},\"y\":{\"field\":\"episode_reward_max\"}},\"id\":\"1070\",\"type\":\"Circle\"},{\"attributes\":{\"fill_color\":{\"value\":\"green\"},\"line_color\":{\"value\":\"green\"},\"size\":{\"units\":\"screen\",\"value\":8},\"x\":{\"field\":\"n\"},\"y\":{\"field\":\"episode_reward_max\"}},\"id\":\"1069\",\"type\":\"Circle\"},{\"attributes\":{},\"id\":\"1064\",\"type\":\"Selection\"},{\"attributes\":{\"text\":\"Episode reward and length means/maxes\"},\"id\":\"1004\",\"type\":\"Title\"},{\"attributes\":{\"label\":{\"value\":\"Episode reward max\"},\"renderers\":[{\"id\":\"1057\"}]},\"id\":\"1067\",\"type\":\"LegendItem\"},{\"attributes\":{},\"id\":\"1045\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"line_alpha\":0.1,\"line_color\":\"blue\",\"x\":{\"field\":\"n\"},\"y\":{\"field\":\"episode_reward_mean\"}},\"id\":\"1038\",\"type\":\"Line\"},{\"attributes\":{\"data_source\":{\"id\":\"1002\"},\"glyph\":{\"id\":\"1069\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"1070\"},\"selection_glyph\":null,\"view\":{\"id\":\"1072\"}},\"id\":\"1071\",\"type\":\"GlyphRenderer\"},{\"attributes\":{},\"id\":\"1010\",\"type\":\"LinearScale\"},{\"attributes\":{\"line_color\":\"green\",\"x\":{\"field\":\"n\"},\"y\":{\"field\":\"episode_reward_max\"}},\"id\":\"1055\",\"type\":\"Line\"},{\"attributes\":{\"label\":{\"value\":\"Episode reward mean\"},\"renderers\":[{\"id\":\"1039\"}]},\"id\":\"1048\",\"type\":\"LegendItem\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"blue\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"blue\"},\"size\":{\"units\":\"screen\",\"value\":8},\"x\":{\"field\":\"n\"},\"y\":{\"field\":\"episode_reward_mean\"}},\"id\":\"1051\",\"type\":\"Circle\"},{\"attributes\":{},\"id\":\"1022\",\"type\":\"PanTool\"},{\"attributes\":{\"data_source\":{\"id\":\"1002\"},\"glyph\":{\"id\":\"1037\"},\"hover_glyph\":null,\"muted_glyph\":null,\"name\":\"Episode reward mean\",\"nonselection_glyph\":{\"id\":\"1038\"},\"selection_glyph\":null,\"view\":{\"id\":\"1040\"}},\"id\":\"1039\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"source\":{\"id\":\"1002\"}},\"id\":\"1072\",\"type\":\"CDSView\"},{\"attributes\":{},\"id\":\"1023\",\"type\":\"WheelZoomTool\"},{\"attributes\":{\"line_color\":\"blue\",\"x\":{\"field\":\"n\"},\"y\":{\"field\":\"episode_reward_mean\"}},\"id\":\"1037\",\"type\":\"Line\"},{\"attributes\":{\"fill_color\":{\"value\":\"blue\"},\"line_color\":{\"value\":\"blue\"},\"size\":{\"units\":\"screen\",\"value\":8},\"x\":{\"field\":\"n\"},\"y\":{\"field\":\"episode_reward_mean\"}},\"id\":\"1050\",\"type\":\"Circle\"},{\"attributes\":{},\"id\":\"1015\",\"type\":\"BasicTicker\"},{\"attributes\":{},\"id\":\"1043\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"items\":[{\"id\":\"1048\"},{\"id\":\"1067\"}],\"location\":\"top_left\"},\"id\":\"1047\",\"type\":\"Legend\"},{\"attributes\":{\"overlay\":{\"id\":\"1028\"}},\"id\":\"1024\",\"type\":\"BoxZoomTool\"},{\"attributes\":{\"bottom_units\":\"screen\",\"fill_alpha\":0.5,\"fill_color\":\"lightgrey\",\"left_units\":\"screen\",\"level\":\"overlay\",\"line_alpha\":1.0,\"line_color\":\"black\",\"line_dash\":[4,4],\"line_width\":2,\"render_mode\":\"css\",\"right_units\":\"screen\",\"top_units\":\"screen\"},\"id\":\"1028\",\"type\":\"BoxAnnotation\"},{\"attributes\":{},\"id\":\"1006\",\"type\":\"DataRange1d\"},{\"attributes\":{},\"id\":\"1025\",\"type\":\"SaveTool\"},{\"attributes\":{},\"id\":\"1008\",\"type\":\"DataRange1d\"},{\"attributes\":{},\"id\":\"1026\",\"type\":\"ResetTool\"},{\"attributes\":{\"source\":{\"id\":\"1002\"}},\"id\":\"1040\",\"type\":\"CDSView\"},{\"attributes\":{\"line_alpha\":0.1,\"line_color\":\"green\",\"x\":{\"field\":\"n\"},\"y\":{\"field\":\"episode_reward_max\"}},\"id\":\"1056\",\"type\":\"Line\"},{\"attributes\":{},\"id\":\"1027\",\"type\":\"HelpTool\"},{\"attributes\":{},\"id\":\"1012\",\"type\":\"LinearScale\"},{\"attributes\":{\"data\":{\"episode_len_mean\":{\"__ndarray__\":\"accKhJH2NEAndmIndjJDQHE9Ctej8E1AcT0K16OgVUD2KFyPwtVcQA==\",\"dtype\":\"float64\",\"shape\":[5]},\"episode_reward_max\":{\"__ndarray__\":\"AAAAAAAAS0AAAAAAACBgQAAAAAAAIGdAAAAAAAAAaUAAAAAAAABpQA==\",\"dtype\":\"float64\",\"shape\":[5]},\"episode_reward_mean\":{\"__ndarray__\":\"accKhJH2NEAndmIndjJDQHE9Ctej8E1AcT0K16OgVUD2KFyPwtVcQA==\",\"dtype\":\"float64\",\"shape\":[5]},\"index\":[0,1,2,3,4],\"n\":[0,1,2,3,4]},\"selected\":{\"id\":\"1064\"},\"selection_policy\":{\"id\":\"1065\"}},\"id\":\"1002\",\"type\":\"ColumnDataSource\"},{\"attributes\":{},\"id\":\"1065\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"axis_label\":\"n\",\"formatter\":{\"id\":\"1043\"},\"ticker\":{\"id\":\"1015\"}},\"id\":\"1014\",\"type\":\"LinearAxis\"},{\"attributes\":{\"axis_label\":\"value\",\"formatter\":{\"id\":\"1045\"},\"ticker\":{\"id\":\"1019\"}},\"id\":\"1018\",\"type\":\"LinearAxis\"},{\"attributes\":{\"source\":{\"id\":\"1002\"}},\"id\":\"1053\",\"type\":\"CDSView\"},{\"attributes\":{\"data_source\":{\"id\":\"1002\"},\"glyph\":{\"id\":\"1050\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"1051\"},\"selection_glyph\":null,\"view\":{\"id\":\"1053\"}},\"id\":\"1052\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"axis\":{\"id\":\"1014\"},\"grid_line_alpha\":0.2,\"ticker\":null},\"id\":\"1017\",\"type\":\"Grid\"}],\"root_ids\":[\"1003\"]},\"title\":\"Bokeh Application\",\"version\":\"2.0.1\"}};\n",
       "  var render_items = [{\"docid\":\"1f968ff0-aea2-4fdf-a642-a7df7b129d30\",\"root_ids\":[\"1003\"],\"roots\":{\"1003\":\"05991210-6881-4778-bfc9-b9f9e3751e95\"}}];\n",
       "  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "\n",
       "  }\n",
       "  if (root.Bokeh !== undefined) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (root.Bokeh !== undefined) {\n",
       "        clearInterval(timer);\n",
       "        embed_document(root);\n",
       "      } else {\n",
       "        attempts++;\n",
       "        if (attempts > 100) {\n",
       "          clearInterval(timer);\n",
       "          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
       "        }\n",
       "      }\n",
       "    }, 10, root)\n",
       "  }\n",
       "})(window);"
      ],
      "application/vnd.bokehjs_exec.v0+json": ""
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "id": "1003"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "source = ColumnDataSource(df)\n",
    "\n",
    "plot = figure(title='Episode reward and length means/maxes')\n",
    "plot.grid.grid_line_alpha=0.2\n",
    "plot.xaxis.axis_label = 'n'\n",
    "plot.yaxis.axis_label = 'value'\n",
    "\n",
    "plot.line(x='n', y='episode_reward_mean', source=source, color='blue', legend_label='Episode reward mean', name='Episode reward mean')\n",
    "plot.circle(x='n', y='episode_reward_mean', source=source, color='blue', size=8)\n",
    "plot.line(x='n', y='episode_reward_max', source=source, color='green', legend_label='Episode reward max', name='Episode reward max')\n",
    "plot.circle(x='n', y='episode_reward_max', source=source, color='green', size=8)\n",
    "plot.legend.location = \"top_left\"\n",
    "\n",
    "hover = HoverTool()\n",
    "hover.tooltips = [\n",
    "    (\"n\", \"$x\"),\n",
    "    (\"mean\", \"$y\")]\n",
    "plot.add_tools(hover)\n",
    "\n",
    "show(plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can't see the graph. [Here is an image](../images/rllib/episode-rewards-means-maxes.png) from one sample run.\n",
    "\n",
    "FYI, here are two views of the whole value for one results. First, a \"pretty print\" output.\n",
    "\n",
    "> **Tip:** The output will be long. When this happens for a cell, right click and select _Enable scrolling for outputs_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3o0wjdZ3Nlh7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom_metrics: {}\n",
      "date: 2020-06-12_09-38-05\n",
      "done: false\n",
      "episode_len_mean: 115.34\n",
      "episode_reward_max: 200.0\n",
      "episode_reward_mean: 115.34\n",
      "episode_reward_min: 10.0\n",
      "episodes_this_iter: 22\n",
      "episodes_total: 398\n",
      "experiment_id: abf747c1074141c2af82ba6b9eae7030\n",
      "hostname: DWAnyscaleMBP.local\n",
      "info:\n",
      "  grad_time_ms: 1240.148\n",
      "  learner:\n",
      "    default_policy:\n",
      "      cur_kl_coeff: 0.30000001192092896\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5840191841125488\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.006415305659174919\n",
      "      model: {}\n",
      "      policy_loss: -0.007684091571718454\n",
      "      total_loss: 841.7330932617188\n",
      "      vf_explained_var: 0.037385497242212296\n",
      "      vf_loss: 841.7388305664062\n",
      "  load_time_ms: 11.097\n",
      "  num_steps_sampled: 20000\n",
      "  num_steps_trained: 19840\n",
      "  sample_time_ms: 2469.437\n",
      "  update_time_ms: 95.303\n",
      "iterations_since_restore: 5\n",
      "node_ip: 192.168.1.149\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "optimizer_steps_this_iter: 1\n",
      "perf:\n",
      "  cpu_util_percent: 31.139999999999997\n",
      "  ram_util_percent: 69.0\n",
      "pid: 34657\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_env_wait_ms: 0.03897672400394885\n",
      "  mean_inference_ms: 0.4698414904748093\n",
      "  mean_processing_ms: 0.10408388820482366\n",
      "time_since_restore: 19.135781288146973\n",
      "time_this_iter_s: 3.482814073562622\n",
      "time_total_s: 19.135781288146973\n",
      "timestamp: 1591979885\n",
      "timesteps_since_restore: 20000\n",
      "timesteps_this_iter: 4000\n",
      "timesteps_total: 20000\n",
      "training_iteration: 5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(pretty_print(results[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll learn about more of these values as continue the tutorial.\n",
    "\n",
    "The whole, long JSON blob, which includes the historical stats about episode rewards and lengths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'episode_reward_max': 200.0,\n",
       " 'episode_reward_min': 10.0,\n",
       " 'episode_reward_mean': 115.34,\n",
       " 'episode_len_mean': 115.34,\n",
       " 'episodes_this_iter': 22,\n",
       " 'policy_reward_min': {},\n",
       " 'policy_reward_max': {},\n",
       " 'policy_reward_mean': {},\n",
       " 'custom_metrics': {},\n",
       " 'hist_stats': {'episode_reward': [200.0,\n",
       "   200.0,\n",
       "   200.0,\n",
       "   186.0,\n",
       "   137.0,\n",
       "   164.0,\n",
       "   163.0,\n",
       "   200.0,\n",
       "   200.0,\n",
       "   189.0,\n",
       "   200.0,\n",
       "   200.0,\n",
       "   200.0,\n",
       "   200.0,\n",
       "   153.0,\n",
       "   167.0,\n",
       "   140.0,\n",
       "   10.0,\n",
       "   200.0,\n",
       "   193.0,\n",
       "   200.0,\n",
       "   200.0,\n",
       "   50.0,\n",
       "   72.0,\n",
       "   58.0,\n",
       "   51.0,\n",
       "   16.0,\n",
       "   170.0,\n",
       "   75.0,\n",
       "   26.0,\n",
       "   31.0,\n",
       "   55.0,\n",
       "   66.0,\n",
       "   46.0,\n",
       "   135.0,\n",
       "   106.0,\n",
       "   119.0,\n",
       "   132.0,\n",
       "   64.0,\n",
       "   59.0,\n",
       "   130.0,\n",
       "   92.0,\n",
       "   154.0,\n",
       "   51.0,\n",
       "   85.0,\n",
       "   123.0,\n",
       "   99.0,\n",
       "   79.0,\n",
       "   79.0,\n",
       "   89.0,\n",
       "   60.0,\n",
       "   64.0,\n",
       "   23.0,\n",
       "   22.0,\n",
       "   32.0,\n",
       "   39.0,\n",
       "   130.0,\n",
       "   42.0,\n",
       "   72.0,\n",
       "   107.0,\n",
       "   43.0,\n",
       "   29.0,\n",
       "   74.0,\n",
       "   110.0,\n",
       "   52.0,\n",
       "   122.0,\n",
       "   185.0,\n",
       "   81.0,\n",
       "   56.0,\n",
       "   106.0,\n",
       "   200.0,\n",
       "   85.0,\n",
       "   84.0,\n",
       "   137.0,\n",
       "   17.0,\n",
       "   185.0,\n",
       "   200.0,\n",
       "   145.0,\n",
       "   33.0,\n",
       "   55.0,\n",
       "   72.0,\n",
       "   171.0,\n",
       "   133.0,\n",
       "   108.0,\n",
       "   145.0,\n",
       "   148.0,\n",
       "   146.0,\n",
       "   143.0,\n",
       "   53.0,\n",
       "   75.0,\n",
       "   200.0,\n",
       "   187.0,\n",
       "   164.0,\n",
       "   199.0,\n",
       "   82.0,\n",
       "   181.0,\n",
       "   200.0,\n",
       "   98.0,\n",
       "   65.0,\n",
       "   160.0],\n",
       "  'episode_lengths': [200,\n",
       "   200,\n",
       "   200,\n",
       "   186,\n",
       "   137,\n",
       "   164,\n",
       "   163,\n",
       "   200,\n",
       "   200,\n",
       "   189,\n",
       "   200,\n",
       "   200,\n",
       "   200,\n",
       "   200,\n",
       "   153,\n",
       "   167,\n",
       "   140,\n",
       "   10,\n",
       "   200,\n",
       "   193,\n",
       "   200,\n",
       "   200,\n",
       "   50,\n",
       "   72,\n",
       "   58,\n",
       "   51,\n",
       "   16,\n",
       "   170,\n",
       "   75,\n",
       "   26,\n",
       "   31,\n",
       "   55,\n",
       "   66,\n",
       "   46,\n",
       "   135,\n",
       "   106,\n",
       "   119,\n",
       "   132,\n",
       "   64,\n",
       "   59,\n",
       "   130,\n",
       "   92,\n",
       "   154,\n",
       "   51,\n",
       "   85,\n",
       "   123,\n",
       "   99,\n",
       "   79,\n",
       "   79,\n",
       "   89,\n",
       "   60,\n",
       "   64,\n",
       "   23,\n",
       "   22,\n",
       "   32,\n",
       "   39,\n",
       "   130,\n",
       "   42,\n",
       "   72,\n",
       "   107,\n",
       "   43,\n",
       "   29,\n",
       "   74,\n",
       "   110,\n",
       "   52,\n",
       "   122,\n",
       "   185,\n",
       "   81,\n",
       "   56,\n",
       "   106,\n",
       "   200,\n",
       "   85,\n",
       "   84,\n",
       "   137,\n",
       "   17,\n",
       "   185,\n",
       "   200,\n",
       "   145,\n",
       "   33,\n",
       "   55,\n",
       "   72,\n",
       "   171,\n",
       "   133,\n",
       "   108,\n",
       "   145,\n",
       "   148,\n",
       "   146,\n",
       "   143,\n",
       "   53,\n",
       "   75,\n",
       "   200,\n",
       "   187,\n",
       "   164,\n",
       "   199,\n",
       "   82,\n",
       "   181,\n",
       "   200,\n",
       "   98,\n",
       "   65,\n",
       "   160]},\n",
       " 'sampler_perf': {'mean_env_wait_ms': 0.03897672400394885,\n",
       "  'mean_processing_ms': 0.10408388820482366,\n",
       "  'mean_inference_ms': 0.4698414904748093},\n",
       " 'off_policy_estimator': {},\n",
       " 'info': {'num_steps_trained': 19840,\n",
       "  'num_steps_sampled': 20000,\n",
       "  'sample_time_ms': 2469.437,\n",
       "  'load_time_ms': 11.097,\n",
       "  'grad_time_ms': 1240.148,\n",
       "  'update_time_ms': 95.303,\n",
       "  'learner': {'default_policy': {'cur_kl_coeff': 0.30000001192092896,\n",
       "    'cur_lr': 4.999999873689376e-05,\n",
       "    'total_loss': 841.7331,\n",
       "    'policy_loss': -0.0076840916,\n",
       "    'vf_loss': 841.73883,\n",
       "    'vf_explained_var': 0.037385497,\n",
       "    'kl': 0.0064153057,\n",
       "    'entropy': 0.5840192,\n",
       "    'entropy_coeff': 0.0,\n",
       "    'model': {}}}},\n",
       " 'optimizer_steps_this_iter': 1,\n",
       " 'timesteps_this_iter': 4000,\n",
       " 'done': False,\n",
       " 'timesteps_total': 20000,\n",
       " 'episodes_total': 398,\n",
       " 'training_iteration': 5,\n",
       " 'experiment_id': 'abf747c1074141c2af82ba6b9eae7030',\n",
       " 'date': '2020-06-12_09-38-05',\n",
       " 'timestamp': 1591979885,\n",
       " 'time_this_iter_s': 3.482814073562622,\n",
       " 'time_total_s': 19.135781288146973,\n",
       " 'pid': 34657,\n",
       " 'hostname': 'DWAnyscaleMBP.local',\n",
       " 'node_ip': '192.168.1.149',\n",
       " 'config': {'num_workers': 1,\n",
       "  'num_envs_per_worker': 1,\n",
       "  'rollout_fragment_length': 200,\n",
       "  'sample_batch_size': -1,\n",
       "  'batch_mode': 'truncate_episodes',\n",
       "  'num_gpus': 0,\n",
       "  'train_batch_size': 4000,\n",
       "  'model': {'conv_filters': None,\n",
       "   'conv_activation': 'relu',\n",
       "   'fcnet_activation': 'tanh',\n",
       "   'fcnet_hiddens': [100, 100],\n",
       "   'free_log_std': False,\n",
       "   'no_final_linear': False,\n",
       "   'vf_share_layers': True,\n",
       "   'use_lstm': False,\n",
       "   'max_seq_len': 20,\n",
       "   'lstm_cell_size': 256,\n",
       "   'lstm_use_prev_action_reward': False,\n",
       "   'state_shape': None,\n",
       "   'framestack': True,\n",
       "   'dim': 84,\n",
       "   'grayscale': False,\n",
       "   'zero_mean': True,\n",
       "   'custom_model': None,\n",
       "   'custom_action_dist': None,\n",
       "   'custom_options': {},\n",
       "   'custom_preprocessor': None},\n",
       "  'optimizer': {},\n",
       "  'gamma': 0.99,\n",
       "  'horizon': None,\n",
       "  'soft_horizon': False,\n",
       "  'no_done_at_end': False,\n",
       "  'env_config': {},\n",
       "  'env': 'CartPole-v0',\n",
       "  'normalize_actions': False,\n",
       "  'clip_rewards': None,\n",
       "  'clip_actions': True,\n",
       "  'preprocessor_pref': 'deepmind',\n",
       "  'lr': 5e-05,\n",
       "  'monitor': False,\n",
       "  'log_level': 'WARN',\n",
       "  'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
       "  'ignore_worker_failures': False,\n",
       "  'log_sys_usage': True,\n",
       "  'use_pytorch': False,\n",
       "  'eager': False,\n",
       "  'eager_tracing': False,\n",
       "  'no_eager_on_workers': False,\n",
       "  'explore': True,\n",
       "  'exploration_config': {'type': 'StochasticSampling'},\n",
       "  'evaluation_interval': None,\n",
       "  'evaluation_num_episodes': 10,\n",
       "  'in_evaluation': False,\n",
       "  'evaluation_config': {},\n",
       "  'evaluation_num_workers': 0,\n",
       "  'custom_eval_function': None,\n",
       "  'use_exec_api': False,\n",
       "  'sample_async': False,\n",
       "  'observation_filter': 'NoFilter',\n",
       "  'synchronize_filters': True,\n",
       "  'tf_session_args': {'intra_op_parallelism_threads': 2,\n",
       "   'inter_op_parallelism_threads': 2,\n",
       "   'gpu_options': {'allow_growth': True},\n",
       "   'log_device_placement': False,\n",
       "   'device_count': {'CPU': 1},\n",
       "   'allow_soft_placement': True},\n",
       "  'local_tf_session_args': {'intra_op_parallelism_threads': 8,\n",
       "   'inter_op_parallelism_threads': 8},\n",
       "  'compress_observations': False,\n",
       "  'collect_metrics_timeout': 180,\n",
       "  'metrics_smoothing_episodes': 100,\n",
       "  'remote_worker_envs': False,\n",
       "  'remote_env_batch_wait_ms': 0,\n",
       "  'min_iter_time_s': 0,\n",
       "  'timesteps_per_iteration': 0,\n",
       "  'seed': None,\n",
       "  'extra_python_environs_for_driver': {},\n",
       "  'extra_python_environs_for_worker': {},\n",
       "  'num_cpus_per_worker': 0,\n",
       "  'num_gpus_per_worker': 0,\n",
       "  'custom_resources_per_worker': {},\n",
       "  'num_cpus_for_driver': 1,\n",
       "  'memory': 0,\n",
       "  'object_store_memory': 0,\n",
       "  'memory_per_worker': 0,\n",
       "  'object_store_memory_per_worker': 0,\n",
       "  'input': 'sampler',\n",
       "  'input_evaluation': ['is', 'wis'],\n",
       "  'postprocess_inputs': False,\n",
       "  'shuffle_buffer_size': 0,\n",
       "  'output': None,\n",
       "  'output_compress_columns': ['obs', 'new_obs'],\n",
       "  'output_max_file_size': 67108864,\n",
       "  'multiagent': {'policies': {},\n",
       "   'policy_mapping_fn': None,\n",
       "   'policies_to_train': None},\n",
       "  'use_critic': True,\n",
       "  'use_gae': True,\n",
       "  'lambda': 1.0,\n",
       "  'kl_coeff': 0.2,\n",
       "  'sgd_minibatch_size': 128,\n",
       "  'shuffle_sequences': True,\n",
       "  'num_sgd_iter': 30,\n",
       "  'lr_schedule': None,\n",
       "  'vf_share_layers': False,\n",
       "  'vf_loss_coeff': 1.0,\n",
       "  'entropy_coeff': 0.0,\n",
       "  'entropy_coeff_schedule': None,\n",
       "  'clip_param': 0.3,\n",
       "  'vf_clip_param': 10.0,\n",
       "  'grad_clip': None,\n",
       "  'kl_target': 0.01,\n",
       "  'simple_optimizer': False,\n",
       "  '_fake_gpus': False},\n",
       " 'time_since_restore': 19.135781288146973,\n",
       " 'timesteps_since_restore': 20000,\n",
       " 'iterations_since_restore': 5,\n",
       " 'perf': {'cpu_util_percent': 31.139999999999997, 'ram_util_percent': 69.0},\n",
       " 'num_healthy_workers': 1}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the `episode_reward` values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "  <div class=\"bk-root\" id=\"51cdf74f-1311-4d4b-9479-b04266d1f00b\" data-root-id=\"1131\"></div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "(function(root) {\n",
       "  function embed_document(root) {\n",
       "    \n",
       "  var docs_json = {\"8ea5d048-7385-4b9e-9055-7ce0993b2f9c\":{\"roots\":{\"references\":[{\"attributes\":{\"below\":[{\"id\":\"1142\"}],\"center\":[{\"id\":\"1145\"},{\"id\":\"1149\"}],\"left\":[{\"id\":\"1146\"}],\"renderers\":[{\"id\":\"1167\"},{\"id\":\"1172\"}],\"title\":{\"id\":\"1132\"},\"toolbar\":{\"id\":\"1157\"},\"x_range\":{\"id\":\"1134\"},\"x_scale\":{\"id\":\"1138\"},\"y_range\":{\"id\":\"1136\"},\"y_scale\":{\"id\":\"1140\"}},\"id\":\"1131\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{\"axis_label\":\"n\",\"formatter\":{\"id\":\"1186\"},\"ticker\":{\"id\":\"1143\"}},\"id\":\"1142\",\"type\":\"LinearAxis\"},{\"attributes\":{},\"id\":\"1136\",\"type\":\"DataRange1d\"},{\"attributes\":{\"source\":{\"id\":\"1169\"}},\"id\":\"1173\",\"type\":\"CDSView\"},{\"attributes\":{\"axis\":{\"id\":\"1146\"},\"dimension\":1,\"grid_line_alpha\":0.2,\"ticker\":null},\"id\":\"1149\",\"type\":\"Grid\"},{\"attributes\":{},\"id\":\"1192\",\"type\":\"UnionRenderers\"},{\"attributes\":{},\"id\":\"1138\",\"type\":\"LinearScale\"},{\"attributes\":{\"callback\":null,\"tooltips\":[[\"n\",\"$x\"],[\"reward\",\"$y\"]]},\"id\":\"1174\",\"type\":\"HoverTool\"},{\"attributes\":{\"bottom_units\":\"screen\",\"fill_alpha\":0.5,\"fill_color\":\"lightgrey\",\"left_units\":\"screen\",\"level\":\"overlay\",\"line_alpha\":1.0,\"line_color\":\"black\",\"line_dash\":[4,4],\"line_width\":2,\"render_mode\":\"css\",\"right_units\":\"screen\",\"top_units\":\"screen\"},\"id\":\"1156\",\"type\":\"BoxAnnotation\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"green\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"green\"},\"size\":{\"units\":\"screen\",\"value\":8},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"1171\",\"type\":\"Circle\"},{\"attributes\":{},\"id\":\"1154\",\"type\":\"ResetTool\"},{\"attributes\":{},\"id\":\"1188\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"data\":{\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99],\"y\":[200.0,200.0,200.0,186.0,137.0,164.0,163.0,200.0,200.0,189.0,200.0,200.0,200.0,200.0,153.0,167.0,140.0,10.0,200.0,193.0,200.0,200.0,50.0,72.0,58.0,51.0,16.0,170.0,75.0,26.0,31.0,55.0,66.0,46.0,135.0,106.0,119.0,132.0,64.0,59.0,130.0,92.0,154.0,51.0,85.0,123.0,99.0,79.0,79.0,89.0,60.0,64.0,23.0,22.0,32.0,39.0,130.0,42.0,72.0,107.0,43.0,29.0,74.0,110.0,52.0,122.0,185.0,81.0,56.0,106.0,200.0,85.0,84.0,137.0,17.0,185.0,200.0,145.0,33.0,55.0,72.0,171.0,133.0,108.0,145.0,148.0,146.0,143.0,53.0,75.0,200.0,187.0,164.0,199.0,82.0,181.0,200.0,98.0,65.0,160.0]},\"selected\":{\"id\":\"1191\"},\"selection_policy\":{\"id\":\"1192\"}},\"id\":\"1169\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"data\":{\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99],\"y\":[200.0,200.0,200.0,186.0,137.0,164.0,163.0,200.0,200.0,189.0,200.0,200.0,200.0,200.0,153.0,167.0,140.0,10.0,200.0,193.0,200.0,200.0,50.0,72.0,58.0,51.0,16.0,170.0,75.0,26.0,31.0,55.0,66.0,46.0,135.0,106.0,119.0,132.0,64.0,59.0,130.0,92.0,154.0,51.0,85.0,123.0,99.0,79.0,79.0,89.0,60.0,64.0,23.0,22.0,32.0,39.0,130.0,42.0,72.0,107.0,43.0,29.0,74.0,110.0,52.0,122.0,185.0,81.0,56.0,106.0,200.0,85.0,84.0,137.0,17.0,185.0,200.0,145.0,33.0,55.0,72.0,171.0,133.0,108.0,145.0,148.0,146.0,143.0,53.0,75.0,200.0,187.0,164.0,199.0,82.0,181.0,200.0,98.0,65.0,160.0]},\"selected\":{\"id\":\"1189\"},\"selection_policy\":{\"id\":\"1190\"}},\"id\":\"1164\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"axis_label\":\"reward\",\"formatter\":{\"id\":\"1188\"},\"ticker\":{\"id\":\"1147\"}},\"id\":\"1146\",\"type\":\"LinearAxis\"},{\"attributes\":{},\"id\":\"1189\",\"type\":\"Selection\"},{\"attributes\":{\"text\":\"Episode rewards\"},\"id\":\"1132\",\"type\":\"Title\"},{\"attributes\":{},\"id\":\"1134\",\"type\":\"DataRange1d\"},{\"attributes\":{},\"id\":\"1155\",\"type\":\"HelpTool\"},{\"attributes\":{\"data_source\":{\"id\":\"1164\"},\"glyph\":{\"id\":\"1165\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"1166\"},\"selection_glyph\":null,\"view\":{\"id\":\"1168\"}},\"id\":\"1167\",\"type\":\"GlyphRenderer\"},{\"attributes\":{},\"id\":\"1143\",\"type\":\"BasicTicker\"},{\"attributes\":{\"axis\":{\"id\":\"1142\"},\"grid_line_alpha\":0.2,\"ticker\":null},\"id\":\"1145\",\"type\":\"Grid\"},{\"attributes\":{},\"id\":\"1191\",\"type\":\"Selection\"},{\"attributes\":{\"overlay\":{\"id\":\"1156\"}},\"id\":\"1152\",\"type\":\"BoxZoomTool\"},{\"attributes\":{},\"id\":\"1190\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"data_source\":{\"id\":\"1169\"},\"glyph\":{\"id\":\"1170\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"1171\"},\"selection_glyph\":null,\"view\":{\"id\":\"1173\"}},\"id\":\"1172\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"line_color\":\"green\",\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"1165\",\"type\":\"Line\"},{\"attributes\":{\"fill_color\":{\"value\":\"green\"},\"line_color\":{\"value\":\"green\"},\"size\":{\"units\":\"screen\",\"value\":8},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"1170\",\"type\":\"Circle\"},{\"attributes\":{},\"id\":\"1151\",\"type\":\"WheelZoomTool\"},{\"attributes\":{},\"id\":\"1153\",\"type\":\"SaveTool\"},{\"attributes\":{},\"id\":\"1186\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_inspect\":\"auto\",\"active_multi\":null,\"active_scroll\":\"auto\",\"active_tap\":\"auto\",\"tools\":[{\"id\":\"1150\"},{\"id\":\"1151\"},{\"id\":\"1152\"},{\"id\":\"1153\"},{\"id\":\"1154\"},{\"id\":\"1155\"},{\"id\":\"1174\"}]},\"id\":\"1157\",\"type\":\"Toolbar\"},{\"attributes\":{},\"id\":\"1147\",\"type\":\"BasicTicker\"},{\"attributes\":{},\"id\":\"1140\",\"type\":\"LinearScale\"},{\"attributes\":{\"line_alpha\":0.1,\"line_color\":\"green\",\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"1166\",\"type\":\"Line\"},{\"attributes\":{},\"id\":\"1150\",\"type\":\"PanTool\"},{\"attributes\":{\"source\":{\"id\":\"1164\"}},\"id\":\"1168\",\"type\":\"CDSView\"}],\"root_ids\":[\"1131\"]},\"title\":\"Bokeh Application\",\"version\":\"2.0.1\"}};\n",
       "  var render_items = [{\"docid\":\"8ea5d048-7385-4b9e-9055-7ce0993b2f9c\",\"root_ids\":[\"1131\"],\"roots\":{\"1131\":\"51cdf74f-1311-4d4b-9479-b04266d1f00b\"}}];\n",
       "  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "\n",
       "  }\n",
       "  if (root.Bokeh !== undefined) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (root.Bokeh !== undefined) {\n",
       "        clearInterval(timer);\n",
       "        embed_document(root);\n",
       "      } else {\n",
       "        attempts++;\n",
       "        if (attempts > 100) {\n",
       "          clearInterval(timer);\n",
       "          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
       "        }\n",
       "      }\n",
       "    }, 10, root)\n",
       "  }\n",
       "})(window);"
      ],
      "application/vnd.bokehjs_exec.v0+json": ""
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "id": "1131"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot2 = figure(title='Episode rewards')\n",
    "plot2.grid.grid_line_alpha=0.2\n",
    "plot2.xaxis.axis_label = 'n'\n",
    "plot2.yaxis.axis_label = 'reward'\n",
    "episode_rewards = results[-1]['hist_stats']['episode_reward']\n",
    "plot2.line(x=range(len(episode_rewards)), y=episode_rewards, color='green')\n",
    "plot2.circle(x=range(len(episode_rewards)), y=episode_rewards, color='green', size=8)\n",
    "\n",
    "hover2 = HoverTool()\n",
    "hover2.tooltips = [\n",
    "    (\"n\", \"$x\"),\n",
    "    (\"reward\", \"$y\")]\n",
    "plot2.add_tools(hover2)\n",
    "\n",
    "show(plot2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "([image](../images/rllib/episode-rewards.png))\n",
    "\n",
    "Some runs do very well while others don't..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FPdkWLrENlh9"
   },
   "source": [
    "### Exercise 2\n",
    "\n",
    "The current network and training configuration are too large and heavy-duty for a simple problem like CartPole. Modify the configuration to use a smaller network and to speed up the optimization of the surrogate objective. (Fewer SGD iterations and a larger batch size should help.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3lp6tqkNNlh9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-12 09:38:54,826\tINFO trainable.py:217 -- Getting current IP.\n",
      "2020-06-12 09:38:54,827\tWARNING util.py:37 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "config = DEFAULT_CONFIG.copy()\n",
    "config['num_workers'] = 3\n",
    "config['num_sgd_iter'] = 30\n",
    "config['sgd_minibatch_size'] = 128\n",
    "config['model']['fcnet_hiddens'] = [100, 100]\n",
    "config['num_cpus_per_worker'] = 0\n",
    "\n",
    "agent = PPOTrainer(config, 'CartPole-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "64FmVP7kNlh_"
   },
   "source": [
    "Train the agent and try to get a reward of 200. If it's training too slowly you may need to modify the config above to use fewer hidden units, a larger `sgd_minibatch_size`, a smaller `num_sgd_iter`, or a larger `num_workers`.\n",
    "\n",
    "This should take around `N` = 20 or 30 training iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XB7sdKUzNliA",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max reward: 66.0\n",
      "Max reward: 103.0\n",
      "Max reward: 200.0\n",
      "Max reward: 200.0\n",
      "Max reward: 200.0\n"
     ]
    }
   ],
   "source": [
    "N=5\n",
    "results = []\n",
    "episode_data = []\n",
    "episode_json = []\n",
    "for n in range(N):\n",
    "    result = agent.train()\n",
    "    results.append(result)\n",
    "    episode = {'n': n, \n",
    "               'episode_reward_mean': result['episode_reward_mean'], \n",
    "               'episode_reward_max': result['episode_reward_max'],  \n",
    "               'episode_len_mean': result['episode_len_mean']}    \n",
    "    episode_data.append(episode)\n",
    "    episode_json.append(json.dumps(episode))\n",
    "    print(f'Max reward: {episode[\"episode_reward_max\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PW6bN9CYNliB"
   },
   "source": [
    "# Using Model Checkpoints\n",
    "\n",
    "Checkpoint the current model. The call to `agent.save()` returns the path to the checkpointed model file, which can be used later to restore the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6uf808LMNliC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/deanwampler/ray_results/PPO_CartPole-v0_2020-06-12_09-38-52v04r6vuz/checkpoint_5/checkpoint-5\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = agent.save()\n",
    "print(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "05icI8bfNliD"
   },
   "source": [
    "Now let's use the trained policy to make predictions.\n",
    "\n",
    "> **Note:** Here we are loading the trained policy in the same process, but in practice, this would normally be done in a different process, for example on a production cluster separate from the training cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8Qq2_AYVNliE"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-12 09:47:03,656\tINFO trainable.py:217 -- Getting current IP.\n",
      "2020-06-12 09:47:03,657\tWARNING util.py:37 -- Install gputil for GPU system monitoring.\n",
      "2020-06-12 09:47:03,720\tINFO trainable.py:217 -- Getting current IP.\n",
      "2020-06-12 09:47:03,721\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/PPO_CartPole-v0_2020-06-12_09-38-52v04r6vuz/checkpoint_5/checkpoint-5\n",
      "2020-06-12 09:47:03,722\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 5, '_timesteps_total': 20000, '_time_total': 15.386901617050171, '_episodes_total': 400}\n"
     ]
    }
   ],
   "source": [
    "trained_config = config.copy()\n",
    "\n",
    "test_agent = PPOTrainer(trained_config, 'CartPole-v0')\n",
    "test_agent.restore(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c2gUUlqkNliG"
   },
   "source": [
    "Now use the trained policy to act in an environment. The key line is the call to `test_agent.compute_action(state)` which uses the trained policy to choose an action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c2gUUlqkNliG"
   },
   "source": [
    "Verify that the cummulative reward received roughly matches up with the reward printed above. It will be at or near 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9asL5Z5lNliH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "state = env.reset()\n",
    "done = False\n",
    "cumulative_reward = 0\n",
    "\n",
    "while not done:\n",
    "    action = test_agent.compute_action(state)  # key line; get the next action\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    cumulative_reward += reward\n",
    "\n",
    "print(cumulative_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next lesson, [02: Introduction to RLlib](02-Introduction-to-RLlib.ipynb) steps back to introduce to RLlib, its goals and the capabilities it provides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Copy of RLlib Tutorial",
   "provenance": []
  },
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
