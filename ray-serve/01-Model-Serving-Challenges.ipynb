{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ray Serve - Model Serving Challenges\n",
    "\n",
    "Â© 2019-2020, Anyscale. All Rights Reserved\n",
    "\n",
    "![Anyscale Academy](../images/AnyscaleAcademy_Logo_clearbanner_141x100.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges of Model Serving\n",
    "\n",
    "Model development happens in a data science research environment. There are many challenges, but also tools at the data scientists disposal.\n",
    "\n",
    "Model deployment to production faces an entirely different set of challenges and requires different tools, although it is desirable to bridge the divide as much as possible.\n",
    "\n",
    "Here is a partial lists of the challenges of model serving:\n",
    "\n",
    "### It Should Be Framework Agnostic\n",
    "\n",
    "Model serving frameworks must be able to serve models from popular systems like TensorFlow, PyTorch, scikit-learn, or even arbitrary Python functions. Even within the same organization, it is common to use several machine learning frameworks. \n",
    "\n",
    "Also, machine learning models are typically surrounded by lots of application logic. For example, some model serving is implemented as a RESTful service to which scoring requests are made. Often this is too restrictive, as some additional processing may be desired as part of the scoring process, and the performance overhead of remote calls may be suboptimal.\n",
    "\n",
    "### Pure Python\n",
    "\n",
    "It has been common recently for model serving to be done using JVM-based systems, since many production enterprises are JVM-based. This is a disadvantage when model training and other data processing are done using Python tools, only. \n",
    "\n",
    "In general, model serving should be intuitive for developers and simple to configure and run. Hence, it is desirable to use pure Python and to avoid verbose configurations using YAML files or other means. \n",
    "\n",
    "Data scientists and engineers use Python to develop their machine learning models, so they should also be able to use Python to deploy their machine learning applications. This need is growing more critical as online learning applications combine training and serving in the same applications.\n",
    "\n",
    "### Simple and Scalable\n",
    "\n",
    "Model serving must be simple to scale on demand across many machines. It must also be easy to upgrade models dynamically, over time. Achieving production uptime and performance requirements are essential for success.\n",
    "\n",
    "### DevOps Integrations\n",
    "\n",
    "Model serving deployments need to integrate with existing \"DevOps\" CI/CD practices for controlled, audited, and predicatble releases. Patterns like [Canary Releases](https://martinfowler.com/bliki/CanaryRelease.html) are particularly useful for testing the efficacy of a new model before replacing existing models, just as this pattern is useful for other software deployments.\n",
    "\n",
    "### Flexible Deployment Patterns\n",
    "\n",
    "There are unique deployment patterns, too. For example, it should be easy to deploy a forest of models, to split traffic to different instances, and to score data in batches for greater efficiency.\n",
    "\n",
    "See also this [Ray blog post](https://medium.com/distributed-computing-with-ray/the-simplest-way-to-serve-your-nlp-model-in-production-with-pure-python-d42b6a97ad55) on the challenges of model serving and the way Ray Serve addresses them. It also provides an example of starting with a simple model, then deploying a more sophisticated model into the running application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Ray Serve?\n",
    "\n",
    "[Ray Serve](https://docs.ray.io/en/latest/serve/index.html) is a scalable model-serving library built on [Ray](https://ray.io).\n",
    "\n",
    "For users, Ray Serve offers these benefits:\n",
    "\n",
    "* **Framework Agnostic**: You can use the same toolkit to serve everything from deep learning models built with [PyTorch](https://docs.ray.io/en/latest/serve/tutorials/pytorch.html#serve-pytorch-tutorial), [Tensorflow](https://docs.ray.io/en/latest/serve/tutorials/tensorflow.html#serve-tensorflow-tutorial), or [Keras](https://docs.ray.io/en/latest/serve/tutorials/tensorflow.html#serve-tensorflow-tutorial), to [scikit-Learn](https://docs.ray.io/en/latest/serve/tutorials/sklearn.html#serve-sklearn-tutorial) models, to arbitrary business logic.\n",
    "* **Python First:** Configure your model serving with pure Python code. No YAML or JSON configurations required.\n",
    "\n",
    "As a library, Ray Serve enables the following:\n",
    "\n",
    "* [Splitting traffic between backends dynamically](https://docs.ray.io/en/latest/serve/advanced.html#serve-split-traffic) with zero downtime. This is accomplished by decoupling routing logic from response handling logic.\n",
    "* [Support for batching](https://docs.ray.io/en/latest/serve/advanced.html#serve-batching) to improve performance helps you meet your performance objectives. You can also use a model for batch and online processing.\n",
    "\n",
    "Since Serve is built on Ray, it also allows you to scale to many machines, in your datacenter or in cloud environments, and it allows you to leverage all of the other Ray frameworks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two Simple Ray Serve Examples\n",
    "\n",
    "We'll explore a more detailed example in the next lesson. Here we explore how simple deployments are simple with Ray Serve! We will first use a function that does \"scoring\", then a class.\n",
    "\n",
    "But first, initialize Ray as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Ray is already running.\n"
     ]
    }
   ],
   "source": [
    "!../tools/start-ray.sh --check --verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray import serve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '192.168.1.149',\n",
       " 'raylet_ip_address': '192.168.1.149',\n",
       " 'redis_address': '192.168.1.149:6379',\n",
       " 'object_store_address': '/tmp/ray/session_2020-07-19_08-56-14_461147_28318/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2020-07-19_08-56-14_461147_28318/sockets/raylet',\n",
       " 'webui_url': 'localhost:8265',\n",
       " 'session_dir': '/tmp/ray/session_2020-07-19_08-56-14_461147_28318'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init(address='auto', ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that Serve leverages the [Flask API](https://flask.palletsprojects.com/en/1.1.x/api/), which is often familiar, as it is a natural first approach for deploying models as RESTful services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests  # for making web requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-19 12:26:53,832\tWARNING import_thread.py:136 -- The actor 'TrainMNIST' has been exported 100 times. It's possible that this warning is accidental, but this may indicate that the same remote function is being defined repeatedly from within many tasks and exported to all of the workers. This can be a performance issue and can be resolved by defining the remote function on the driver instead. See https://github.com/ray-project/ray/issues/6240 for more discussion.\n"
     ]
    }
   ],
   "source": [
    "serve.init(name='serve-example-1')  # Name for this Serve instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def echo(flask_request):          # Uses the Flask API \n",
    "    return \"hello \" + flask_request.args.get(\"name\", \"serve!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "serve.create_backend(\"hello\", echo)\n",
    "serve.create_endpoint(\"hello\", backend=\"hello\", route=\"/hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0: hello request_0\n",
      " 1: hello request_1\n",
      " 2: hello request_2\n",
      " 3: hello request_3\n",
      " 4: hello request_4\n",
      " 5: hello request_5\n",
      " 6: hello request_6\n",
      " 7: hello request_7\n",
      " 8: hello request_8\n",
      " 9: hello request_9\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    response = requests.get(f\"http://127.0.0.1:8000/hello?name=request_{i}\").text\n",
    "    print(f'{i:2d}: {response}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see `hello request_N` in the output. Try making `requests.get()` invocations without the `?name=request_{i}` parameter. You should see `hello serve!`.\n",
    "\n",
    "We'll explain the concepts of _backends_ and _endpoints_ below. \n",
    "\n",
    "Now let's serve another \"model\" in the same service:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Counter:\n",
    "    def __init__(self, initial_count = 0):\n",
    "        self.count = initial_count\n",
    "\n",
    "    def __call__(self, flask_request):\n",
    "        self.count += 1\n",
    "        return {\"current_counter\": self.count, \"args\": flask_request.args}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we create the _backend_, we can pass constructor arguments after the label and the name of the class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "serve.create_backend(\"counter\", Counter, 0)  # initial_count = 0\n",
    "serve.create_endpoint(\"counter\", backend=\"counter\", route=\"/counter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0: {'current_counter': 21, 'args': {'i': '0'}}\n",
      " 1: {'current_counter': 22, 'args': {'i': '1'}}\n",
      " 2: {'current_counter': 23, 'args': {'i': '2'}}\n",
      " 3: {'current_counter': 24, 'args': {'i': '3'}}\n",
      " 4: {'current_counter': 25, 'args': {'i': '4'}}\n",
      " 5: {'current_counter': 26, 'args': {'i': '5'}}\n",
      " 6: {'current_counter': 27, 'args': {'i': '6'}}\n",
      " 7: {'current_counter': 28, 'args': {'i': '7'}}\n",
      " 8: {'current_counter': 29, 'args': {'i': '8'}}\n",
      " 9: {'current_counter': 30, 'args': {'i': '9'}}\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    response = requests.get(f\"http://127.0.0.1:8000/counter?i={i}\").json()\n",
    "    print(f'{i:2d}: {response}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise - Add Another New Backend and Endpoint\n",
    "\n",
    "Using either a function or a stateful class, add another _backend_ and _endpoint_, then try it out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ray Serve Concepts\n",
    "\n",
    "Let's explain _backends_ and _endpoints_.\n",
    "\n",
    "For more details, see this [key concepts](https://docs.ray.io/en/latest/serve/key-concepts.html) documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backends\n",
    "\n",
    "Backends define the implementation of your business logic or models that will handle requests when queries come in to _endpoints._ \n",
    "\n",
    "To define a backend, first define the âhandlerâ or business logic that will take requests and construct responses. Specifically, the handler should take as input a [Flask Request object](https://flask.palletsprojects.com/en/1.1.x/api/?highlight=request#flask.Request) and return any JSON-serializable object as output. \n",
    "\n",
    "Use a function when your response is _stateless_ and a class when your response is _stateful_ (although the class instances could be stateless, of course). Another advantage of using a class is the ability to specify constructor arguments in `serve.create_backend`, as was shown in the `counter` example above.\n",
    "\n",
    "Finally, a backend is defined using `serve.create_backend`, specifying a logical, unique name, and the handler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can list all defined backends and delete them to reclaim resources. However, a backend cannot be deleted while it is in use by an endpoint, because then traffic to an endpoint could not be handled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hello': {'accepts_batches': False,\n",
       "  'num_replicas': 1,\n",
       "  'max_batch_size': None},\n",
       " 'counter': {'accepts_batches': False,\n",
       "  'num_replicas': 1,\n",
       "  'max_batch_size': None},\n",
       " 'counter_toss': {'accepts_batches': False,\n",
       "  'num_replicas': 1,\n",
       "  'max_batch_size': None}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serve.create_backend(\"counter_toss\", Counter, 0)\n",
    "serve.list_backends()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "serve.delete_backend(\"counter_toss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hello': {'accepts_batches': False,\n",
       "  'num_replicas': 1,\n",
       "  'max_batch_size': None},\n",
       " 'counter': {'accepts_batches': False,\n",
       "  'num_replicas': 1,\n",
       "  'max_batch_size': None}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serve.list_backends()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Endpoints\n",
    "\n",
    "While a backend defines the request handling logic, an endpoint allows you to expose a backend via HTTP. Endpoints are âlogicalâ and can have one or multiple backends that serve requests to them. \n",
    "\n",
    "To create an endpoint, you specify a name for the endpoint, the name of a backend to handle requests to the endpoint, and the route and the list of HTTP methods (e.g., `[GET]`, which is the default) where it will be accesible. By default endpoints are serviced only by the backend provided to `serve.create_endpoint`, but in some cases you may want to specify multiple backends for an endpoint, e.g., for A/B testing or incremental rollout. For information on traffic splitting, please see [Splitting Traffic Between Backends](https://docs.ray.io/en/latest/serve/advanced.html#serve-split-traffic).\n",
    "\n",
    "Let's define a second endpoint for our `hello` backend, this one providing `POST` access. (We could have defined the original `hello` endpoint to support `POST` and `GET` using `methods = ['POST', 'GET']`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "serve.create_endpoint(\"post_hello\", backend=\"hello\", route=\"/post_hello\", methods=[\"POST\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['hello', 'counter', 'post_hello']),\n",
       " {'hello': {'route': '/hello', 'methods': ['GET'], 'traffic': {'hello': 1.0}},\n",
       "  'counter': {'route': '/counter',\n",
       "   'methods': ['GET'],\n",
       "   'traffic': {'counter': 1.0}},\n",
       "  'post_hello': {'route': '/post_hello',\n",
       "   'methods': ['POST'],\n",
       "   'traffic': {'hello': 1.0}}})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eds = serve.list_endpoints()\n",
    "eds.keys(), eds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=32967)\u001b[0m 2020-07-19 12:28:03,336\tINFO (unknown file):0 -- gc.collect() freed 10 refs in 0.12182619399936812 seconds\n",
      "\u001b[2m\u001b[36m(pid=32970)\u001b[0m 2020-07-19 12:28:03,328\tINFO (unknown file):0 -- gc.collect() freed 50 refs in 0.1284467939995011 seconds\n",
      "\u001b[2m\u001b[36m(pid=32965)\u001b[0m 2020-07-19 12:28:03,341\tINFO (unknown file):0 -- gc.collect() freed 10 refs in 0.09797865200016531 seconds\n",
      "\u001b[2m\u001b[36m(pid=35708)\u001b[0m 2020-07-19 12:28:03,402\tINFO (unknown file):0 -- gc.collect() freed 24 refs in 0.069170064999998 seconds\n",
      "\u001b[2m\u001b[36m(pid=35707)\u001b[0m 2020-07-19 12:28:03,406\tINFO (unknown file):0 -- gc.collect() freed 24 refs in 0.06263057399999994 seconds\n",
      "\u001b[2m\u001b[36m(pid=35705)\u001b[0m 2020-07-19 12:28:06,177\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_6_2020-07-19_12-27-44yalgnoh8/tmpa3icubdorestore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=35705)\u001b[0m 2020-07-19 12:28:06,177\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 20, '_timesteps_total': None, '_time_total': 5.945605278015137, '_episodes_total': None}\n",
      "\u001b[2m\u001b[36m(pid=35693)\u001b[0m 2020-07-19 12:28:10,238\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_4_2020-07-19_12-27-448ta7yf03/tmp5hl9rdterestore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=35693)\u001b[0m 2020-07-19 12:28:10,238\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 40, '_timesteps_total': None, '_time_total': 11.75360107421875, '_episodes_total': None}\n",
      "\u001b[2m\u001b[36m(pid=35700)\u001b[0m 2020-07-19 12:28:14,354\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_6_2020-07-19_12-27-44yalgnoh8/tmpykkux59rrestore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=35700)\u001b[0m 2020-07-19 12:28:14,355\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 50, '_timesteps_total': None, '_time_total': 14.572225570678711, '_episodes_total': None}\n",
      "\u001b[2m\u001b[36m(pid=35706)\u001b[0m 2020-07-19 12:28:19,336\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_4_2020-07-19_12-27-448ta7yf03/tmpbvdnpu2vrestore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=35706)\u001b[0m 2020-07-19 12:28:19,337\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 80, '_timesteps_total': None, '_time_total': 23.32195734977722, '_episodes_total': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-19 12:28:22,480\tWARNING worker.py:1047 -- WARNING: 29 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2020-07-19 12:28:22,480\tWARNING worker.py:1047 -- WARNING: 30 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2020-07-19 12:28:22,481\tWARNING worker.py:1047 -- WARNING: 31 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2020-07-19 12:28:22,482\tWARNING worker.py:1047 -- WARNING: 32 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=35698)\u001b[0m 2020-07-19 12:28:22,307\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_5_2020-07-19_12-27-44ljj1h24e/tmpruyf3jhgrestore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=35698)\u001b[0m 2020-07-19 12:28:22,308\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 60, '_timesteps_total': None, '_time_total': 17.396848917007446, '_episodes_total': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-19 12:28:23,195\tWARNING worker.py:1047 -- WARNING: 33 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2020-07-19 12:28:23,229\tWARNING worker.py:1047 -- WARNING: 34 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2020-07-19 12:28:23,230\tWARNING worker.py:1047 -- WARNING: 35 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2020-07-19 12:28:23,232\tWARNING worker.py:1047 -- WARNING: 36 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=35698)\u001b[0m 2020-07-19 12:28:23,520\tINFO (unknown file):0 -- gc.collect() freed 24 refs in 0.185089112 seconds\n",
      "\u001b[2m\u001b[36m(pid=35700)\u001b[0m 2020-07-19 12:28:23,562\tINFO (unknown file):0 -- gc.collect() freed 24 refs in 0.22155145800000042 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-19 12:28:24,105\tWARNING worker.py:1047 -- WARNING: 36 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2020-07-19 12:28:24,106\tWARNING worker.py:1047 -- WARNING: 37 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=35742)\u001b[0m 2020-07-19 12:28:26,860\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_0_2020-07-19_12-27-44scip0vwj/tmpp5jeuscdrestore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=35742)\u001b[0m 2020-07-19 12:28:26,860\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 100, '_timesteps_total': None, '_time_total': 29.10167098045349, '_episodes_total': None}\n",
      "\u001b[2m\u001b[36m(pid=35741)\u001b[0m 2020-07-19 12:28:26,931\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_4_2020-07-19_12-27-448ta7yf03/tmp37at2jwcrestore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=35741)\u001b[0m 2020-07-19 12:28:26,932\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 70, '_timesteps_total': None, '_time_total': 20.65435791015625, '_episodes_total': None}\n",
      "\u001b[2m\u001b[36m(pid=35750)\u001b[0m 2020-07-19 12:28:29,236\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_5_2020-07-19_12-27-44ljj1h24e/tmpod9tp3hyrestore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=35750)\u001b[0m 2020-07-19 12:28:29,236\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 80, '_timesteps_total': None, '_time_total': 24.729949712753296, '_episodes_total': None}\n",
      "\u001b[2m\u001b[36m(pid=35749)\u001b[0m 2020-07-19 12:28:36,202\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_7_2020-07-19_12-27-44fo011hk8/tmpe3uba7ktrestore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=35749)\u001b[0m 2020-07-19 12:28:36,202\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 90, '_timesteps_total': None, '_time_total': 25.877020359039307, '_episodes_total': None}\n",
      "\u001b[2m\u001b[36m(pid=35747)\u001b[0m 2020-07-19 12:28:41,257\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_4_2020-07-19_12-27-448ta7yf03/tmpd1mb0fbyrestore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=35747)\u001b[0m 2020-07-19 12:28:41,258\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 100, '_timesteps_total': None, '_time_total': 32.648266077041626, '_episodes_total': None}\n",
      "\u001b[2m\u001b[36m(pid=35747)\u001b[0m 2020-07-19 12:28:43,729\tINFO (unknown file):0 -- gc.collect() freed 24 refs in 0.10742990600000013 seconds\n",
      "\u001b[2m\u001b[36m(pid=35748)\u001b[0m 2020-07-19 12:28:45,030\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_7_2020-07-19_12-27-44fo011hk8/tmpyhw6jap2restore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=35748)\u001b[0m 2020-07-19 12:28:45,030\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 110, '_timesteps_total': None, '_time_total': 36.3375506401062, '_episodes_total': None}\n",
      "\u001b[2m\u001b[36m(pid=35746)\u001b[0m 2020-07-19 12:28:47,822\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_4_2020-07-19_12-27-448ta7yf03/tmpghzzmwwhrestore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=35746)\u001b[0m 2020-07-19 12:28:47,823\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 130, '_timesteps_total': None, '_time_total': 42.88013410568237, '_episodes_total': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-19 12:28:53,351\tWARNING worker.py:1047 -- WARNING: 29 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2020-07-19 12:28:53,362\tWARNING worker.py:1047 -- WARNING: 30 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2020-07-19 12:28:54,358\tWARNING worker.py:1047 -- WARNING: 31 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2020-07-19 12:28:54,359\tWARNING worker.py:1047 -- WARNING: 32 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=35744)\u001b[0m 2020-07-19 12:28:54,387\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_7_2020-07-19_12-27-44fo011hk8/tmp2iupk4nurestore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=35744)\u001b[0m 2020-07-19 12:28:54,387\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 170, '_timesteps_total': None, '_time_total': 54.18228888511658, '_episodes_total': None}\n",
      "\u001b[2m\u001b[36m(pid=35813)\u001b[0m 2020-07-19 12:28:56,571\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_2_2020-07-19_12-27-44y6xgdfnx/tmp7jlvpey0restore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=35813)\u001b[0m 2020-07-19 12:28:56,571\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 180, '_timesteps_total': None, '_time_total': 57.54036355018616, '_episodes_total': None}\n",
      "\u001b[2m\u001b[36m(pid=35814)\u001b[0m 2020-07-19 12:29:00,920\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_3_2020-07-19_12-27-44rbbnbqo9/tmp5qnpf0j7restore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=35814)\u001b[0m 2020-07-19 12:29:00,920\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 180, '_timesteps_total': None, '_time_total': 57.54036355018616, '_episodes_total': None}\n",
      "\u001b[2m\u001b[36m(pid=35744)\u001b[0m 2020-07-19 12:29:03,884\tINFO (unknown file):0 -- gc.collect() freed 24 refs in 0.26365058900000093 seconds\n",
      "\u001b[2m\u001b[36m(pid=35746)\u001b[0m 2020-07-19 12:29:03,892\tINFO (unknown file):0 -- gc.collect() freed 24 refs in 0.2655831979999945 seconds\n",
      "\u001b[2m\u001b[36m(pid=35815)\u001b[0m 2020-07-19 12:29:05,470\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_2_2020-07-19_12-27-44y6xgdfnx/tmp6i_hgbwxrestore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=35815)\u001b[0m 2020-07-19 12:29:05,470\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 200, '_timesteps_total': None, '_time_total': 64.6800332069397, '_episodes_total': None}\n",
      "\u001b[2m\u001b[36m(pid=35812)\u001b[0m 2020-07-19 12:29:09,636\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_3_2020-07-19_12-27-44rbbnbqo9/tmpmd6ltqfyrestore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=35812)\u001b[0m 2020-07-19 12:29:09,636\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 180, '_timesteps_total': None, '_time_total': 58.041619062423706, '_episodes_total': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-19 12:29:11,543\tWARNING worker.py:1047 -- WARNING: 29 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2020-07-19 12:29:13,801\tWARNING worker.py:1047 -- WARNING: 29 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=35829)\u001b[0m 2020-07-19 12:29:15,610\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_2_2020-07-19_12-27-44y6xgdfnx/tmphiie1jborestore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=35829)\u001b[0m 2020-07-19 12:29:15,610\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 230, '_timesteps_total': None, '_time_total': 73.58340334892273, '_episodes_total': None}\n",
      "\u001b[2m\u001b[36m(pid=35833)\u001b[0m 2020-07-19 12:29:17,282\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_3_2020-07-19_12-27-44rbbnbqo9/tmpyo4e1mnkrestore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=35833)\u001b[0m 2020-07-19 12:29:17,282\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 220, '_timesteps_total': None, '_time_total': 69.50588965415955, '_episodes_total': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-19 12:29:19,735\tWARNING worker.py:1047 -- WARNING: 29 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2020-07-19 12:29:21,508\tWARNING worker.py:1047 -- WARNING: 29 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=35837)\u001b[0m 2020-07-19 12:29:23,103\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_2_2020-07-19_12-27-44y6xgdfnx/tmpj88oleghrestore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=35837)\u001b[0m 2020-07-19 12:29:23,103\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 220, '_timesteps_total': None, '_time_total': 74.05315923690796, '_episodes_total': None}\n",
      "\u001b[2m\u001b[36m(pid=35841)\u001b[0m 2020-07-19 12:29:24,633\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_3_2020-07-19_12-27-44rbbnbqo9/tmpr33tmpnmrestore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=35841)\u001b[0m 2020-07-19 12:29:24,634\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 220, '_timesteps_total': None, '_time_total': 74.05315923690796, '_episodes_total': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-19 12:29:30,194\tWARNING worker.py:1047 -- WARNING: 28 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=35852)\u001b[0m 2020-07-19 12:29:32,456\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_2_2020-07-19_12-27-44y6xgdfnx/tmpgj2butqfrestore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=35852)\u001b[0m 2020-07-19 12:29:32,456\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 290, '_timesteps_total': None, '_time_total': 95.7921245098114, '_episodes_total': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-19 12:29:36,416\tWARNING worker.py:1047 -- WARNING: 28 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=35858)\u001b[0m 2020-07-19 12:29:38,829\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_3_2020-07-19_12-27-44rbbnbqo9/tmph2gigw0yrestore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=35858)\u001b[0m 2020-07-19 12:29:38,829\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 270, '_timesteps_total': None, '_time_total': 90.09775471687317, '_episodes_total': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-19 12:29:44,180\tWARNING worker.py:1047 -- WARNING: 28 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=35862)\u001b[0m 2020-07-19 12:29:46,449\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_3_2020-07-19_12-27-44rbbnbqo9/tmpqx8rox2orestore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=35862)\u001b[0m 2020-07-19 12:29:46,449\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 340, '_timesteps_total': None, '_time_total': 108.94926905632019, '_episodes_total': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-19 12:29:49,160\tWARNING worker.py:1047 -- WARNING: 28 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=35868)\u001b[0m 2020-07-19 12:29:51,460\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_3_2020-07-19_12-27-44rbbnbqo9/tmpsu9fg86srestore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=35868)\u001b[0m 2020-07-19 12:29:51,460\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 350, '_timesteps_total': None, '_time_total': 111.461012840271, '_episodes_total': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-19 12:29:56,748\tWARNING worker.py:1047 -- WARNING: 28 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=35872)\u001b[0m 2020-07-19 12:29:59,163\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_3_2020-07-19_12-27-44rbbnbqo9/tmpfsvbidrirestore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=35872)\u001b[0m 2020-07-19 12:29:59,164\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 380, '_timesteps_total': None, '_time_total': 119.25662803649902, '_episodes_total': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-19 12:30:01,860\tWARNING worker.py:1047 -- WARNING: 27 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=35876)\u001b[0m 2020-07-19 12:30:03,952\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_3_2020-07-19_12-27-44rbbnbqo9/tmp8153sjd_restore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=35876)\u001b[0m 2020-07-19 12:30:03,952\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 400, '_timesteps_total': None, '_time_total': 124.5083417892456, '_episodes_total': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-19 12:30:06,228\tWARNING worker.py:1047 -- WARNING: 26 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=35881)\u001b[0m 2020-07-19 12:30:07,956\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_3_2020-07-19_12-27-44rbbnbqo9/tmpgu9j54f7restore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=35881)\u001b[0m 2020-07-19 12:30:07,956\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 430, '_timesteps_total': None, '_time_total': 131.96725988388062, '_episodes_total': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-19 12:30:09,931\tWARNING worker.py:1047 -- WARNING: 26 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=35884)\u001b[0m 2020-07-19 12:30:11,492\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_3_2020-07-19_12-27-44rbbnbqo9/tmp6nvl84ojrestore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=35884)\u001b[0m 2020-07-19 12:30:11,492\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 450, '_timesteps_total': None, '_time_total': 136.04182147979736, '_episodes_total': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-19 12:30:13,433\tWARNING worker.py:1047 -- WARNING: 26 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=35887)\u001b[0m 2020-07-19 12:30:15,049\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_3_2020-07-19_12-27-44rbbnbqo9/tmpxlu6urn8restore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=35887)\u001b[0m 2020-07-19 12:30:15,049\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 470, '_timesteps_total': None, '_time_total': 139.7582449913025, '_episodes_total': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-19 12:30:17,176\tWARNING worker.py:1047 -- WARNING: 26 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=35891)\u001b[0m 2020-07-19 12:30:18,762\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_3_2020-07-19_12-27-44rbbnbqo9/tmpsrlazxw_restore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=35891)\u001b[0m 2020-07-19 12:30:18,762\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 490, '_timesteps_total': None, '_time_total': 143.7051305770874, '_episodes_total': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-19 12:30:20,727\tWARNING worker.py:1047 -- WARNING: 26 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=35895)\u001b[0m 2020-07-19 12:30:22,413\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_3_2020-07-19_12-27-44rbbnbqo9/tmpfcsbzgcjrestore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=35895)\u001b[0m 2020-07-19 12:30:22,413\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 500, '_timesteps_total': None, '_time_total': 145.60392022132874, '_episodes_total': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-19 12:30:24,451\tWARNING worker.py:1047 -- WARNING: 26 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=35898)\u001b[0m 2020-07-19 12:30:26,198\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_3_2020-07-19_12-27-44rbbnbqo9/tmpjkgotbp6restore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=35898)\u001b[0m 2020-07-19 12:30:26,198\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 520, '_timesteps_total': None, '_time_total': 149.5488314628601, '_episodes_total': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-19 12:30:28,431\tWARNING worker.py:1047 -- WARNING: 26 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=35902)\u001b[0m 2020-07-19 12:30:30,357\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_3_2020-07-19_12-27-44rbbnbqo9/tmpeir_s8bsrestore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=35902)\u001b[0m 2020-07-19 12:30:30,357\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 540, '_timesteps_total': None, '_time_total': 153.65792846679688, '_episodes_total': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-19 12:30:32,582\tWARNING worker.py:1047 -- WARNING: 26 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=35905)\u001b[0m 2020-07-19 12:30:34,468\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_3_2020-07-19_12-27-44rbbnbqo9/tmp6hkak2ourestore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=35905)\u001b[0m 2020-07-19 12:30:34,468\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 560, '_timesteps_total': None, '_time_total': 158.14876461029053, '_episodes_total': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-19 12:30:36,644\tWARNING worker.py:1047 -- WARNING: 26 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=35910)\u001b[0m 2020-07-19 12:30:38,373\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_3_2020-07-19_12-27-44rbbnbqo9/tmpcz3kp7rurestore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=35910)\u001b[0m 2020-07-19 12:30:38,373\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 580, '_timesteps_total': None, '_time_total': 162.5185694694519, '_episodes_total': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-19 12:30:40,451\tWARNING worker.py:1047 -- WARNING: 26 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=35913)\u001b[0m 2020-07-19 12:30:42,129\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_3_2020-07-19_12-27-44rbbnbqo9/tmpzkgn_cyqrestore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=35913)\u001b[0m 2020-07-19 12:30:42,129\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 600, '_timesteps_total': None, '_time_total': 166.58062028884888, '_episodes_total': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-19 12:30:44,117\tWARNING worker.py:1047 -- WARNING: 26 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=35916)\u001b[0m 2020-07-19 12:30:45,913\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_3_2020-07-19_12-27-44rbbnbqo9/tmphy5xp4adrestore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=35916)\u001b[0m 2020-07-19 12:30:45,913\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 610, '_timesteps_total': None, '_time_total': 168.56984043121338, '_episodes_total': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-19 12:30:47,806\tWARNING worker.py:1047 -- WARNING: 26 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=35920)\u001b[0m 2020-07-19 12:30:49,446\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_3_2020-07-19_12-27-44rbbnbqo9/tmpxnzpa7ajrestore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=35920)\u001b[0m 2020-07-19 12:30:49,446\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 630, '_timesteps_total': None, '_time_total': 172.5762631893158, '_episodes_total': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-19 12:30:51,430\tWARNING worker.py:1047 -- WARNING: 26 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=35923)\u001b[0m 2020-07-19 12:30:53,071\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_3_2020-07-19_12-27-44rbbnbqo9/tmpeyedgq0urestore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=35923)\u001b[0m 2020-07-19 12:30:53,071\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 650, '_timesteps_total': None, '_time_total': 176.31491565704346, '_episodes_total': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-19 12:30:55,160\tWARNING worker.py:1047 -- WARNING: 26 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=35926)\u001b[0m 2020-07-19 12:30:56,830\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_3_2020-07-19_12-27-44rbbnbqo9/tmpymqkisrwrestore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=35926)\u001b[0m 2020-07-19 12:30:56,831\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 670, '_timesteps_total': None, '_time_total': 180.2201509475708, '_episodes_total': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-19 12:30:58,965\tWARNING worker.py:1047 -- WARNING: 26 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=35930)\u001b[0m 2020-07-19 12:31:00,729\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_3_2020-07-19_12-27-44rbbnbqo9/tmpxqlz0k7srestore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=35930)\u001b[0m 2020-07-19 12:31:00,729\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 690, '_timesteps_total': None, '_time_total': 184.2665309906006, '_episodes_total': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-19 12:31:02,889\tWARNING worker.py:1047 -- WARNING: 26 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=35933)\u001b[0m 2020-07-19 12:31:04,720\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_3_2020-07-19_12-27-44rbbnbqo9/tmp02qj6c1krestore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=35933)\u001b[0m 2020-07-19 12:31:04,720\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 710, '_timesteps_total': None, '_time_total': 188.37194323539734, '_episodes_total': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-19 12:31:06,799\tWARNING worker.py:1047 -- WARNING: 26 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=35937)\u001b[0m 2020-07-19 12:31:08,602\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_3_2020-07-19_12-27-44rbbnbqo9/tmpg8e2xz3jrestore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=35937)\u001b[0m 2020-07-19 12:31:08,602\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 730, '_timesteps_total': None, '_time_total': 192.54877281188965, '_episodes_total': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-19 12:31:10,702\tWARNING worker.py:1047 -- WARNING: 26 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=35940)\u001b[0m 2020-07-19 12:31:12,465\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_3_2020-07-19_12-27-44rbbnbqo9/tmp9inf8bx7restore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=35940)\u001b[0m 2020-07-19 12:31:12,465\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 750, '_timesteps_total': None, '_time_total': 196.63041615486145, '_episodes_total': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-19 12:31:14,504\tWARNING worker.py:1047 -- WARNING: 26 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=35943)\u001b[0m 2020-07-19 12:31:16,243\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_3_2020-07-19_12-27-44rbbnbqo9/tmpi_j7zhpsrestore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=35943)\u001b[0m 2020-07-19 12:31:16,243\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 760, '_timesteps_total': None, '_time_total': 198.66285300254822, '_episodes_total': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-19 12:31:18,283\tWARNING worker.py:1047 -- WARNING: 26 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=35951)\u001b[0m 2020-07-19 12:31:19,952\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_3_2020-07-19_12-27-44rbbnbqo9/tmpts4ok484restore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=35951)\u001b[0m 2020-07-19 12:31:19,952\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 780, '_timesteps_total': None, '_time_total': 202.6780195236206, '_episodes_total': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-19 12:45:35,562\tWARNING worker.py:1047 -- WARNING: 25 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2020-07-19 12:45:35,585\tWARNING worker.py:1047 -- WARNING: 26 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2020-07-19 12:45:35,598\tWARNING worker.py:1047 -- WARNING: 27 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2020-07-19 12:45:35,625\tWARNING worker.py:1047 -- WARNING: 28 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2020-07-19 12:45:35,700\tWARNING worker.py:1047 -- WARNING: 29 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2020-07-19 12:45:35,750\tWARNING worker.py:1047 -- WARNING: 30 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2020-07-19 12:45:35,774\tWARNING worker.py:1047 -- WARNING: 31 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2020-07-19 12:45:35,786\tWARNING worker.py:1047 -- WARNING: 32 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2020-07-19 12:45:37,009\tWARNING worker.py:1047 -- WARNING: 33 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2020-07-19 12:45:37,013\tWARNING worker.py:1047 -- WARNING: 34 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2020-07-19 12:45:37,045\tWARNING worker.py:1047 -- WARNING: 35 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2020-07-19 12:45:37,076\tWARNING worker.py:1047 -- WARNING: 36 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2020-07-19 12:45:37,091\tWARNING worker.py:1047 -- WARNING: 37 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2020-07-19 12:45:37,103\tWARNING worker.py:1047 -- WARNING: 38 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2020-07-19 12:45:37,114\tWARNING worker.py:1047 -- WARNING: 39 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2020-07-19 12:45:37,125\tWARNING worker.py:1047 -- WARNING: 40 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=36195)\u001b[0m 2020-07-19 12:45:48,754\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_4_2020-07-19_12-45-35itu3_kes/tmpolwns3cgrestore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=36195)\u001b[0m 2020-07-19 12:45:48,754\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 10, '_timesteps_total': None, '_time_total': 3.3741214275360107, '_episodes_total': None}\n",
      "\u001b[2m\u001b[36m(pid=36191)\u001b[0m 2020-07-19 12:45:54,267\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_2_2020-07-19_12-45-35neychnk7/tmpm4oy1z3krestore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=36191)\u001b[0m 2020-07-19 12:45:54,268\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 30, '_timesteps_total': None, '_time_total': 10.464881420135498, '_episodes_total': None}\n",
      "\u001b[2m\u001b[36m(pid=36189)\u001b[0m 2020-07-19 12:45:58,630\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_3_2020-07-19_12-45-35e2dfmxwt/tmplbyhh7tirestore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=36189)\u001b[0m 2020-07-19 12:45:58,630\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 10, '_timesteps_total': None, '_time_total': 3.3741214275360107, '_episodes_total': None}\n",
      "\u001b[2m\u001b[36m(pid=36189)\u001b[0m 2020-07-19 12:45:59,503\tINFO (unknown file):0 -- gc.collect() freed 24 refs in 0.0903270859999985 seconds\n",
      "\u001b[2m\u001b[36m(pid=36190)\u001b[0m 2020-07-19 12:46:07,884\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_2_2020-07-19_12-45-35neychnk7/tmpc904htw2restore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=36190)\u001b[0m 2020-07-19 12:46:07,884\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 30, '_timesteps_total': None, '_time_total': 10.336280345916748, '_episodes_total': None}\n",
      "\u001b[2m\u001b[36m(pid=36190)\u001b[0m 2020-07-19 12:46:09,589\tINFO (unknown file):0 -- gc.collect() freed 24 refs in 0.07572257000000349 seconds\n",
      "\u001b[2m\u001b[36m(pid=36194)\u001b[0m 2020-07-19 12:46:12,699\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_5_2020-07-19_12-45-35bmxdht98/tmpoguqsfpjrestore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=36194)\u001b[0m 2020-07-19 12:46:12,699\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 30, '_timesteps_total': None, '_time_total': 9.425882577896118, '_episodes_total': None}\n",
      "\u001b[2m\u001b[36m(pid=36192)\u001b[0m 2020-07-19 12:46:17,849\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_2_2020-07-19_12-45-35neychnk7/tmplupygj5urestore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=36192)\u001b[0m 2020-07-19 12:46:17,849\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 70, '_timesteps_total': None, '_time_total': 22.367480993270874, '_episodes_total': None}\n",
      "\u001b[2m\u001b[36m(pid=36192)\u001b[0m 2020-07-19 12:46:19,647\tINFO (unknown file):0 -- gc.collect() freed 24 refs in 0.2113984980000012 seconds\n",
      "\u001b[2m\u001b[36m(pid=36193)\u001b[0m 2020-07-19 12:46:22,388\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_5_2020-07-19_12-45-35bmxdht98/tmpwx4lk7furestore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=36193)\u001b[0m 2020-07-19 12:46:22,388\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 90, '_timesteps_total': None, '_time_total': 29.410155773162842, '_episodes_total': None}\n",
      "\u001b[2m\u001b[36m(pid=36188)\u001b[0m 2020-07-19 12:46:30,387\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_2_2020-07-19_12-45-35neychnk7/tmpea0a0vgorestore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=36188)\u001b[0m 2020-07-19 12:46:30,387\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 100, '_timesteps_total': None, '_time_total': 32.63788175582886, '_episodes_total': None}\n",
      "\u001b[2m\u001b[36m(pid=36186)\u001b[0m 2020-07-19 12:46:35,340\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_6_2020-07-19_12-45-35co9a9ol5/tmp11pm1vlprestore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=36186)\u001b[0m 2020-07-19 12:46:35,340\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 110, '_timesteps_total': None, '_time_total': 35.58680605888367, '_episodes_total': None}\n",
      "\u001b[2m\u001b[36m(pid=36193)\u001b[0m 2020-07-19 12:46:39,703\tINFO (unknown file):0 -- gc.collect() freed 24 refs in 0.1662518390000045 seconds\n",
      "\u001b[2m\u001b[36m(pid=36186)\u001b[0m 2020-07-19 12:46:39,743\tINFO (unknown file):0 -- gc.collect() freed 24 refs in 0.07482161700000489 seconds\n",
      "\u001b[2m\u001b[36m(pid=36183)\u001b[0m 2020-07-19 12:46:44,336\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_2_2020-07-19_12-45-35neychnk7/tmpkg_7meqgrestore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=36183)\u001b[0m 2020-07-19 12:46:44,337\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 140, '_timesteps_total': None, '_time_total': 45.5325813293457, '_episodes_total': None}\n",
      "\u001b[2m\u001b[36m(pid=36183)\u001b[0m 2020-07-19 12:46:49,738\tINFO (unknown file):0 -- gc.collect() freed 24 refs in 0.17130385300001194 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-19 12:46:54,956\tWARNING worker.py:1047 -- WARNING: 29 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2020-07-19 12:46:54,969\tWARNING worker.py:1047 -- WARNING: 30 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2020-07-19 12:46:54,972\tWARNING worker.py:1047 -- WARNING: 31 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2020-07-19 12:46:54,975\tWARNING worker.py:1047 -- WARNING: 32 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=36254)\u001b[0m 2020-07-19 12:47:00,383\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_4_2020-07-19_12-45-35itu3_kes/tmpwycjdbhcrestore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=36254)\u001b[0m 2020-07-19 12:47:00,383\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 170, '_timesteps_total': None, '_time_total': 57.446412801742554, '_episodes_total': None}\n",
      "\u001b[2m\u001b[36m(pid=36256)\u001b[0m 2020-07-19 12:47:05,532\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_7_2020-07-19_12-45-35k9f29ctj/tmpb2orqoyarestore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=36256)\u001b[0m 2020-07-19 12:47:05,532\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 150, '_timesteps_total': None, '_time_total': 48.65259599685669, '_episodes_total': None}\n",
      "\u001b[2m\u001b[36m(pid=36257)\u001b[0m 2020-07-19 12:47:10,858\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_7_2020-07-19_12-45-35k9f29ctj/tmpfyk19rfdrestore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=36257)\u001b[0m 2020-07-19 12:47:10,859\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 190, '_timesteps_total': None, '_time_total': 67.71323776245117, '_episodes_total': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-19 12:47:13,559\tWARNING worker.py:1047 -- WARNING: 29 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2020-07-19 12:47:13,560\tWARNING worker.py:1047 -- WARNING: 30 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2020-07-19 12:47:13,572\tWARNING worker.py:1047 -- WARNING: 31 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2020-07-19 12:47:14,445\tWARNING worker.py:1047 -- WARNING: 32 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2020-07-19 12:47:14,446\tWARNING worker.py:1047 -- WARNING: 33 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2020-07-19 12:47:14,449\tWARNING worker.py:1047 -- WARNING: 34 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2020-07-19 12:47:14,453\tWARNING worker.py:1047 -- WARNING: 35 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2020-07-19 12:47:14,458\tWARNING worker.py:1047 -- WARNING: 35 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2020-07-19 12:47:15,214\tWARNING worker.py:1047 -- WARNING: 36 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2020-07-19 12:47:15,215\tWARNING worker.py:1047 -- WARNING: 37 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=36268)\u001b[0m 2020-07-19 12:47:18,517\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_7_2020-07-19_12-45-35k9f29ctj/tmpfpzic9lwrestore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=36268)\u001b[0m 2020-07-19 12:47:18,517\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 210, '_timesteps_total': None, '_time_total': 70.34668278694153, '_episodes_total': None}\n",
      "\u001b[2m\u001b[36m(pid=36270)\u001b[0m 2020-07-19 12:47:18,630\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_1_2020-07-19_12-45-35jklcsl8m/tmpfc8chrmzrestore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=36270)\u001b[0m 2020-07-19 12:47:18,631\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 210, '_timesteps_total': None, '_time_total': 70.34668278694153, '_episodes_total': None}\n",
      "\u001b[2m\u001b[36m(pid=36276)\u001b[0m 2020-07-19 12:47:24,389\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_3_2020-07-19_12-45-35e2dfmxwt/tmphrj44btbrestore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=36276)\u001b[0m 2020-07-19 12:47:24,389\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 190, '_timesteps_total': None, '_time_total': 67.71323776245117, '_episodes_total': None}\n",
      "\u001b[2m\u001b[36m(pid=36277)\u001b[0m 2020-07-19 12:47:24,446\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_7_2020-07-19_12-45-35k9f29ctj/tmp7v79c0xzrestore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=36277)\u001b[0m 2020-07-19 12:47:24,446\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 220, '_timesteps_total': None, '_time_total': 79.82255411148071, '_episodes_total': None}\n",
      "\u001b[2m\u001b[36m(pid=36272)\u001b[0m 2020-07-19 12:47:29,640\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_1_2020-07-19_12-45-35jklcsl8m/tmphvx1z5jxrestore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=36272)\u001b[0m 2020-07-19 12:47:29,640\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 220, '_timesteps_total': None, '_time_total': 79.82255411148071, '_episodes_total': None}\n",
      "\u001b[2m\u001b[36m(pid=36271)\u001b[0m 2020-07-19 12:47:29,710\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_3_2020-07-19_12-45-35e2dfmxwt/tmp5aup2ukirestore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=36271)\u001b[0m 2020-07-19 12:47:29,710\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 240, '_timesteps_total': None, '_time_total': 83.4758951663971, '_episodes_total': None}\n",
      "\u001b[2m\u001b[36m(pid=36275)\u001b[0m 2020-07-19 12:47:34,365\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_7_2020-07-19_12-45-35k9f29ctj/tmpwzdqzgvirestore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=36275)\u001b[0m 2020-07-19 12:47:34,365\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 240, '_timesteps_total': None, '_time_total': 87.58522987365723, '_episodes_total': None}\n",
      "\u001b[2m\u001b[36m(pid=36274)\u001b[0m 2020-07-19 12:47:39,109\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_3_2020-07-19_12-45-35e2dfmxwt/tmp1v93u_threstore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=36274)\u001b[0m 2020-07-19 12:47:39,109\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 250, '_timesteps_total': None, '_time_total': 86.87141537666321, '_episodes_total': None}\n",
      "\u001b[2m\u001b[36m(pid=36273)\u001b[0m 2020-07-19 12:47:40,645\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_7_2020-07-19_12-45-35k9f29ctj/tmp4x5k9cmgrestore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=36273)\u001b[0m 2020-07-19 12:47:40,645\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 270, '_timesteps_total': None, '_time_total': 93.21954464912415, '_episodes_total': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-19 12:47:42,362\tWARNING worker.py:1047 -- WARNING: 29 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2020-07-19 12:47:42,374\tWARNING worker.py:1047 -- WARNING: 30 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2020-07-19 12:47:43,284\tWARNING worker.py:1047 -- WARNING: 31 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2020-07-19 12:47:43,296\tWARNING worker.py:1047 -- WARNING: 32 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=36269)\u001b[0m 2020-07-19 12:47:43,290\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_1_2020-07-19_12-45-35jklcsl8m/tmpe46lflykrestore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=36269)\u001b[0m 2020-07-19 12:47:43,290\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 270, '_timesteps_total': None, '_time_total': 93.21954464912415, '_episodes_total': None}\n",
      "\u001b[2m\u001b[36m(pid=36305)\u001b[0m 2020-07-19 12:47:45,641\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_3_2020-07-19_12-45-35e2dfmxwt/tmp54i72t82restore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=36305)\u001b[0m 2020-07-19 12:47:45,642\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 280, '_timesteps_total': None, '_time_total': 100.19562983512878, '_episodes_total': None}\n",
      "\u001b[2m\u001b[36m(pid=36306)\u001b[0m 2020-07-19 12:47:49,418\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_7_2020-07-19_12-45-35k9f29ctj/tmpy9z62ilorestore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=36306)\u001b[0m 2020-07-19 12:47:49,418\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 290, '_timesteps_total': None, '_time_total': 103.61294555664062, '_episodes_total': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-19 12:47:52,858\tWARNING worker.py:1047 -- WARNING: 29 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=36307)\u001b[0m 2020-07-19 12:47:54,215\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_7_2020-07-19_12-45-35k9f29ctj/tmp546wo9y_restore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=36307)\u001b[0m 2020-07-19 12:47:54,216\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 310, '_timesteps_total': None, '_time_total': 110.75332880020142, '_episodes_total': None}\n",
      "\u001b[2m\u001b[36m(pid=36319)\u001b[0m 2020-07-19 12:47:55,450\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_0_2020-07-19_12-45-3564_a1t6i/tmpbe1u3q12restore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=36319)\u001b[0m 2020-07-19 12:47:55,450\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 300, '_timesteps_total': None, '_time_total': 106.75056505203247, '_episodes_total': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-19 12:47:57,347\tWARNING worker.py:1047 -- WARNING: 29 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2020-07-19 12:47:58,784\tWARNING worker.py:1047 -- WARNING: 29 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=36322)\u001b[0m 2020-07-19 12:48:00,033\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_7_2020-07-19_12-45-35k9f29ctj/tmp5lfbm5sqrestore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=36322)\u001b[0m 2020-07-19 12:48:00,033\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 310, '_timesteps_total': None, '_time_total': 109.949049949646, '_episodes_total': None}\n",
      "\u001b[2m\u001b[36m(pid=36326)\u001b[0m 2020-07-19 12:48:01,266\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_0_2020-07-19_12-45-3564_a1t6i/tmpaa64itarrestore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=36326)\u001b[0m 2020-07-19 12:48:01,266\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 330, '_timesteps_total': None, '_time_total': 117.10326147079468, '_episodes_total': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-19 12:48:04,778\tWARNING worker.py:1047 -- WARNING: 26 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=36329)\u001b[0m 2020-07-19 12:48:06,502\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_7_2020-07-19_12-45-35k9f29ctj/tmp9ss4ag4qrestore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=36329)\u001b[0m 2020-07-19 12:48:06,502\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 340, '_timesteps_total': None, '_time_total': 119.46471166610718, '_episodes_total': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-19 12:48:09,771\tWARNING worker.py:1047 -- WARNING: 26 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=36333)\u001b[0m 2020-07-19 12:48:11,512\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_0_2020-07-19_12-45-3564_a1t6i/tmpkdrbmjnlrestore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=36333)\u001b[0m 2020-07-19 12:48:11,513\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 350, '_timesteps_total': None, '_time_total': 121.4858295917511, '_episodes_total': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-19 12:48:13,416\tWARNING worker.py:1047 -- WARNING: 26 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=36336)\u001b[0m 2020-07-19 12:48:15,000\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_0_2020-07-19_12-45-3564_a1t6i/tmp88e8_d3crestore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=36336)\u001b[0m 2020-07-19 12:48:15,000\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 370, '_timesteps_total': None, '_time_total': 125.4720389842987, '_episodes_total': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-19 12:48:16,969\tWARNING worker.py:1047 -- WARNING: 26 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=36351)\u001b[0m 2020-07-19 12:48:18,605\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_0_2020-07-19_12-45-3564_a1t6i/tmp27etjyq7restore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=36351)\u001b[0m 2020-07-19 12:48:18,605\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 390, '_timesteps_total': None, '_time_total': 129.2809920310974, '_episodes_total': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-19 12:52:04,463\tWARNING worker.py:1047 -- WARNING: 25 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2020-07-19 12:52:04,484\tWARNING worker.py:1047 -- WARNING: 26 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2020-07-19 12:52:04,511\tWARNING worker.py:1047 -- WARNING: 27 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2020-07-19 12:52:04,525\tWARNING worker.py:1047 -- WARNING: 28 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2020-07-19 12:52:04,538\tWARNING worker.py:1047 -- WARNING: 29 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2020-07-19 12:52:04,563\tWARNING worker.py:1047 -- WARNING: 30 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2020-07-19 12:52:04,584\tWARNING worker.py:1047 -- WARNING: 31 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2020-07-19 12:52:04,611\tWARNING worker.py:1047 -- WARNING: 32 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2020-07-19 12:52:05,721\tWARNING worker.py:1047 -- WARNING: 33 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2020-07-19 12:52:05,722\tWARNING worker.py:1047 -- WARNING: 34 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2020-07-19 12:52:05,723\tWARNING worker.py:1047 -- WARNING: 35 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2020-07-19 12:52:05,726\tWARNING worker.py:1047 -- WARNING: 36 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2020-07-19 12:52:05,734\tWARNING worker.py:1047 -- WARNING: 37 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2020-07-19 12:52:05,741\tWARNING worker.py:1047 -- WARNING: 38 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=36469)\u001b[0m 2020-07-19 12:52:19,240\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_6_2020-07-19_12-52-046qdb9hj3/tmpz0c6usjarestore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=36469)\u001b[0m 2020-07-19 12:52:19,241\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 10, '_timesteps_total': None, '_time_total': 3.780517339706421, '_episodes_total': None}\n",
      "\u001b[2m\u001b[36m(pid=36469)\u001b[0m 2020-07-19 12:52:21,684\tINFO (unknown file):0 -- gc.collect() freed 24 refs in 0.06826275999999964 seconds\n",
      "\u001b[2m\u001b[36m(pid=36470)\u001b[0m 2020-07-19 12:52:24,104\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_2_2020-07-19_12-52-04_wzanqfm/tmpmorijcavrestore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=36470)\u001b[0m 2020-07-19 12:52:24,105\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 20, '_timesteps_total': None, '_time_total': 7.0238564014434814, '_episodes_total': None}\n",
      "\u001b[2m\u001b[36m(pid=36467)\u001b[0m 2020-07-19 12:52:30,657\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_6_2020-07-19_12-52-046qdb9hj3/tmpkqqom7lxrestore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=36467)\u001b[0m 2020-07-19 12:52:30,657\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 30, '_timesteps_total': None, '_time_total': 10.256705284118652, '_episodes_total': None}\n",
      "\u001b[2m\u001b[36m(pid=36467)\u001b[0m 2020-07-19 12:52:31,687\tINFO (unknown file):0 -- gc.collect() freed 24 refs in 0.06235025899999869 seconds\n",
      "\u001b[2m\u001b[36m(pid=36470)\u001b[0m 2020-07-19 12:52:31,657\tINFO (unknown file):0 -- gc.collect() freed 24 refs in 0.21497525500000236 seconds\n",
      "\u001b[2m\u001b[36m(pid=36465)\u001b[0m 2020-07-19 12:52:35,538\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_4_2020-07-19_12-52-04du2wv6xs/tmpojp51o6yrestore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=36465)\u001b[0m 2020-07-19 12:52:35,539\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 50, '_timesteps_total': None, '_time_total': 17.076108932495117, '_episodes_total': None}\n",
      "\u001b[2m\u001b[36m(pid=36462)\u001b[0m 2020-07-19 12:52:40,951\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_6_2020-07-19_12-52-046qdb9hj3/tmp5siic3h5restore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=36462)\u001b[0m 2020-07-19 12:52:40,951\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 70, '_timesteps_total': None, '_time_total': 23.682976007461548, '_episodes_total': None}\n",
      "\u001b[2m\u001b[36m(pid=36462)\u001b[0m 2020-07-19 12:52:41,698\tINFO (unknown file):0 -- gc.collect() freed 24 refs in 0.1974797319999979 seconds\n",
      "\u001b[2m\u001b[36m(pid=36465)\u001b[0m 2020-07-19 12:52:41,722\tINFO (unknown file):0 -- gc.collect() freed 24 refs in 0.0641874840000014 seconds\n",
      "\u001b[2m\u001b[36m(pid=36460)\u001b[0m 2020-07-19 12:52:44,917\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_3_2020-07-19_12-52-04jsgefn3_/tmpas39ebpfrestore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=36460)\u001b[0m 2020-07-19 12:52:44,917\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 60, '_timesteps_total': None, '_time_total': 20.31567120552063, '_episodes_total': None}\n",
      "\u001b[2m\u001b[36m(pid=36468)\u001b[0m 2020-07-19 12:52:45,916\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_7_2020-07-19_12-52-040hqn1o5s/tmpltdgtor5restore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=36468)\u001b[0m 2020-07-19 12:52:45,916\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 70, '_timesteps_total': None, '_time_total': 23.89347553253174, '_episodes_total': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-19 12:52:49,288\tWARNING worker.py:1047 -- WARNING: 29 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2020-07-19 12:52:49,290\tWARNING worker.py:1047 -- WARNING: 30 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2020-07-19 12:52:49,291\tWARNING worker.py:1047 -- WARNING: 31 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2020-07-19 12:52:49,293\tWARNING worker.py:1047 -- WARNING: 32 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2020-07-19 12:52:49,976\tWARNING worker.py:1047 -- WARNING: 33 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2020-07-19 12:52:49,989\tWARNING worker.py:1047 -- WARNING: 34 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2020-07-19 12:52:49,990\tWARNING worker.py:1047 -- WARNING: 35 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2020-07-19 12:52:49,992\tWARNING worker.py:1047 -- WARNING: 36 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=36463)\u001b[0m 2020-07-19 12:52:50,048\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_6_2020-07-19_12-52-046qdb9hj3/tmpg2sopgjnrestore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=36463)\u001b[0m 2020-07-19 12:52:50,048\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 70, '_timesteps_total': None, '_time_total': 23.89347553253174, '_episodes_total': None}\n",
      "\u001b[2m\u001b[36m(pid=36522)\u001b[0m 2020-07-19 12:52:53,346\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_7_2020-07-19_12-52-040hqn1o5s/tmpskhm6ct7restore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=36522)\u001b[0m 2020-07-19 12:52:53,346\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 90, '_timesteps_total': None, '_time_total': 30.35236954689026, '_episodes_total': None}\n",
      "\u001b[2m\u001b[36m(pid=36526)\u001b[0m 2020-07-19 12:52:59,415\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_3_2020-07-19_12-52-04jsgefn3_/tmpxwkzjc6frestore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=36526)\u001b[0m 2020-07-19 12:52:59,415\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 90, '_timesteps_total': None, '_time_total': 30.35236954689026, '_episodes_total': None}\n",
      "\u001b[2m\u001b[36m(pid=36529)\u001b[0m 2020-07-19 12:53:05,733\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_6_2020-07-19_12-52-046qdb9hj3/tmpzwzurivdrestore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=36529)\u001b[0m 2020-07-19 12:53:05,733\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 110, '_timesteps_total': None, '_time_total': 38.44915699958801, '_episodes_total': None}\n",
      "\u001b[2m\u001b[36m(pid=36527)\u001b[0m 2020-07-19 12:53:10,495\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_5_2020-07-19_12-52-04_4x4l6bo/tmpk0_8wvwzrestore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=36527)\u001b[0m 2020-07-19 12:53:10,495\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 130, '_timesteps_total': None, '_time_total': 44.89846754074097, '_episodes_total': None}\n",
      "\u001b[2m\u001b[36m(pid=36527)\u001b[0m 2020-07-19 12:53:12,010\tINFO (unknown file):0 -- gc.collect() freed 24 refs in 0.07489525299999755 seconds\n",
      "\u001b[2m\u001b[36m(pid=36524)\u001b[0m 2020-07-19 12:53:15,632\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_6_2020-07-19_12-52-046qdb9hj3/tmps91vcl6nrestore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=36524)\u001b[0m 2020-07-19 12:53:15,632\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 140, '_timesteps_total': None, '_time_total': 48.06865906715393, '_episodes_total': None}\n",
      "\u001b[2m\u001b[36m(pid=36528)\u001b[0m 2020-07-19 12:53:16,739\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_5_2020-07-19_12-52-04_4x4l6bo/tmpzju7d7ljrestore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=36528)\u001b[0m 2020-07-19 12:53:16,739\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 150, '_timesteps_total': None, '_time_total': 50.47431206703186, '_episodes_total': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-19 12:53:21,194\tWARNING worker.py:1047 -- WARNING: 29 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2020-07-19 12:53:21,195\tWARNING worker.py:1047 -- WARNING: 30 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2020-07-19 12:53:21,815\tWARNING worker.py:1047 -- WARNING: 31 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2020-07-19 12:53:21,816\tWARNING worker.py:1047 -- WARNING: 32 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2020-07-19 12:53:22,797\tWARNING worker.py:1047 -- WARNING: 33 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2020-07-19 12:53:22,798\tWARNING worker.py:1047 -- WARNING: 34 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2020-07-19 12:53:22,799\tWARNING worker.py:1047 -- WARNING: 34 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2020-07-19 12:53:22,801\tWARNING worker.py:1047 -- WARNING: 35 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2020-07-19 12:53:23,247\tWARNING worker.py:1047 -- WARNING: 36 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=36523)\u001b[0m 2020-07-19 12:53:24,050\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_6_2020-07-19_12-52-046qdb9hj3/tmps6lb66skrestore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=36523)\u001b[0m 2020-07-19 12:53:24,050\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 160, '_timesteps_total': None, '_time_total': 53.702807903289795, '_episodes_total': None}\n",
      "\u001b[2m\u001b[36m(pid=36551)\u001b[0m 2020-07-19 12:53:27,377\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_5_2020-07-19_12-52-04_4x4l6bo/tmp45ft2zvkrestore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=36551)\u001b[0m 2020-07-19 12:53:27,377\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 160, '_timesteps_total': None, '_time_total': 53.702807903289795, '_episodes_total': None}\n",
      "\u001b[2m\u001b[36m(pid=36558)\u001b[0m 2020-07-19 12:53:30,463\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_1_2020-07-19_12-52-04hwtnym23/tmpadgssuu5restore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=36558)\u001b[0m 2020-07-19 12:53:30,463\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 190, '_timesteps_total': None, '_time_total': 69.95982456207275, '_episodes_total': None}\n",
      "\u001b[2m\u001b[36m(pid=36555)\u001b[0m 2020-07-19 12:53:35,919\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_1_2020-07-19_12-52-04hwtnym23/tmpj2qoayjzrestore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=36555)\u001b[0m 2020-07-19 12:53:35,920\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 180, '_timesteps_total': None, '_time_total': 63.067728996276855, '_episodes_total': None}\n",
      "\u001b[2m\u001b[36m(pid=36554)\u001b[0m 2020-07-19 12:53:37,049\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_0_2020-07-19_12-52-043o3xz6wi/tmpfsilsm_erestore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=36554)\u001b[0m 2020-07-19 12:53:37,049\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 180, '_timesteps_total': None, '_time_total': 63.067728996276855, '_episodes_total': None}\n",
      "\u001b[2m\u001b[36m(pid=36556)\u001b[0m 2020-07-19 12:53:41,783\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_1_2020-07-19_12-52-04hwtnym23/tmp0sgaeu25restore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=36556)\u001b[0m 2020-07-19 12:53:41,784\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 220, '_timesteps_total': None, '_time_total': 81.51921725273132, '_episodes_total': None}\n",
      "\u001b[2m\u001b[36m(pid=36552)\u001b[0m 2020-07-19 12:53:42,563\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_0_2020-07-19_12-52-043o3xz6wi/tmpdyn_a9xerestore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=36552)\u001b[0m 2020-07-19 12:53:42,564\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 190, '_timesteps_total': None, '_time_total': 65.10032176971436, '_episodes_total': None}\n",
      "\u001b[2m\u001b[36m(pid=36557)\u001b[0m 2020-07-19 12:53:46,800\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_1_2020-07-19_12-52-04hwtnym23/tmpfj8egsr0restore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=36557)\u001b[0m 2020-07-19 12:53:46,800\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 210, '_timesteps_total': None, '_time_total': 74.10653257369995, '_episodes_total': None}\n",
      "\u001b[2m\u001b[36m(pid=36553)\u001b[0m 2020-07-19 12:53:50,771\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_5_2020-07-19_12-52-04_4x4l6bo/tmpbwak0pg9restore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=36553)\u001b[0m 2020-07-19 12:53:50,771\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 240, '_timesteps_total': None, '_time_total': 88.66653251647949, '_episodes_total': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-19 12:53:51,237\tWARNING worker.py:1047 -- WARNING: 29 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2020-07-19 12:53:54,033\tWARNING worker.py:1047 -- WARNING: 29 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=36592)\u001b[0m 2020-07-19 12:53:53,898\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_6_2020-07-19_12-52-046qdb9hj3/tmpt0drt6vzrestore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=36592)\u001b[0m 2020-07-19 12:53:53,898\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 220, '_timesteps_total': None, '_time_total': 77.37985229492188, '_episodes_total': None}\n",
      "\u001b[2m\u001b[36m(pid=36595)\u001b[0m 2020-07-19 12:53:56,585\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_5_2020-07-19_12-52-04_4x4l6bo/tmpdfx5nmxorestore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=36595)\u001b[0m 2020-07-19 12:53:56,585\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 220, '_timesteps_total': None, '_time_total': 75.03413772583008, '_episodes_total': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-19 12:53:56,926\tWARNING worker.py:1047 -- WARNING: 29 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2020-07-19 12:53:59,728\tWARNING worker.py:1047 -- WARNING: 29 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=36598)\u001b[0m 2020-07-19 12:53:59,479\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_6_2020-07-19_12-52-046qdb9hj3/tmp6a_fzssorestore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=36598)\u001b[0m 2020-07-19 12:53:59,479\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 230, '_timesteps_total': None, '_time_total': 77.89022159576416, '_episodes_total': None}\n",
      "\u001b[2m\u001b[36m(pid=36602)\u001b[0m 2020-07-19 12:54:02,258\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_5_2020-07-19_12-52-04_4x4l6bo/tmpbu_a46bbrestore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=36602)\u001b[0m 2020-07-19 12:54:02,259\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 280, '_timesteps_total': None, '_time_total': 101.22713851928711, '_episodes_total': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-19 12:54:04,966\tWARNING worker.py:1047 -- WARNING: 28 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=36606)\u001b[0m 2020-07-19 12:54:07,050\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_5_2020-07-19_12-52-04_4x4l6bo/tmp6227fo8krestore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=36606)\u001b[0m 2020-07-19 12:54:07,051\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 260, '_timesteps_total': None, '_time_total': 86.63231945037842, '_episodes_total': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-19 12:54:12,364\tWARNING worker.py:1047 -- WARNING: 26 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=36612)\u001b[0m 2020-07-19 12:54:14,122\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_6_2020-07-19_12-52-046qdb9hj3/tmpm4lpraoqrestore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=36612)\u001b[0m 2020-07-19 12:54:14,122\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 280, '_timesteps_total': None, '_time_total': 91.36839985847473, '_episodes_total': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-19 12:54:20,257\tWARNING worker.py:1047 -- WARNING: 26 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=36616)\u001b[0m 2020-07-19 12:54:21,956\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/TrainMNIST/TrainMNIST_6_2020-07-19_12-52-046qdb9hj3/tmpaytf7baqrestore_from_object/checkpoint\n",
      "\u001b[2m\u001b[36m(pid=36616)\u001b[0m 2020-07-19 12:54:21,956\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 320, '_timesteps_total': None, '_time_total': 99.53643941879272, '_episodes_total': None}\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    response = requests.post(f\"http://127.0.0.1:8000/post_hello\", data = {'name': f'request_{i}'})\n",
    "    print(f'{i:2d}: {response}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replacing an Existing Endpoint or Backend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serve is a Singleton in the Ray Cluster\n",
    "\n",
    "You may have noticed that when defining endpoints and backends, we called Serve API methods, not methods on a Serve _class instance_. Serve is actually a [singleton](https://en.wikipedia.org/wiki/Singleton_pattern) in the whole Ray cluster, not just the driver program. \n",
    "\n",
    "This means that even when you terminate this notebook, our definitions above will persist! Hence, you need to clean up any endpoints and backends that are no longer needed, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = serve.list_endpoints()\n",
    "for name in eps.keys():\n",
    "    serve.delete_endpoint(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bes = serve.list_backends()\n",
    "for name in bes.keys():\n",
    "    serve.delete_backend(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'serve' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0092e3a39133>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0meps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mserve\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_endpoints\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mbes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mserve\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_backends\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'endpoints: {eps}, backends {bes}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'serve' is not defined"
     ]
    }
   ],
   "source": [
    "eps = serve.list_endpoints()\n",
    "bes = serve.list_backends()\n",
    "print(f'endpoints: {eps}, backends {bes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serve.shutdown(name='serve-example-1') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
