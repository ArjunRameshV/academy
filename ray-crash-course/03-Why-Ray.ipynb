{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why Ray?\n",
    "\n",
    "[Ray](https://ray.io) is a system for scaling Python applications from your laptop to a cluster with relative ease. It emerged from the [RISELab](https://rise.cs.berkeley.edu/) at Berkeley in response to the problems researchers faced writing advanced ML applications and libraries that could easily scale to a cluster. These researchers found that none of the existing solutions were flexible enough and easy enough to use for their needs. Hence, Ray was born. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Do We Need Ray?\n",
    "\n",
    "Consider the following charts:\n",
    "![Two Trends](../images/TwoTrends.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ML/AI model sizes have grown enormously in recent years, roughly 35x every 18 months, which is considerably faster than Moore's Law! Hence, this growth is far outstripping the growth of hardware capabilities. The only way to meet the need for sufficient compute power is to go distributed, as [Ion Stoica recently wrote](https://anyscale.com/blog/the-future-of-computing-is-distributed/).\n",
    "\n",
    "At the same time, the use of Python is growing rapidly, because it is a very popular language for data science. Many of the ML/AI toolkits are Python-based. Hence, there is a pressing need for powerful, yet easy-to-use tools for scaling Python applications horizontally. This is the motivation for Ray."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why are tools needed? First, the Python interpreter itself is not designed for massive scalability and high performance. Many python libraries with these requirements use C/C++ backends to work around Python limitations, like the so-called _global interpreter lock_, which effectively makes Python scripts single threaded. \n",
    "\n",
    "Some of the most popular, general-purpose tools for this purpose include the following:\n",
    "* [asyncio](https://docs.python.org/3/library/asyncio.html) for _async/await_-style (coroutine) concurrency.\n",
    "* [multiprocessing.Pool](https://docs.python.org/3/library/multiprocessing.html?highlight=pool#module-multiprocessing.pool) for creating a pool of asynchronous workers\n",
    "* [joblib](https://joblib.readthedocs.io/en/latest/) for creating lightweight pipelines \n",
    "\n",
    "However, while all of them make it easier to exploit all the CPU cores on your machine, they don't provide distributed computing beyond the boundaries of your machine. Ray breaks that boundary.\n",
    "\n",
    "In fact, Ray also provides implementations of these APIs, so you are no longer limited to the boundaries of a single machine.\n",
    "\n",
    "We'll learn more about these implementations in [lesson 6](06-AdoptingRay.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider this image:\n",
    "![ML Landscape](../images/ML-Landscape.png)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It shows major tasks required in many ML-based application development and deployment, all of which typically require distributed implementations to scale large enough to process the compute and data load in a timely manner:\n",
    "\n",
    "* **Featurization:** Features are the data \"attributes\" that appear to be most useful for modeling the domain.\n",
    "* **Streaming:** New data often arrives in realtime and may be processed in realtime, too.\n",
    "* **Hyperameter Tuning:** What are the best kinds of models for this domain? When using neural networks, what is the ideal _architecture_ for the network? This model \"metadata\" is also called the _hyperparameters_. Since discovering the hyperparameters can be an expensive process of training lots of candidates, specialized techniques in their own right have merged for this purpose, as we'll learn in the _Ray Tune_ module.\n",
    "* **Training:** Once the best (or at least good enough) hyperparameters are chosen, the model has to be trained on real data and sometimes retrained periodically as new data arrives.\n",
    "* **Simulation:** An important part of many _reinforcement learning_ applications is running a simulator, such as a game engine or robot simulation, against which the RL system is trained to maximize the \"reward\" when operating in that environment or the real analog. The simulator is one example of a compute pattern that is quite a bit different from the normal \"dataflow\" or query-like patterns that many big data tools support well. Also, this simulator may be run many, many times as part of the hyperameter tuning or training process, requiring efficient, cluster-wide execution.\n",
    "* **Model Serving:** Finally, when the model is trained, it needs to be served, so that it can be applied to new data, sometimes with low latency requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the Ray vision:\n",
    "![Ray across the board](../images/ML-Landscape-Ray.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The core Ray system, which we'll explore in this module, provides the cluster-wide scheduling of work (which we'll call _tasks_) and management of _distributed state_, another important requirement in real-world distributed systems.\n",
    "\n",
    "On top of Ray, a growing family of domain-specific libraries support many of the functions we've discussed, like the ones shown in the image. Other tutorial modules in this repo explore those libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Ray Tune:** For hyperparameter tuning. Tune integrates several optimization algorithms and integrates with many ML frameworks.\n",
    "* **Ray SGD:** For _stochastic gradient descent_ (SGD). This is a relatively new library, currently supporting PyTorch with other support for other systems forthcoming.\n",
    "* **Ray RLlib:** For reinforcement learning. Many of the widely-used and recent algorithms are implemented. RL often involves running and interoperating with a simulator for the environment (e.g., an actual game engine).\n",
    "* **Ray Serve:** Primarily targeted at model serving, but flexible enough for many scalable web service scenarios.\n",
    "\n",
    "All leverage Ray for cluster-wide scalability. All will be covered in depth in forthcoming tutorial modules. Many Ray users will never actually use the core Ray API, but instead use one or more of these domain-specific APIs. You might be one of those people ;) If you need to implement distributed applications, the current _Ray Core_ tutorial module will help you understand Ray, how it gives you the tools you need for most requirements, and how it works.\n",
    "\n",
    "Even if you never need to write code in the Ray API, this module will not only help you appreciate how Ray makes your Ray-based API work, but also how to understand and fix performance issues when they arise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For More Information on Ray and Anyscale\n",
    "\n",
    "* [ray.io](https://ray.io): The Ray website. In particular:\n",
    "    * [Documentation](https://ray.readthedocs.io/en/latest/): The full Ray documentation\n",
    "    * [Blog](https://medium.com/distributed-computing-with-ray): The Ray blog\n",
    "    * [GitHub](https://github.com/ray-project/ray): The source code for Ray\n",
    "* [anyscale.com](https://anyscale.com/): The company developing Ray and these tutorials. In particular:\n",
    "    * [Blog](https://anyscale.com/blog/): The Anyscale blog\n",
    "    * [Events](https://anyscale.com/events/): Online events, [Ray Summit](http://raysummit.org), and meetups   \n",
    "    * [Academy](https://anyscale.com/academy/): Training for Ray and Anyscale products    \n",
    "    * [Jobs](https://jobs.lever.co/anyscale): Yes, we're hiring!\n",
    "* Community:\n",
    "    * [Ray Slack](ray-distributed.slack.com): The best forum for help on Ray. Use the `#tutorials` channel to ask for help on these tutorials!\n",
    "    * [ray-dev mailing list](https://groups.google.com/forum/?nomobile=true#!forum/ray-dev)\n",
    "    * [@raydistributed](https://twitter.com/raydistributed)\n",
    "    * [@anyscalecompute](https://twitter.com/anyscalecompute)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "382.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
