{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ray Concepts - Data Parallelism (Part 2)\n",
    "\n",
    "The previous lesson explored Ray's core concepts and how they work. We learned how to define Ray _tasks_, run them, and retrieve the results. We  also started learning about how Ray schedules tasks in a distributed environment.\n",
    "\n",
    "This lesson completes the discussion of Ray tasks by exploring how task dependencies are handled. We'll also look under the hood at Ray's architecture and runtime behavior.\n",
    "\n",
    "> **Tip:** Recall that the [Ray Package Reference](https://ray.readthedocs.io/en/latest/package-ref.html) in the [Ray Docs](https://ray.readthedocs.io/en/latest/) is useful for exploring the API features we'll learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and initialize Ray. We're adding NumPy for the examples and the tutorial `util` library:\n",
    "\n",
    "import ray, time, sys    # New notebook, so new process\n",
    "import numpy as np       # Used for examples\n",
    "sys.path.append('..')    # Import our own libraries starting in the project root directory\n",
    "\n",
    "from util.printing import pnd, pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-06 13:29:41,448\tINFO resource_spec.py:204 -- Starting Ray with 4.05 GiB memory available for workers and up to 2.03 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n",
      "2020-04-06 13:29:41,786\tINFO services.py:1146 -- View the Ray dashboard at \u001b[1m\u001b[32mlocalhost:8266\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '192.168.1.149',\n",
       " 'redis_address': '192.168.1.149:52078',\n",
       " 'object_store_address': '/tmp/ray/session_2020-04-06_13-29-41_439823_29297/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2020-04-06_13-29-41_439823_29297/sockets/raylet',\n",
       " 'webui_url': 'localhost:8266',\n",
       " 'session_dir': '/tmp/ray/session_2020-04-06_13-29-41_439823_29297'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init(ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's work with a new remote function. Previously, our `expensive` and `expensive_task` functions returned tuples that included time durations. Obviously the durations were useful for understanding how long the functions took to execute. Now, it will be more convenient to not return \"metadata\" like this, but just data values that we care about, because we are going to pass them to other functions. \n",
    "\n",
    "Hence, we'll define _dependentcy_ relationships between tasks. We'll learn how Ray handles these dependent, asynchronous computations.\n",
    "\n",
    "So, let's define a task to return a random NumPy array of some size `n`. As before, we'll add a sleep time, one tenth the size of `n`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def make_array(n):\n",
    "    time.sleep(n/10.0)\n",
    "    return np.random.standard_normal(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define a task that can add two NumPy arrays together. The arrays need to be the same size, but we'll ignore any checking for this requirement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def add_arrays(a1, a2):\n",
    "    time.sleep(a1.size/10.0)\n",
    "    return np.add(a1, a2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets use them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.46278599  1.3091778   0.83837591  1.53840337 -0.30042672 -0.06983262\n",
      "  0.12076512 -1.19392557  0.25440688 -0.12887382 -1.05815889  1.11084168\n",
      " -1.53943636  0.26230243 -0.72355366 -1.64677165  0.46320472 -1.11021867\n",
      "  1.35706638 -1.99027404]\n",
      "Total time: duration:  4.032 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "id1 = make_array.remote(20)\n",
    "id2 = make_array.remote(20)\n",
    "id3 = add_arrays.remote(id1, id2)\n",
    "print(ray.get(id3))\n",
    "pd(time.time() - start, prefix=\"Total time:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something subtle and \"magical\" happened here; when we called `add_arrays`, we didn't need to call `ray.get()` first for `id1` and `id2`, since `add_arrays` expects NumPy arrays. Because `add_arrays` is a Ray task, Ray automatically does the extraction for us, so we can write code that looks more natural.\n",
    "\n",
    "Furthermore, note that the `add_arrays` task effectively depends on the outputs of the two `make_array` tasks. Ray won't run `add_arrays` until the other tasks are finished. Hence, _Ray handles task dependencies automatically for us._ \n",
    "\n",
    "This is why the elapsed time is about 4 seconds. We used a size of 20, so we slept 2 seconds in each call to `make_array`, but those happened in parallel, _followed_ by a second sleep of 2 seconds in `add_arrays`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from the previous lesson that we explored when to call `ray.get()` to avoid forcing tasks to become synchronous when they should be asynchronous. This additional example illustrates two key points:\n",
    "\n",
    "* _Don't ask for results you don't need._\n",
    "* _Don't ask for the results you need until you really need them._\n",
    "\n",
    "We don't need to see the objects for `id1` and `id2`. We only need the final array for `id3`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using ray.wait() with ray.get()\n",
    "\n",
    "We've seen several examples of the best idiomatic way to use `ray.get()`. Here again is an example from the last lesson:\n",
    "\n",
    "```python\n",
    "start = time.time()\n",
    "ids = [expensive_task.remote(n) for n in range(5)]  # Fire off the asynchronous tasks\n",
    "for n2, duration in ray.get(ids):                   # Retrieve all the values from the list of futures\n",
    "    p(n2, duration)\n",
    "pd(time.time() - start, prefix=\"Total time:\")\n",
    "```\n",
    "\n",
    "Let's try it again with our new methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: []\n",
      "10: [-2.70758035 -0.99925313 -1.84091163  1.96740565  2.4847109   0.98100963\n",
      " -1.24172508 -1.70306979 -1.90439842  2.30524534]\n",
      "20: [-1.07595426  1.3081596  -1.68500055 -0.14567988 -0.08601576  0.95707809\n",
      " -0.01646987  1.45861077 -3.42676922 -1.51516786  3.82055149 -0.99646118\n",
      "  3.50673127 -3.91386345 -1.33745696 -1.99430198  0.37948619  0.49195842\n",
      "  1.28710605  0.04740637]\n",
      "30: [-1.7477596  -0.57319718  0.6634652  -2.67788299 -2.76149058  3.49889132\n",
      "  3.78105266 -0.19833039 -3.89597939 -1.60106731  0.63028907  1.86981812\n",
      " -1.69830529  0.73105321 -3.48515744 -0.46035605 -0.94157455 -0.71955543\n",
      " -0.44421647  0.00737847  1.22214649 -2.05272376 -4.71482278 -1.1625598\n",
      "  2.56262296  0.1909566  -1.77348868  0.24206547  0.53507591 -2.23155352]\n",
      "40: [ 3.66916878  1.34677088  0.09755048 -1.89966179 -0.76563557 -2.29965443\n",
      " -0.67844454 -2.97577402 -0.76400462 -0.30970419  0.20638629 -2.59799329\n",
      " -0.07680009  1.73103041  2.87109796  0.17240239 -0.16506798  0.52234051\n",
      "  5.35822363  0.5717204  -1.84936071 -0.26036525  0.06535798  2.85078044\n",
      " -2.41232586 -0.90223796 -1.82117583  0.8780666  -4.24934352 -0.1009302\n",
      " -1.59340817  2.91135378 -0.86130533  1.94709926 -1.72022801 -0.25866652\n",
      "  0.98982058  1.30526098  0.7367221   3.36748044]\n",
      "Total time: duration:  8.017 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "array_ids = [make_array.remote(n*10) for n in range(5)]\n",
    "added_array_ids = [add_arrays.remote(id, id) for id in array_ids]\n",
    "for array in ray.get(added_array_ids):\n",
    "    print(f'{array.size}: {array}')\n",
    "pd(time.time() - start, prefix=\"Total time:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On my machine, I waited 8 seconds and then everything was printed at once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two fundamental problems with the way we've used `ray.get()` so far:\n",
    "\n",
    "1. There's no timeout, in case something gets \"hung\".\n",
    "2. We have to wait for _all_ the objects to be available before `ray.get()` returns.\n",
    "\n",
    "The ability to specify a timeout is essential in production code as a defensive measure. Many potential problems could happen in a real production system, any one of which could cause the task we're waiting on to take an abnormally long time to complete or never complete. Our application would be deadlocked waiting on this task. Hence, it's **strongly recommended** in production software to always use timeouts on blocking calls, so that the application can attempt some sort of recovery in situations like this, or at least report the error and \"degrade gracefully\".\n",
    "\n",
    "Actually, there _is_ a `timeout=<value>` option you can pass to `ray.get()` ([documentation](https://ray.readthedocs.io/en/latest/package-ref.html#ray.get)), but it will most likely be removed in a future release of Ray. Why remove it if timeouts are important? This change will simplify the implementation of `ray.get()` and encourage the use of `ray.wait()` for waiting ([documentation](https://ray.readthedocs.io/en/latest/package-ref.html#ray.wait)) instead, followed by using `ray.get()` to retrieve values for tasks that `ray.wait()` tells us are finished. \n",
    "\n",
    "Using `ray.wait()` is also the way to fix the second problem with using `ray.get()` by itself, that we have to wait for all tasks to finish before we get any values back. Some of those tasks might finish quickly, like our contrived examples that sleep for short durations compared to other invocations. \n",
    "\n",
    "When you have a list of asynchronous tasks, you want to process the results of them as soon they become available, even while others continue to run. Use `ray.wait()` for this purpose.\n",
    "\n",
    "Therefore, while `ray.get()` is simple and convenient, for _production code_, we recommend using `ray.wait()`, **with** timeouts, for blocking on running tasks. Then use `ray.get()` to retrieve values of completed tasks. Now we'll learn how to use these two together. For a longer discussion on `ray.wait()`, see [this blog post](https://medium.com/distributed-computing-with-ray/ray-tips-and-tricks-part-i-ray-wait-9ed7a0b9836d).\n",
    "\n",
    "Here is the previous example rewritten to use `ray.wait()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Returned   1 completed tasks. (elapsed time:  0.005)\n",
      "0: []\n",
      "Returned   1 completed tasks. (elapsed time:  2.012)\n",
      "10: [ 0.72887255  2.75195066 -2.49829625 -0.26751108  5.23830508 -0.67140578\n",
      "  0.98708413  2.42503449  1.97375834  0.02753384]\n",
      "Returned   1 completed tasks. (elapsed time:  4.015)\n",
      "20: [-2.41666655e+00 -3.80435235e+00 -2.07063148e+00 -2.93908226e-01\n",
      " -2.99361546e-03 -2.79436701e+00  4.12693904e+00 -1.12538571e-01\n",
      "  3.80724688e+00 -3.29507009e+00  1.38222164e+00 -2.52271210e+00\n",
      "  1.54011302e+00  2.53180030e+00 -2.77913815e-01  1.45905084e+00\n",
      "  2.61237500e-01 -2.02075131e+00  2.14265696e+00 -1.13976267e+00]\n",
      "Returned   1 completed tasks. (elapsed time:  6.012)\n",
      "30: [-0.20290182  2.36678258 -1.28286253  2.07838619 -1.16608706 -0.56844014\n",
      "  4.72363961 -1.00907559 -1.94756655 -0.37628311 -1.05179824 -3.51640776\n",
      "  1.05171619  1.18265669  0.36203459 -0.21969228 -0.24842565 -1.3490668\n",
      " -4.69822701  3.50000913  0.45740144  0.74539234  0.09017152  0.61303288\n",
      " -0.83737517 -2.74525008 -1.77579065  0.73242404  1.98960842 -0.6902046 ]\n",
      "Returned   1 completed tasks. (elapsed time:  8.012)\n",
      "40: [-0.7642636  -5.34975287 -0.95098853  0.42362318 -0.88425905 -1.9574385\n",
      "  0.75921739  0.96145872  1.72679944  0.69532403  3.23321967  2.60685026\n",
      " -0.26016481  1.27254054 -0.16379796  1.62618277 -1.47584712 -1.03757576\n",
      " -2.16839105  0.91777835  4.40096829  1.54622785  0.40304952 -2.23283167\n",
      "  1.24186702  1.15757802  1.21334583 -2.1222667  -3.24519431 -2.01967588\n",
      "  0.01130926 -2.06362754  1.00869602  1.68497185 -0.7076441   0.75116092\n",
      "  1.91466824 -0.26340262  0.77036224 -2.81411822]\n",
      "Total time: duration:  8.014 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "array_ids = [make_array.remote(n*10) for n in range(5)]\n",
    "added_array_ids = [add_arrays.remote(id, id) for id in array_ids]\n",
    "\n",
    "waiting_ids = list(added_array_ids)        # Assign a working list to the full list of ids\n",
    "while len(waiting_ids) > 0:                # Loop until all tasks have completed\n",
    "    # Call ray.wait with:\n",
    "    #   1. the list of ids we're still waiting to complete,\n",
    "    #   2. tell it to return immediately as soon as one of them completes,\n",
    "    #   3. tell it wait up to 10 seconds before timing out.\n",
    "    ready_ids, remaining_ids = ray.wait(waiting_ids, num_returns=1, timeout=10.0)\n",
    "    print('Returned {:3d} completed tasks. (elapsed time: {:6.3f})'.format(len(ready_ids), time.time() - start))\n",
    "    for array in ray.get(ready_ids):\n",
    "        print(f'{array.size}: {array}')\n",
    "        \n",
    "    waiting_ids = remaining_ids  # Reset this list; don't include the completed ids in the list again!\n",
    "    \n",
    "pd(time.time() - start, prefix=\"Total time:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it still takes about 8 seconds to complete, 4 seconds for the longest invocation of `make_array` and 4 seconds for the invocation of `add_arrays`, but since the others complete more quickly, we see their results as soon as they become available, at 0, 2, 4, and 6 second intervals.\n",
    "\n",
    "> **Warning:** For each call to `ray.wait()` in a loop like this, it's important to remove the ids that have completed. Otherwise, `ray.wait()` will return immediately with the same list containg the first completed item, over and over again; you'll loop forever!! Resetting the list is easy, since the second list returned by `ray.wait()` is the rest of the items that are still running. So, that's what we use.\n",
    "\n",
    "Now let's try it with `num_returns = 2`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Returned   2 completed tasks. (elapsed time:  2.010)\n",
      "0: []\n",
      "10: [ 1.7215186  -1.20758138 -0.44587926 -1.9540483   0.42110723 -1.10633336\n",
      " -0.69527172 -1.82910789 -1.5393299   0.83943492]\n",
      "Returned   2 completed tasks. (elapsed time:  6.009)\n",
      "20: [ 1.52389709 -1.5249968  -3.31701453  1.47170745  0.17517923 -0.22834857\n",
      "  3.08451281 -2.64095245  0.56596446 -2.46669565 -1.29576393 -1.30380599\n",
      "  0.84097717  0.02547861 -0.91041547  0.6328555  -4.02808826  2.71486281\n",
      " -2.29350491  1.40191421]\n",
      "30: [-1.32017215  3.59105792 -0.87426021  0.58457614  2.0363863   2.25183006\n",
      "  3.84991622 -0.19640412  1.13065231  0.2807348   0.22267571  0.18652756\n",
      "  1.34203745  0.10917007 -0.28581186 -0.31800248  1.13338547 -1.0040262\n",
      " -3.16359625 -0.73747263  1.74298806  2.11467092 -1.46946596  0.01565229\n",
      "  0.81415958 -0.71352269 -0.88725705 -1.69699777 -0.19842466 -4.92212589]\n",
      "Returned   1 completed tasks. (elapsed time:  8.014)\n",
      "40: [ 3.55486039  1.29231059  2.71110267 -1.1542018  -0.98062425  2.85048659\n",
      "  0.57192423  2.87841225  0.5357724  -1.51347621 -1.16038308  2.78611515\n",
      "  0.27317677  0.26468275 -1.532469   -1.43970334 -0.572281   -4.22075721\n",
      " -1.02860917 -1.10627737 -0.35233326 -2.22512527 -2.9820754   0.74168456\n",
      "  1.44278329 -0.17660943 -1.24346773 -5.30223197  3.02499289 -0.97585133\n",
      "  1.42738687  3.10234872 -1.77465841 -0.3906324  -1.40148334  1.78290713\n",
      "  3.23652756 -0.91734739  1.74870791  0.99506548]\n",
      "Total time: duration:  8.016 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "array_ids = [make_array.remote(n*10) for n in range(5)]\n",
    "added_array_ids = [add_arrays.remote(id, id) for id in array_ids]\n",
    "\n",
    "waiting_ids = list(added_array_ids)        # Assign a working list to the full list of ids\n",
    "while len(waiting_ids) > 0:                # Loop until all tasks have completed\n",
    "    # Call ray.wait with:\n",
    "    #   1. the list of ids we're still waiting to complete,\n",
    "    #   2. tell it to return immediately as soon as TWO of them complete,\n",
    "    #   3. tell it wait up to 10 seconds before timing out.\n",
    "    return_n = 2 if len(waiting_ids) >= 2 else 1   # See discussion in the next cell\n",
    "    ready_ids, remaining_ids = ray.wait(waiting_ids, num_returns=return_n, timeout=10.0)\n",
    "    print('Returned {:3d} completed tasks. (elapsed time: {:6.3f})'.format(len(ready_ids), time.time() - start))\n",
    "    for array in ray.get(ready_ids):\n",
    "        print(f'{array.size}: {array}')\n",
    "        \n",
    "    waiting_ids = remaining_ids  # Reset this list; don't include the completed ids in the list again!\n",
    "    \n",
    "pd(time.time() - start, prefix=\"Total time:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get two at a time output. Note that we don't actually pass `num_returns=2` every time. If you ask for more items than the length of the input list, you get an error. So, we compute `num_returns`, using `2` except when there's only one task to wait on, in which case we use `1`. So, in fact, the output for `40` was a single task result, because we started with `5` and processed two at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "The following cell is identical to the last one. Modify it to use a timeout of `2.5` seconds, shorter than our longest tasks. What happens now? Try using other times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "array_ids = [make_array.remote(n*10) for n in range(5)]\n",
    "added_array_ids = [add_arrays.remote(id, id) for id in array_ids]\n",
    "\n",
    "waiting_ids = list(added_array_ids)        # Assign a working list to the full list of ids\n",
    "while len(waiting_ids) > 0:                # Loop until all tasks have completed\n",
    "    # Call ray.wait with:\n",
    "    #   1. the list of ids we're still waiting to complete,\n",
    "    #   2. tell it to return immediately as soon as TWO of them complete,\n",
    "    #   3. tell it wait up to 10 seconds before timing out.\n",
    "    return_n = 2 if len(waiting_ids) >= 2 else 1   # See discussion in the next cell\n",
    "    ready_ids, remaining_ids = ray.wait(waiting_ids, num_returns=return_n, timeout=10.0)\n",
    "    print('Returned {:3d} completed tasks. (elapsed time: {:6.3f})'.format(len(ready_ids), time.time() - start))\n",
    "    for array in ray.get(ready_ids):\n",
    "        print(f'{array.size}: {array}')\n",
    "        \n",
    "    waiting_ids = remaining_ids  # Reset this list; don't include the completed ids in the list again!\n",
    "    \n",
    "pd(time.time() - start, prefix=\"Total time:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion:\n",
    "\n",
    "> **Tips:**\n",
    ">\n",
    "> 1. Use `ray.wait()` with a timeout to wait for one or more running tasks. Then use `ray.get()` to retrieve the values for the finished tasks.\n",
    "> 2. Don't ask for results you don't need.\n",
    "> 3. Don't ask for the results you need until you really need them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Distributed Task Management Works\n",
    "\n",
    "> **Note:** If you just want to learn the Ray API, you can safely skip the rest of this lesson (notebook) for now. It continues the exploration of how Ray works internally, which we started in the previous lesson. However, you should come back to this material at some point, so you'll develop a better understanding of how Ray works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of the last lesson, we examined Ray task scheduling at a high-level, by watching the Ray Dashboard and analyzing the performance times. Now we'll walk through some images that show the process Ray follows to place tasks around a cluster. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume we will invoke the `make_array` task twice, then invoke `add_arrays` to sum the returned NumPy arrays. Graphically, it looks as follows:\n",
    "![Ray under the hood 1](../images/Ray-Cluster/Ray-Cluster.001.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does this get scheduled in a cluster? Here we'll assume a three-node cluster that has resources for running two Ray worker tasks per node (under powered compared to what we learned using Ray Dashboard last lesson!).\n",
    "![Ray under the hood 2](../images/Ray-Cluster/Ray-Cluster.002.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, assume that the driver program is running on Node1. So it will invoke the local scheduler to schedule the three tasks.\n",
    "![Ray under the hood 3](../images/Ray-Cluster/Ray-Cluster.003.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Immediately the ids for the task futures are returned. The _Global Control Store_ tracks where every task is running and every object is stored in the local _Object Stores_.\n",
    "![Ray under the hood 4](../images/Ray-Cluster/Ray-Cluster.004.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose the local scheduler has available capacity in the first worker on the same node. It schedules the first `make_array` task there.\n",
    "![Ray under the hood 5](../images/Ray-Cluster/Ray-Cluster.005.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It decides to schedule the second `make_array` task in a worker on node 2.\n",
    "![Ray under the hood 6](../images/Ray-Cluster/Ray-Cluster.006.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the two tasks finish, they place their result objects in their local object stores.\n",
    "![Ray under the hood 7](../images/Ray-Cluster/Ray-Cluster.007.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now `add_array` can be scheduled, because the two tasks it depends on are done. Let's suppose it gets scheduled in the second worker on Node 1.\n",
    "![Ray under the hood 8](../images/Ray-Cluster/Ray-Cluster.008.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first object it needs is already on the same node, in the object store, so the `add_arrays` task can _read it directly from shared memory_. No copying is required to the worker's process space.\n",
    "![Ray under the hood 9](../images/Ray-Cluster/Ray-Cluster.009.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the second object is on a different node, so Ray copies it to the local object store. \n",
    "![Ray under the hood 10](../images/Ray-Cluster/Ray-Cluster.010.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it can also be read from shared memory.\n",
    "![Ray under the hood 11](../images/Ray-Cluster/Ray-Cluster.011.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When `add_arrays` is finished, it writes its results to the local object store.\n",
    "![Ray under the hood 12](../images/Ray-Cluster/Ray-Cluster.012.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, if the driver calls `ray.get(id3)`, it will return `obj3`.\n",
    "![Ray under the hood 13](../images/Ray-Cluster/Ray-Cluster.013.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whew! Hopefully you have a better sense of what Ray does under the hood. Scheduling tasks on other nodes and copying objects between object stores is efficient, but incurs unavoidable network overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "382.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
