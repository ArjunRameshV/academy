{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ray Crash Course - Actors\n",
    "\n",
    "We explored how Ray _tasks_ allow us to maximize utilization of our cluster resources. However, a typical challenge in distributed systems is management of distributed _state_. Ray's solution is _Actors_.\n",
    "\n",
    "The [Actor Model](https://en.wikipedia.org/wiki/Actor_model) is really similar to what Alan Kay had in mind for object-oriented programming, which he implemented in [Smalltalk](https://en.wikipedia.org/wiki/Smalltalk). Specifically, objects are autonomous agents that communicate and coordinate activities by exchanging messages. Each object/actor manages its own encapsulated state. Modern actor model implementations add constructs for distributed computation and thread-safe concurrency. \n",
    "\n",
    "As we'll see, Ray converts normal Python classes into Ray actors, analogous to way stateless functions were turned into Ray tasks. Classes are used because they are a familiar tool in languages like Python for encapsulating state with methods for manipulating and exposing the state.\n",
    "\n",
    "In addition to [Ray's actor model implementation](https://ray.readthedocs.io/en/latest/actors.html), other implementations include [Erlang](https://www.erlang.org/), the first production-grade implementation of actors, [Akka](https://akka.io/) for the JVM, and [many others](https://en.wikipedia.org/wiki/Actor_model#Actor_libraries_and_frameworks).\n",
    "\n",
    "> **Tip:** For more about Ray, see [ray.io](https://ray.io) or the [Ray documentation](https://ray.readthedocs.io/en/latest/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll demonstrate Ray actors using a simplified implementation of a _parameter server_. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Server Scenario\n",
    "\n",
    "The idea of a _parameter server_ emerged as a way to manage model parameters at large scales. A parameter server is effectively an optimized database, possibly distributed, for updating and sharing model parameters in model training and serving scenarios, especially for very large deep learning models.\n",
    "\n",
    "Here is a conceptual sketch of a parameter server and other components that use it.\n",
    "\n",
    "![Parameter Server](../images/Parameter-Server.png)\n",
    "\n",
    "New data is stored for use in the next cycle of model training. It is also passed to a model server to score the data with one or more previously trained models. The scored data is sent to other services as required. Training data is feed from storage to the trainers. The parameter server is used as durable storage for parameters used by trainers during models training, and it serves parameters for previously-trained models to the model server. \n",
    "\n",
    "For more information on the concept of a parameter server:\n",
    "\n",
    "* [Parameter Server for Distributed Machine learning](https://medium.com/coinmonks/parameter-server-for-distributed-machine-learning-fd79d99f84c3)\n",
    "* [Scaling Distributed Machine Learning with the Parameter Server](https://www.cs.cmu.edu/~muli/file/parameter_server_osdi14.pdf)\n",
    "\n",
    "For a more extensive example of parameter server usage in Ray, see [this section of the Ray documentation](https://ray.readthedocs.io/en/latest/auto_examples/plot_parameter_server.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with some imports and definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally you would let Ray determine the actual number of CPU cores, but on a single machine, we'll pretend we have more of them, which is what `num_workers` will be used for.\n",
    "\n",
    "**Mini Exercise:** Play with these numbers. For older machines, a shorter sleep time or smaller `num_workers` or `num_iterations` might be useful. The `num_params` value doesn't make much difference in the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 16          # Number of CPUs we'll \"pretend\" we have available\n",
    "num_parameters = 10000    # Number of parameters (array size) in a model\n",
    "num_trainers = 4          # Number of trainers that use the parameter server\n",
    "num_iterations = 500      # Number of iterations for a training run (per trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a simple Python class, `ParameterServer`, that wraps a dictionary of parameter objects as values associated with arbitrary names as keys. Each model trainer we implement will use its own key for its set of parameters. This would work if the trainers are all working on the same model, with each trainer working on a partition of the model parameters. It would also work when each trainer has a whole model, with other trainers working on different models.\n",
    "\n",
    "Finally, the parameter server can also serve trained model parameters to model servers.\n",
    "\n",
    "We'll leave the type of parameters unspecified, so different kinds of parameter sets can be managed transparently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParameterServer():\n",
    "    def __init__(self):\n",
    "        self.parameters = {}\n",
    "\n",
    "    def get(self, key):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            The parameters for a given key or None by default.\n",
    "        \"\"\"\n",
    "        return self.parameters.get(key, None)\n",
    "\n",
    "    def set(self, key, new_params):\n",
    "        self.parameters[key] = new_params\n",
    "        \n",
    "    def keys(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            The dictionary keys object converted to a list. This conversion isn't\n",
    "            particularly necessary here, but it will be needed for remote objects, \n",
    "            because a dictionary keys object can't be pickled! So, to minimize \n",
    "            required changes later, we'll return a list here.\n",
    "        \"\"\"\n",
    "        return list(self.parameters.keys())\n",
    "    \n",
    "    def __str__(self):  # Useful for printing below\n",
    "        return f'ParameterServer(keys={self.keys()})'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trainer runs a user-specified number of iterations to improve the parameter set. To simulate a real training process like gradient descent, we compute random deltas and apply them to the last state of the parameters. The parameter set is kept here, so this class is also stateful, but updates are written to the server every time.\n",
    "\n",
    "Unlike `ParameterServer`, we assume the parameters are a list, for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self, name, param_server, initial_parameters):\n",
    "        \"\"\"Pushes the initial parameters to the parameter server\"\"\"\n",
    "        self.name = name\n",
    "        self.parameters = initial_parameters\n",
    "        self.param_server = param_server\n",
    "        self.send_to_server()\n",
    "\n",
    "    def send_to_server(self):\n",
    "        self.param_server.set(self.name, self.parameters)\n",
    "\n",
    "    def train(self, num_iterations):\n",
    "        \"\"\"\n",
    "        Simulate training by running 'num_iterations' iterations. For each step, compute \n",
    "        random deltas to update the parameters, simulating gradient descent. Push the\n",
    "        updated parameters to the parameter server.\n",
    "        \n",
    "        Returns:\n",
    "            (name, params)\n",
    "        \"\"\"\n",
    "        for _ in range(num_iterations):\n",
    "            deltas = [random.uniform(-0.1,0.1) for i in range(self.parameters.size)]\n",
    "            self.parameters = np.add(self.parameters, deltas)\n",
    "        self.send_to_server()\n",
    "        return (self.name, self.parameters)\n",
    "    \n",
    "    def __str__(self):  # Useful for printing below\n",
    "        return f'Trainer(name={self.name}, parameters={self.parameters}, param_server={self.param_server}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate `num_trainers` trainers, simulating training over a partitioned parameter set. For simplicity, we'll use a single parameter server. In a real-world scenario, this might become a bottleneck, so several servers might be used for the partitioned parameter set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = ParameterServer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 Trainers created:\n",
      "Trainer(name=Trainer0, parameters=[0. 0. 0. ... 0. 0. 0.], param_server=ParameterServer(keys=['Trainer0', 'Trainer1', 'Trainer2', 'Trainer3'])\n",
      "Trainer(name=Trainer1, parameters=[0. 0. 0. ... 0. 0. 0.], param_server=ParameterServer(keys=['Trainer0', 'Trainer1', 'Trainer2', 'Trainer3'])\n",
      "Trainer(name=Trainer2, parameters=[0. 0. 0. ... 0. 0. 0.], param_server=ParameterServer(keys=['Trainer0', 'Trainer1', 'Trainer2', 'Trainer3'])\n",
      "Trainer(name=Trainer3, parameters=[0. 0. 0. ... 0. 0. 0.], param_server=ParameterServer(keys=['Trainer0', 'Trainer1', 'Trainer2', 'Trainer3'])\n"
     ]
    }
   ],
   "source": [
    "ts = [Trainer(f'Trainer{i}', ps, np.zeros(num_parameters)) for i in range(num_trainers)]\n",
    "print(f'{num_trainers} Trainers created:')\n",
    "for t in ts:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the initial parameters were written to the server:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer0: [0. 0. 0. ... 0. 0. 0.]\n",
      "Trainer1: [0. 0. 0. ... 0. 0. 0.]\n",
      "Trainer2: [0. 0. 0. ... 0. 0. 0.]\n",
      "Trainer3: [0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "for key in ps.keys():\n",
    "    print(f'{key}: {ps.get(key)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell takes about 5 seconds user time, 10 seconds wall time, on a 2019 model MacBook Pro 13\"..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.81 s, sys: 77.5 ms, total: 4.89 s\n",
      "Wall time: 4.92 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Trainer0',\n",
       "  array([-0.00329442,  0.86496108, -1.51937846, ..., -0.88591523,\n",
       "          0.3509482 , -1.96414379])),\n",
       " ('Trainer1',\n",
       "  array([-0.65204375, -0.02438378,  0.24815471, ...,  1.05218844,\n",
       "         -0.02307567,  1.50306938])),\n",
       " ('Trainer2',\n",
       "  array([-0.58644958, -0.52373158, -0.47386362, ...,  1.01543566,\n",
       "         -1.32350437, -0.36347235])),\n",
       " ('Trainer3',\n",
       "  array([ 1.30368004,  0.37275973, -0.63920378, ..., -1.60705274,\n",
       "          2.21114751, -0.94467373]))]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time [t.train(num_iterations) for t in ts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer0: [-0.00329442  0.86496108 -1.51937846 ... -0.88591523  0.3509482\n",
      " -1.96414379]\n",
      "Trainer1: [-0.65204375 -0.02438378  0.24815471 ...  1.05218844 -0.02307567\n",
      "  1.50306938]\n",
      "Trainer2: [-0.58644958 -0.52373158 -0.47386362 ...  1.01543566 -1.32350437\n",
      " -0.36347235]\n",
      "Trainer3: [ 1.30368004  0.37275973 -0.63920378 ... -1.60705274  2.21114751\n",
      " -0.94467373]\n"
     ]
    }
   ],
   "source": [
    "for key in ps.keys():\n",
    "    print(f'{key}: {ps.get(key)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling the Parameter Server with Ray\n",
    "\n",
    "Let's make this faster with Ray. Clearly the trainers are completely independent, so they should run in parallel. We'll also make the `ParameterServer` a Ray actor, even though there is currently only one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-21 17:16:56,243\tERROR worker.py:682 -- Calling ray.init() again after it has already been called.\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "ray.init(num_cpus = num_workers, ignore_reinit_error = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dashboard URL: http://localhost:8266\n"
     ]
    }
   ],
   "source": [
    "print(f'Dashboard URL: http://{ray.get_webui_url()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Ray Actor from a Class.\n",
    "\n",
    "We created Ray _tasks_ by decorating normal Python _functions_ with `@ray.remote`. Similarly, we create Ray _actors_ by decorating normal Python _classes_ with `@ray.remote`.\n",
    "\n",
    "One important difference between actors and classes is how you access _fields_ (or _attributes_) of an instance. While in Python you can just say `my_instance.field`, this isn't supported at this time for Ray actors. Instead, you have to provide accessor methods for all fields (or derived state) that users might need. \n",
    "\n",
    "Fortunately, we can subclass a normal Python class to create an actor, and add the additional accessor methods we might need. In our case, we already have everything we need in `ParameterServer`, specifically the `keys()` and `get()` methods, although you could add a method to return the entire dictionary at once. So, our actor declaration, `RayParameterServer`, is quite simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "class RayParameterServer(ParameterServer):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actor instances are constructed with `.remote(...)`, like calling tasks, and actor methods are called with `my_instance.method.remote(...)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "rps = RayParameterServer.remote()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corresponding Ray trainer has to invoke the remote server using `param_server.x.remote(...)`. Fortunately, we already encapsulated calls to the parameter server in `send_to_server()`, so all we need to do to create a new trainer is to subclass `Trainer` and override that method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "class RayTrainer(Trainer):\n",
    "    def __init__(self, name, param_server, initial_parameters):\n",
    "        super().__init__(name, param_server, initial_parameters)\n",
    "\n",
    "    def send_to_server(self):\n",
    "        \"\"\"A synchronously send updates.\"\"\"\n",
    "        self.param_server.set.remote(self.name, self.parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 Ray Trainers created:\n",
      "Actor(RayTrainer, 8e3e47920100)\n",
      "Actor(RayTrainer, 169cae520100)\n",
      "Actor(RayTrainer, a1651cc50100)\n",
      "Actor(RayTrainer, a000cd7e0100)\n"
     ]
    }
   ],
   "source": [
    "rts = [RayTrainer.remote(f'RayTrainer{i}', rps, np.zeros(num_parameters)) for i in range(num_trainers)]\n",
    "print(f'{num_trainers} Ray Trainers created:')\n",
    "for rt in rts:\n",
    "    print(rt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell takes about 25 **milliseconds**, 1.5 wall time seconds, on the same 2019 model MacBook Pro 13\", roughly 1/4th the wall time as above, reflecting the difference between using one core previously and all four-cores on the machine's CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24.7 ms, sys: 9.93 ms, total: 34.6 ms\n",
      "Wall time: 1.51 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('RayTrainer0',\n",
       "  array([ 0.03254323,  0.32799843,  1.14463483, ..., -1.04798486,\n",
       "         -3.02949342,  0.87614499])),\n",
       " ('RayTrainer1',\n",
       "  array([ 2.26093849,  1.18781758,  0.90302837, ..., -0.14009133,\n",
       "          1.36071276,  0.29690826])),\n",
       " ('RayTrainer2',\n",
       "  array([ 0.55147404, -0.91344929, -0.64431792, ...,  0.47301978,\n",
       "          1.69434414, -0.17184378])),\n",
       " ('RayTrainer3',\n",
       "  array([-1.03341044,  1.18637555, -0.89236703, ...,  1.57739792,\n",
       "         -0.39743502, -1.08034883]))]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time ray.get([rt.train.remote(num_iterations) for rt in rts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RayTrainer0', 'RayTrainer2', 'RayTrainer3', 'RayTrainer1']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.get(rps.keys.remote())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "`RayTrainer` asynchronously sends the updates to the `ParameterServer`, but doesn't confirm success. Many distributed systems require a confirmation before an update is considered confirmed. Modify `send_to_server` to wait for confirmation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "Try different values of the parameters, `num_workers`, `num_parameters`, `num_trainers`, and `num_iterations`. How do the results change?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
