{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ray Crash Course - Actors\n",
    "\n",
    "We explored how Ray _tasks_ allow us to maximize utilization of our cluster resources. However, a typical challenge in distributed systems is management of distributed _state_. Ray's solution is _Actors_.\n",
    "\n",
    "The [Actor Model](https://en.wikipedia.org/wiki/Actor_model) is really similar to what Alan Kay had in mind for object-oriented programming, which he implemented in [Smalltalk](https://en.wikipedia.org/wiki/Smalltalk). Specifically, objects are autonomous agents that communicate and coordinate activities by exchanging messages. Each object/actor manages its own encapsulated state. Modern actor model implementations add constructs for distributed computation and thread-safe concurrency. \n",
    "\n",
    "In addition to [Ray's actor model implementation](https://ray.readthedocs.io/en/latest/actors.html), other implementations include [Erlang](https://www.erlang.org/), the first production-grade implementation of actors, [Akka](https://akka.io/) for the JVM, and [many others](https://en.wikipedia.org/wiki/Actor_model#Actor_libraries_and_frameworks).\n",
    "\n",
    "For more about Ray, see [ray.io](http://ray.io)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll demonstrate Ray actors using a simplified implementation of a _parameter server_. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Server Scenario\n",
    "\n",
    "The idea of a _parameter server_ emerged as a way to manage model parameters at large scales. A parameter server is effectively an optimized database, possibly distributed, for updating and sharing model parameters in model training and serving scenarios, especially for very large deep learning models.\n",
    "\n",
    "For more information on the idea of a parameter server:\n",
    "\n",
    "* [Parameter Server for Distributed Machine learning](https://medium.com/coinmonks/parameter-server-for-distributed-machine-learning-fd79d99f84c3)\n",
    "* [Scaling Distributed Machine Learning with the Parameter Server](https://www.cs.cmu.edu/~muli/file/parameter_server_osdi14.pdf)\n",
    "\n",
    "For a more extensive example of parameter server usage in Ray, see [this section of the Ray documentation](https://ray.readthedocs.io/en/latest/auto_examples/plot_parameter_server.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with some imports and definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normally you would let Ray determine the actual number of cores, \n",
    "# but on a single machine, we'll pretend we have more of them.\n",
    "# Mini Exercise: Play with these numbers. For older machines, a shorter sleep time \n",
    "# or smaller `ncpus` or `niterations` might be useful. The `nparams` value won't \n",
    "# make a lot of difference\n",
    "ncpus = 16             # Number of CPUs we'll \"pretend\" we have available\n",
    "nparams = 100          # Number of parameters (array size) in a model\n",
    "ntrainers = 5          # Number of trainers that use the parameter server\n",
    "niterations = 100      # Number of iterations for a training run (per trainer)\n",
    "sleep_interval = 0.1   # Pause to simulate an expensive computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a simple, \"local\" implementation first, then use Ray. `LocalParameterServer` just wraps a dictionary of parameter objects as values associated with arbitrary names as keys. Each model trainer we implement will use its own key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalParameterServer():\n",
    "    def __init__(self):\n",
    "        self.parameters = {}\n",
    "\n",
    "    def get(self, key):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            The parameters for a given key or None by default.\n",
    "        \"\"\"\n",
    "        return self.parameters.get(key, None)\n",
    "\n",
    "    def set(self, key, new_params):\n",
    "        self.parameters[key] = new_params\n",
    "        \n",
    "    def keys(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            The dictionary keys object converted to a list. This conversion isn't\n",
    "            particularly necessary here, but it will be needed for remote objects, \n",
    "            because a dictionary keys object can't be pickled! So, to minimize \n",
    "            required changes later, we'll return a list here.\n",
    "        \"\"\"\n",
    "        return list(self.parameters.keys())        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The local model trainer runs a specified number of iterations to improve the parameter set. Here, we'll just use\n",
    "random updates, followed by short sleep calls to simulate expensive work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalTrainer():\n",
    "    def __init__(self, name, number_parameters, param_server):\n",
    "        self.name = name\n",
    "        self.number_parameters = number_parameters\n",
    "        self.param_server = param_server\n",
    "\n",
    "    def train(self, number_iterations):\n",
    "        \"\"\"\n",
    "        Simulate training by running 'number_iterations' iterations. For each one, compute \n",
    "        random deltas to update the parameters followed by a small sleep value to simulate\n",
    "        a long-running computation. When finished, set the updated parameters in the parameter\n",
    "        server.\n",
    "        \n",
    "        Returns:\n",
    "            (name, params)\n",
    "        \"\"\"\n",
    "        params = self.param_server.get(self.name) or np.zeros(self.number_parameters)\n",
    "        for _ in range(number_iterations):\n",
    "            deltas = [random.uniform(-0.1,0.1) for _ in range(self.number_parameters)]\n",
    "            params = np.add(params, deltas)\n",
    "            time.sleep(sleep_interval)\n",
    "        self.param_server.set(self.name, params)\n",
    "        return (self.name, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate `ntrainers` trainers, simulating training over a partitioned parameter set. For simplicity, we'll use a single parameter server. In a real-world scenario, this might become a bottleneck, so several servers might be used for the partitioned parameter set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lps = LocalParameterServer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5 LocalTrainers created'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lts = [LocalTrainer(f'LocalTrainer{i}', nparams, lps) for i in range(ntrainers)]\n",
    "f'{ntrainers} LocalTrainers created'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell takes 50-55 seconds on a 2019 model MacBook Pro 13\"..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-267d01a2769d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0mlt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mniterations\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-32-267d01a2769d>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0mlt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mniterations\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-20-275f5ace9b23>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, number_iterations)\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \"\"\"\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_server\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber_parameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mdeltas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber_parameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "[lt.train(niterations) for lt in lts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LocalTrainer0',\n",
       " 'LocalTrainer1',\n",
       " 'LocalTrainer2',\n",
       " 'LocalTrainer3',\n",
       " 'LocalTrainer4']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lps.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a real model-training scenario, the next step might be to combine the trainers' outputs in some way, such as joining the partitions of a model.\n",
    "\n",
    "Can we make this faster?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-20 17:35:16,176\tERROR worker.py:682 -- Calling ray.init() again after it has already been called.\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "ray.init(num_cpus = ncpus, ignore_reinit_error = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Because of features we added to `LocalParameterServer`, specifically the `params_length()` method and returning a list from `keys()`, to implement a remote parameter server we can just declare it with the `@ray.remote` annotation and parent class `LocalParameterServer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "class RemoteParameterServer(LocalParameterServer):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "rps = RemoteParameterServer.remote()  # Constructed with \".remote()\", like we saw for tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the corresponding remote trainer has to invoke the remote server using `param_server.x.remote(...)`, so we can't just use `LocalTrainer`, because we'll get errors calling methods on the remote server. So, let's implement a version that uses `RemoteParameterServer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "class RemoteTrainer():\n",
    "    def __init__(self, name, number_parameters, param_server):\n",
    "        self.name = name\n",
    "        self.number_parameters = number_parameters\n",
    "        self.param_server = param_server\n",
    "\n",
    "    def train(self, number_iterations):\n",
    "        \"\"\"\n",
    "        Simulate training by running 'number_iterations' iterations. For each one, compute \n",
    "        random deltas to update the parameters followed by a small sleep value to simulate\n",
    "        a long-running computation. When finished, set the updated parameters in the parameter\n",
    "        server.\n",
    "        \n",
    "        Returns:\n",
    "            (name, params)\n",
    "        \"\"\"\n",
    "        params = ray.get(self.param_server.get.remote(self.name)) or np.zeros(self.number_parameters)\n",
    "        for _ in range(number_iterations):\n",
    "            deltas = [random.uniform(-0.1,0.1) for _ in range(self.number_parameters)]\n",
    "            params = np.add(params, deltas)\n",
    "            time.sleep(sleep_interval)\n",
    "        self.param_server.set.remote(self.name, params)\n",
    "        return (self.name, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5 RemoteTrainers starting'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rts = [RemoteTrainer.remote(f'RemoteTrainer{i}', nparams, rps) for i in range(ntrainers)]\n",
    "f'{ntrainers} RemoteTrainers starting'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell takes about 10 seconds on a 2019 model MacBook Pro 13\". Compare with the local, sequential run above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 277 ms, sys: 107 ms, total: 384 ms\n",
      "Wall time: 10.3 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('RemoteTrainer0',\n",
       "  array([-0.76578108,  0.64613257, -0.42304991,  0.31097099, -0.53648638,\n",
       "          0.78404893, -0.80588895, -1.07725911, -0.09400659, -0.15740178,\n",
       "         -0.90572274,  0.09697633, -0.17633377,  0.16215123,  0.16490174,\n",
       "         -0.82036564, -0.01410076, -1.05618224, -0.79512241, -0.55465338,\n",
       "          0.91992343, -0.44730101,  0.12511925,  0.0896709 , -1.25615709,\n",
       "         -0.09593135,  0.02951344,  0.99751251,  1.6713826 ,  0.44934778,\n",
       "         -0.08681112, -0.38581867, -0.39818031,  0.22117686,  0.07371581,\n",
       "         -0.85347809,  0.19613094, -0.44982533, -0.07463884,  0.08599995,\n",
       "          0.14580855, -0.86564899, -0.69032072,  0.03033596, -0.32182171,\n",
       "         -0.06738351,  0.51779742,  0.47534912, -0.93897788, -0.17648893,\n",
       "          0.43458898,  0.05148876,  0.85317384, -0.0750082 ,  0.29649449,\n",
       "         -0.08501202,  0.09668437, -0.01648455,  0.50063064,  0.21857703,\n",
       "          0.08161356, -0.65354514,  0.20471728,  1.04165816, -0.00244974,\n",
       "         -0.50202676, -0.25230224,  0.08863695,  0.27981954,  0.66626291,\n",
       "         -0.80564715, -0.94090103, -0.72614577,  0.41520424,  0.61589256,\n",
       "         -0.44606892, -0.65734013, -1.24359196, -0.17133028,  0.56509263,\n",
       "          0.42379224,  1.06318898,  1.0545254 ,  1.09635634, -0.06237567,\n",
       "         -0.79863093, -0.37652558,  0.52207452, -0.28647363,  0.21796806,\n",
       "          0.4243865 ,  0.27041813,  0.13651318,  0.06007311, -0.49850899,\n",
       "          0.24396374,  0.16834314, -0.28679337, -0.5423292 ,  0.52143491])),\n",
       " ('RemoteTrainer1',\n",
       "  array([-0.2184868 , -0.39658687, -0.51499664,  0.10886474, -0.92295238,\n",
       "          0.025997  ,  0.99043908, -0.69074028,  1.25475931,  0.20513093,\n",
       "          0.34039728, -1.05625515,  0.29953356,  0.60228775,  0.51481567,\n",
       "         -0.11709804, -0.12486386,  0.73195568, -0.65446092, -1.36156412,\n",
       "          0.50541707, -0.39115814,  0.02096401,  0.31465199,  0.09910914,\n",
       "         -0.51589526,  0.18318223,  0.17216456, -0.989894  , -0.98849484,\n",
       "          0.76379114, -0.43170629,  0.11310908, -1.02030598, -1.13031505,\n",
       "          0.92632486, -0.3465746 , -0.12821322,  1.31041966, -0.78249806,\n",
       "         -0.65516931,  0.60700738,  0.14624425, -0.13358451,  0.57380724,\n",
       "         -0.59646707,  0.79443284, -0.7378085 ,  0.63473039, -0.29874506,\n",
       "         -0.25085278, -0.66588684, -0.21930529,  0.4352182 ,  1.42577097,\n",
       "         -0.97680672, -0.65189821,  0.43263676, -0.41009253, -0.14075834,\n",
       "          0.44028359, -0.21997084, -0.69132191, -0.65104293,  0.26431   ,\n",
       "         -0.14380325,  0.31806423, -0.94780872,  0.56711434,  0.09640809,\n",
       "         -0.19077913, -0.35186452,  0.03848772,  1.15162256, -0.56663659,\n",
       "         -0.52581897,  0.30809801,  0.30110416,  0.48985759,  0.56220497,\n",
       "          0.67229779,  0.78825405,  0.18637584, -0.3529604 ,  0.62049329,\n",
       "         -0.52690049, -0.5782697 , -0.61781432,  0.53791546, -0.22170356,\n",
       "          0.22880792, -0.13036673, -0.09740383,  0.46489468, -0.17222048,\n",
       "         -0.17015886, -0.19952705, -0.6073563 , -0.74319234,  0.36231423])),\n",
       " ('RemoteTrainer2',\n",
       "  array([ 0.57084281, -0.43625975,  0.35515647,  1.08213987,  0.49808186,\n",
       "         -0.01783353, -0.42346713, -0.59402771, -0.39321242, -0.08449408,\n",
       "          0.48540183,  0.26750529, -0.02993484,  0.74999626, -0.41705573,\n",
       "         -0.34587698, -0.28800377, -0.63684564, -0.04896376,  0.09125707,\n",
       "          1.46232359,  0.07580261,  0.23557272, -0.26932837, -0.17093243,\n",
       "          0.68173265,  0.2229231 , -0.85633614,  0.59484485,  0.24232714,\n",
       "          0.67193913,  0.03265138,  0.29441211, -0.5039321 , -0.65097852,\n",
       "         -0.49911994, -0.26296732, -0.76864416,  0.09514378,  0.01930245,\n",
       "         -0.61325328, -0.14761149, -0.82560894,  0.31761467, -0.16430064,\n",
       "          0.29489563,  0.07793725, -0.6494167 ,  0.54417701, -0.9031784 ,\n",
       "         -1.65657465, -0.35176882,  0.76857948,  1.03621894,  0.22186949,\n",
       "          0.49840229, -0.33343788,  1.0111239 , -0.11229171, -1.01113593,\n",
       "          0.2705324 ,  0.37665492, -0.1945035 , -0.3456354 , -0.13560775,\n",
       "          1.23041895, -0.31985235, -0.28683175,  0.43151571, -0.3913427 ,\n",
       "         -0.9149055 , -0.26509967, -0.06220936,  0.00630257, -0.13946507,\n",
       "          0.38255005,  0.6153809 ,  0.52465103,  0.9683908 , -0.66163483,\n",
       "          0.95651733,  0.2813992 , -0.3628656 , -0.60768893,  0.28281174,\n",
       "          0.06697133, -0.86173877, -1.20792014, -0.08015406, -0.50740412,\n",
       "         -0.25343659, -0.529521  , -0.06878202,  0.13907519, -0.40564773,\n",
       "         -0.01001491,  0.62686347,  0.35115673, -0.677903  , -1.01787332])),\n",
       " ('RemoteTrainer3',\n",
       "  array([ 1.61375581e+00, -9.00799019e-02, -9.18916960e-02,  7.86956339e-01,\n",
       "          6.57584062e-01, -3.94701465e-01,  3.83315206e-01,  7.57098130e-01,\n",
       "          1.92886122e-01, -2.92972883e-01,  3.10221126e-01, -8.64087463e-01,\n",
       "          2.55177194e-03,  8.93204287e-01,  2.92054234e-02,  2.24760165e-01,\n",
       "          9.67101659e-01, -7.35921264e-01, -6.63881115e-02, -2.06099514e-01,\n",
       "          7.57476080e-01,  1.11608310e+00,  2.06750409e-01, -3.99214040e-01,\n",
       "         -2.06775583e-01, -4.11250549e-01,  2.02686413e-01,  5.65673361e-02,\n",
       "         -4.89760723e-01,  4.76449204e-01, -1.90274289e-01,  5.18864872e-01,\n",
       "         -3.99074565e-01, -7.54701961e-02, -5.54153646e-01, -9.01230883e-01,\n",
       "          4.64767264e-01,  1.30665306e+00, -3.00215263e-01,  7.35181338e-01,\n",
       "         -2.80569081e-01,  1.50919350e-01, -5.18754552e-01,  3.29372991e-01,\n",
       "         -3.72787837e-01,  1.10649516e-01, -3.33683255e-01,  1.47026547e+00,\n",
       "          4.01484120e-01, -2.89592769e-01,  7.02420491e-01, -1.54652051e-01,\n",
       "         -5.80728964e-01,  3.50854318e-01, -1.36174011e-01,  7.12609366e-01,\n",
       "         -1.74640877e-01, -1.52802792e-01,  1.15513195e+00,  6.30445065e-01,\n",
       "          8.11773736e-01, -2.45386713e-02,  2.92566520e-01,  3.22461259e-01,\n",
       "         -9.28021878e-01,  5.62766750e-04, -1.40365680e-01,  2.86278848e-01,\n",
       "          3.01294452e-01,  6.17622254e-01, -4.91770970e-01,  6.25341933e-01,\n",
       "          3.28775772e-01,  1.48491927e-01,  6.17146421e-02,  4.78373940e-01,\n",
       "          3.55653796e-01, -1.16321483e+00,  7.58745390e-01, -1.74662733e+00,\n",
       "         -1.77360797e-01, -1.48886026e-01,  1.83150034e-01, -3.12313898e-01,\n",
       "         -1.00890583e-01,  5.95344748e-01, -9.31193586e-02,  7.80437292e-03,\n",
       "          2.67577230e-01,  2.64234656e-01, -5.91497771e-01, -5.85740712e-01,\n",
       "         -3.30609253e-01,  8.11892916e-01,  8.83435571e-01, -1.47622761e-01,\n",
       "          1.74318525e-01,  7.70486801e-01, -8.50325573e-01, -9.85711031e-01])),\n",
       " ('RemoteTrainer4',\n",
       "  array([ 0.24495186, -0.82222972, -0.49703894, -0.60906138,  0.36006728,\n",
       "         -0.1575377 ,  1.16401676,  0.00473425,  0.02248365,  0.21099617,\n",
       "          0.15097411,  0.13309453, -0.45851457, -0.05336868,  0.8501179 ,\n",
       "         -0.07698774,  0.26325887,  0.73910137,  0.93709146, -0.46928869,\n",
       "         -0.56386934,  0.08691664,  0.86698851,  0.871303  ,  0.03466154,\n",
       "          0.12258639,  0.69332861,  0.27711893, -0.26913055, -0.3539683 ,\n",
       "         -1.26442547, -0.35746049, -0.4319045 ,  0.13812988, -1.0398262 ,\n",
       "          1.19027831, -0.10893482,  0.30182681, -0.27425929, -0.15071851,\n",
       "         -0.30327579,  0.60391329, -0.71627474,  0.00376875, -0.52253963,\n",
       "         -0.25307575,  0.17919269,  0.5151377 ,  0.06942824, -0.84817501,\n",
       "          0.39953879,  0.64194227, -0.49580852,  1.04367738, -0.27768467,\n",
       "          0.00435593,  0.75946817,  0.29928501,  1.2831031 ,  0.36585593,\n",
       "         -0.10080258,  0.22156561,  0.59768489,  0.18624143, -0.94606618,\n",
       "          0.2586108 , -0.24102138,  0.16191818,  0.04764047,  0.35496388,\n",
       "          0.34732691, -0.62551788,  0.35782796, -0.07019356, -0.84490012,\n",
       "         -0.67307608,  0.01790709,  1.6176562 ,  0.23666641,  0.39735772,\n",
       "         -0.99421905, -0.33660282, -0.89598918,  0.74569009,  0.33619175,\n",
       "          0.66669168, -0.6520823 , -0.13010004, -1.32394551, -0.43969707,\n",
       "         -0.36047003, -0.55951752, -0.03671839, -0.04866885, -1.36270711,\n",
       "          1.02783136,  0.04151909,  0.74102103,  0.02258905,  0.40719632]))]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time ray.get([rt.train.remote(niterations) for rt in rts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RemoteTrainer3',\n",
       " 'RemoteTrainer2',\n",
       " 'RemoteTrainer1',\n",
       " 'RemoteTrainer4',\n",
       " 'RemoteTrainer0']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.get(rps.keys.remote())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the timing was about 1/5 for the remote execution. Recall that `ntrainers` is `5` at the top of this notebook. Hence, the results indicate that the training with the `LocalTrainers` happened synchronously, while the `RemoteTrainers` ran in parallel."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
