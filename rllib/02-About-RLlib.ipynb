{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About RLlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the [previous lesson](01-Introduction-to-Reinforcement-Learning.ipynb), we learned the basic concepts of reinforcement learning, with a \"taste\" of [RLlib](https://rllib.io) and [OpenAI Gym](https://gym.openai.com). This lesson takes a step back to provide more information about RLlib and the features it provides. The subsequent lessons will continue our exploration of RL algorithms and tools.\n",
    "\n",
    "For more information about RLlib and its open source community:\n",
    "\n",
    "* [documentation](https://ray.readthedocs.io/en/latest/rllib.html)\n",
    "* [GitHub repo](https://github.com/ray-project/ray/tree/master/rllib#rllib-scalable-reinforcement-learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RLlib is structured conceptually like this:\n",
    "\n",
    "![RLlib architecture](../images/RLlib-architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The applications we mentioned in the [Introduction](01-Introduction-to-Reinforcement-Learning.ipynb) are summarized on top. \n",
    "\n",
    "Next we decide our agent approach:\n",
    "\n",
    "* Just one agent? The traditional RL problem.\n",
    "* Multiple cooperating agents? TODO\n",
    "* A hierarchy of agents operating at different scopes?\n",
    "* Offline batch? This relatively new technique is useful for situations where an environment simulator doesn't exist and it's not possible to train in the real environment (e.g., a chemical plant), but what can we learn from the historical log data acrued from the real system?\n",
    "\n",
    "Under the approach, the user access RLlib through a consistent, concise API.\n",
    "\n",
    "The API provides a wide list of the popular RL algorithms.\n",
    "\n",
    "RLlib leverages Ray for efficient, cluster-wide scalability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: more conceptual information about RLlib, e.g., discuss...\n",
    "\n",
    "* the modules the user needs to understand\n",
    "* the integrations with TensorFlow, PyTorch, etc. \n",
    "\n",
    "In these lessons, we'll use RLlib with TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the current list of supported algorithms, each of which often specifies a particular system architecture. We'll explore many of these in various lessons:\n",
    "\n",
    "TODO: Provide summaries.\n",
    "\n",
    "### High-throughput Architectures\n",
    "\n",
    "* [Distributed Prioritized Experience Replay (Ape-X)](https://ray.readthedocs.io/en/latest/rllib-algorithms.html#distributed-prioritized-experience-replay-ape-x)\n",
    "* [Importance Weighted Actor-Learner Architecture (IMPALA)](https://ray.readthedocs.io/en/latest/rllib-algorithms.html#importance-weighted-actor-learner-architecture-impala)\n",
    "* [Asynchronous Proximal Policy Optimization (APPO)](https://ray.readthedocs.io/en/latest/rllib-algorithms.html#asynchronous-proximal-policy-optimization-appo)\n",
    "\n",
    "### Gradient-based\n",
    "\n",
    "* [Soft Actor-Critic (SAC)](https://ray.readthedocs.io/en/latest/rllib-algorithms.html#soft-actor-critic-sac)\n",
    "* [Advantage Actor-Critic (A2C, A3C)](https://ray.readthedocs.io/en/latest/rllib-algorithms.html#advantage-actor-critic-a2c-a3c)\n",
    "* [Deep Deterministic Policy Gradients (DDPG, TD3)](https://ray.readthedocs.io/en/latest/rllib-algorithms.html#deep-deterministic-policy-gradients-ddpg-td3)\n",
    "* [Deep Q Networks (DQN, Rainbow, Parametric DQN)](https://ray.readthedocs.io/en/latest/rllib-algorithms.html#deep-q-networks-dqn-rainbow-parametric-dqn)\n",
    "* [Policy Gradients](https://ray.readthedocs.io/en/latest/rllib-algorithms.html#policy-gradients)\n",
    "* [Proximal Policy Optimization (PPO)](https://docs.ray.io/en/latest/rllib-algorithms.html#proximal-policy-optimization-ppo)\n",
    "\n",
    "### Gradient-free\n",
    "\n",
    "* [Augmented Random Search (ARS)](https://ray.readthedocs.io/en/latest/rllib-algorithms.html#augmented-random-search-ars)\n",
    "* [Evolution Strategies](https://ray.readthedocs.io/en/latest/rllib-algorithms.html#evolution-strategies)\n",
    "\n",
    "### Multi-agent Specific\n",
    "\n",
    "* [QMIX Monotonic Value Factorisation (QMIX, VDN, IQN)](https://ray.readthedocs.io/en/latest/rllib-algorithms.html#qmix-monotonic-value-factorisation-qmix-vdn-iqn)\n",
    "\n",
    "### Offline\n",
    "\n",
    "* [Advantage Re-Weighted Imitation Learning (MARWIL)](https://ray.readthedocs.io/en/latest/rllib-algorithms.html#advantage-re-weighted-imitation-learning-marwil)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next lesson, [03: Application: Cart Pole](03-Application-Cart-Pole.ipynb) returns to the _cart pole_ example, where we train a moving car to balance a vertical pole. Based on the `CartPole-v0` environment from OpenAI Gym."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Copy of RLlib Tutorial",
   "provenance": []
  },
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
