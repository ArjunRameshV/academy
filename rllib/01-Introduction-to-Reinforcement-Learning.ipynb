{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Reinforcement Learning\n",
    "\n",
    "_Reinforcement Learning_ is the category of machine learning that focuses on training one or more _agents_ to achieve maximal _rewards_ while operating in an environment. This lesson discusses the core concepts of RL, while subsequent lessons explore RLlib in depth. We'll use two examples with exercises to give you a taste of RL. If you already understand RL concepts, you can either skim this lesson or skip to the [next lesson](02-About-RLlib.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RL is a deep topic and a focus of intense research. We can only scratch the surface here, so let's begin with some RL references for further information:\n",
    "\n",
    "## Books, Videos, etc.\n",
    "\n",
    "The RISE Lab and U.C. Berkeley has many useful tutorials, videos, etc.:\n",
    "\n",
    "* [RISE Lab YouTube channel](https://www.youtube.com/channel/UCP2-wiA964pif0secCpPbfw/videos)\n",
    "* [RISE Camp 2019](https://risecamp.berkeley.edu/)\n",
    "\n",
    "Several blog posts and series provide concise introductions to RL:\n",
    "\n",
    "* [Reinforcement Learning Explained](https://www.oreilly.com/radar/reinforcement-learning-explained/). A gentle introduction to the ideas of RL.\n",
    "* [A Beginner's Guide to Deep Reinforcement Learning](https://pathmind.com/wiki/deep-reinforcement-learning). From Pathmind, which uses RLlib for its products and services. Lots of good references at the end of this post.\n",
    "* [An Outsider's Tour of Reinforcement Learning](http://www.argmin.net/2018/06/25/outsider-rl/). A series of posts on technical aspects of RL.\n",
    "\n",
    "Several books are available on RL:\n",
    "\n",
    "* [*Practical Reinforcement Learning*](https://www.endtoend.ai/practical-rl/), by Seungjae Ryan Lee.\n",
    "* [*Hands-On Reinforcement Learning with Python*](https://learning.oreilly.com/library/view/hands-on-reinforcement-learning/9781788836524/), by Sudharsan Ravichandiran, Packt (2018-06-01)\n",
    "* [*Hands-On Reinforcement Learning for Games*](https://www.packtpub.com/game-development/hands-on-game-ai-with-python), by Micheal Lanham, Packt (2020-01-03)\n",
    "* [*Grokking Deep Reinforcement Learning*](https://www.manning.com/books/grokking-deep-reinforcement-learning), by Miguel Morales, Manning (Summer 2020 - previews available). Deep RL means using deep learning as part of the training system.\n",
    "* [*Reinforcement Learning: An Introduction*](http://incompleteideas.net/book/bookdraft2018jan1.pdf), by Richard S. Sutton and Andrew G. Barto, MIT Press (2018-01-01). This is the definitive textbook. Deep, but highly recommended. See this independent [repo of Python code](https://github.com/Pulkit-Khandelwal/Reinforcement-Learning-Notebooks).\n",
    "\n",
    "Other video tutorials and academic courses with materials available online:\n",
    "\n",
    "* [University College London COMPM050/COMPGI13](https://www.davidsilver.uk/teaching/)\n",
    "* [UC Berkeley CS 285](http://rail.eecs.berkeley.edu/deeprlcourse/)\n",
    "* [CS 294 Deep Reinforcement Learning, Spring 2017](http://rll.berkeley.edu/deeprlcourse/)\n",
    "* [A Tutorial on Reinforcement Learning I - YouTube](https://www.youtube.com/watch?v=fIKkhoI1kF4)\n",
    "* [A Tutorial on Reinforcement Learning II - YouTube](https://www.youtube.com/watch?v=8hK0NnG_DhY)\n",
    "* [ICML 2017 Tutorial](https://sites.google.com/view/icml17deeprl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Reinforcement Learning?\n",
    "\n",
    "> **GOAL:** The goal of the section is to introduce the basic concepts of RL, specifically the _Markov Decision Process_ abstraction, and to show its use in Python.\n",
    "\n",
    "Consider the following image:\n",
    "\n",
    "![RL Concepts](../images/RL-concepts.png)\n",
    "\n",
    "In RL, an **agent** interacts with an **environment** to maximize a **reward**. The agent makes **observations** about the **state** of the environment and takes **actions** that it believes will maximize the long-term reward. However, at the moment, it can only observe the immediate reward and it remembers past rewards. So, the training process usually involves lots and lot of reply of the game, the robot simulator traversing a virtual space, etc., so the agent can learn from repeated trials what decisions/actions work best to maximal the long-term reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RL has many applications, most famously these:\n",
    "\n",
    "![Alpha Go](../images/alpha-go.png)\n",
    "![Game](../images/game.png)\n",
    "![Robot Arm](../images/robot-arm.gif)\n",
    "![Walking Man](../images/walking-man.gif)\n",
    "![Autonomous Vehicle](../images/autonomous-vehicle.jpeg)\n",
    "![Two-legged Robot](../images/two-legged-robot.jpeg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More general industry applications that are emerging include the following:\n",
    "\n",
    "* **Process optimization:** industrial processes (factories, pipelines) and other business processes, routing problems, cluster optimization.\n",
    "* **Ad serving and recommendations:** Some of the traditional methods, including _collaborative filtering_ are hard to scale for very large data sets. Can RL train an agent to do an effective job more efficiently than traditional methods?\n",
    "* **Finance:** where markets are time-oriented _environments_ with automated trading systems are the _agents_. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hideCode": false,
    "hidePrompt": false,
    "id": "bAptEafKNlhm"
   },
   "source": [
    "### Markov Decision Processes\n",
    "\n",
    "Let's understand RL in more technical terms.\n",
    "\n",
    "> **The key abstraction in reinforcement learning is the Markov Decision Process (MDP).**\n",
    "\n",
    "An MDP models sequential interactions with an external environment. It consists of the following:\n",
    "\n",
    "- a **state space**\n",
    "- a set of **actions**\n",
    "- a **transition function** that describes the probability of being in a state $s'$ at time $t+1$ given that the MDP was in state $s$ at time $t$ and action $a$ was taken.\n",
    "- a **reward function**, which determines the reward received at time $t$.\n",
    "- a **discount factor** $\\gamma$, which is used when calculating the cumulative reward from the rewards received after each action is taken. It is used to \"discount\" earlier rewards vs. more recent rewards. The value is between 0 and 1.\n",
    "\n",
    "More details about MDP are available [here](https://en.wikipedia.org/wiki/Markov_decision_process). Note what we said in the third bullet, that the new state only depends on the previous state and the action taken. The assumption is that we can simplify our effort by ignoring all the previous states except the last one and still achieve good results. This is known as the [Markov property](https://en.wikipedia.org/wiki/Markov_property).\n",
    "\n",
    "**NOTE:** Reinforcement learning algorithms are often applied to problems that don't strictly fit into the MDP framework. In particular, situations in which the state of the environment is not fully observed lead to violations of the MDP assumption. Nevertheless, RL algorithms can be applied anyway."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hideCode": false,
    "hidePrompt": false,
    "id": "bAptEafKNlhm"
   },
   "source": [
    "### Policies\n",
    "\n",
    "A **policy** is a function that takes in a **state** and returns an **action**. A policy may be stochastic (i.e., it may sample from a probability distribution) or it can be deterministic.\n",
    "\n",
    "The **goal of reinforcement learning** is to learn a **policy** for maximizing the cumulative reward in an MDP. That is, we wish to find a policy $\\pi$ which solves the following optimization problem\n",
    "\n",
    "\\begin{equation}\n",
    "\\arg\\max_{\\pi} \\sum_{t=1}^T \\gamma^t R_t(\\pi),\n",
    "\\end{equation}\n",
    "\n",
    "where $T$ is the number of steps taken in the MDP (this is a random variable and may depend on $\\pi$) and $R_t$ is the reward received at time $t$ (also a random variable which depends on $\\pi$). Note the use of $\\gamma$, the discount factor.\n",
    "\n",
    "A number of algorithms are available for solving reinforcement learning problems. Several of the most widely known are [value iteration](https://en.wikipedia.org/wiki/Markov_decision_process#Value_iteration), [policy iteration](https://en.wikipedia.org/wiki/Markov_decision_process#Policy_iteration), and [Q learning](https://en.wikipedia.org/wiki/Q-learning), which we'll explore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hideCode": false,
    "hidePrompt": false,
    "id": "bAptEafKNlhm"
   },
   "source": [
    "### RL in Python\n",
    "\n",
    "The `gym` Python module provides MDP interfaces to a variety of simulators. For example, the CartPole environment interfaces with a simple simulator that simulates the physics of balancing a pole on a cart. The CartPole problem is described at https://gym.openai.com/envs/CartPole-v0. Here is an image from that website:\n",
    "\n",
    "![Cart Pole](../images/Cart-Pole.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hideCode": false,
    "hidePrompt": false,
    "id": "bAptEafKNlhm"
   },
   "source": [
    "This example fits into the MDP framework as follows.\n",
    "- The **state** consists of the position and velocity of the cart (moving in one dimension from left to right) as well as the angle and angular velocity of the pole that is balancing on the cart.\n",
    "- The **actions** are to decrease or increase the cart's velocity by one unit.\n",
    "- The **transition function** is deterministic and is determined by simulating physical laws.\n",
    "- The **reward function** is a constant 1 as long as the pole is upright, and 0 once the pole has fallen over. Therefore, maximizing the reward means balancing the pole for as long as possible.\n",
    "- The **discount factor** in this case can be taken to be 1.\n",
    "\n",
    "More information about the `gym` Python module is available at https://gym.openai.com/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a9Kwo5ZfNlhn"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WPpofaxQNlhp"
   },
   "source": [
    "The code below illustrates how to create and manipulate MDPs in Python. An MDP can be created by calling `gym.make`. Gym environments are identified by names like `CartPole-v0`. A **catalog of built-in environments** can be found at https://gym.openai.com/envs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "hideCode": false,
    "hidePrompt": false,
    "id": "6DZ68SG9Nlhp",
    "outputId": "293be60b-8107-42f2-c54a-58f3eaf295f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created env: <TimeLimit<CartPoleEnv<CartPole-v0>>>\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "print('Created env:', env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xn5PqgDzNlhr"
   },
   "source": [
    "Reset the state of the MDP by calling `env.reset()`. This call returns the initial state of the MDP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "hideCode": false,
    "hidePrompt": false,
    "id": "zRA58dOFNlhs",
    "outputId": "7aba4eac-fb0f-4654-eb49-0fbd6d664f28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The starting state is: [ 0.01418265 -0.02770248 -0.00307166 -0.0419574 ]\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "print('The starting state is:', state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the state is the position of the cart, its velocity, the angle of the pole, and the angular velocity of the pole."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8MuXXesWNlhu"
   },
   "source": [
    "The `env.step` method takes an action (in the case of the CartPole environment, the appropriate actions are 0 or 1, for moving left or right). It returns a tuple of four things:\n",
    "1. the new state of the environment\n",
    "2. a reward\n",
    "3. a boolean indicating whether the simulation has finished\n",
    "4. a dictionary of miscellaneous extra information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "hideCode": false,
    "hidePrompt": false,
    "id": "TufVaMz_Nlhu",
    "outputId": "920b9758-7d85-49e8-f8ef-4586b6947dea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.01510358 -0.38331264 -0.02109857  0.58458855] 1.0 False {}\n"
     ]
    }
   ],
   "source": [
    "# Simulate taking an action in the environment. Appropriate actions for\n",
    "# the CartPole environment are 0 and 1 (for moving left and right).\n",
    "action = 0\n",
    "state, reward, done, info = env.step(action)\n",
    "print(state, reward, done, info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uBIoIuWYNlhw"
   },
   "source": [
    "A **rollout** is a simulation of a policy in an environment. It alternates between choosing actions using some policy and taking those actions in the environment.\n",
    "\n",
    "The code below performs a rollout in a given environment. It takes **random actions** until the simulation has finished and returns the cumulative reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "Zp00mr88Nlhw",
    "outputId": "f0d01977-00c9-4ad2-931a-7f9730b5b005"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.0\n",
      "24.0\n"
     ]
    }
   ],
   "source": [
    "def random_rollout(env):\n",
    "    state = env.reset()\n",
    "    \n",
    "    done = False\n",
    "    cumulative_reward = 0\n",
    "\n",
    "    # Keep looping as long as the simulation has not finished.\n",
    "    while not done:\n",
    "        # Choose a random action (either 0 or 1).\n",
    "        action = np.random.choice([0, 1])\n",
    "        \n",
    "        # Take the action in the environment.\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        # Update the cumulative reward.\n",
    "        cumulative_reward += reward\n",
    "    \n",
    "    # Return the cumulative reward.\n",
    "    return cumulative_reward\n",
    "    \n",
    "reward = random_rollout(env)\n",
    "print(reward)\n",
    "reward = random_rollout(env)\n",
    "print(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i3FVvEJRNlhy"
   },
   "source": [
    "### Exercise 1\n",
    "\n",
    "Finish implementing the `rollout_policy` function below, which should take an environment *and* a policy. The *policy* is a function that takes in a *state* and returns an *action*. The main difference is that instead of choosing a **random action**, the action should be chosen **with the policy** (as a function of the state).\n",
    "\n",
    "> **Note:** Exercise solutions for this tutorial can be found [here](solutions/solutions.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 375
    },
    "colab_type": "code",
    "id": "9PgmkROqNlhy",
    "outputId": "91445278-aeb7-4e86-e1b7-c92e5ba830a2"
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-9b15a0a9af13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mreward1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrollout_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_policy1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mreward2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrollout_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_policy2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-9b15a0a9af13>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mreward1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrollout_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_policy1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mreward2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrollout_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_policy2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-9b15a0a9af13>\u001b[0m in \u001b[0;36mrollout_policy\u001b[0;34m(env, policy)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# EXERCISE: Fill out this function by copying the 'random_rollout' function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# and then modifying it to choose the action using the policy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Return the cumulative reward.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def rollout_policy(env, policy):\n",
    "    state = env.reset()\n",
    "    \n",
    "    done = False\n",
    "    cumulative_reward = 0\n",
    "\n",
    "    # EXERCISE: Fill out this function by copying the 'random_rollout' function\n",
    "    # and then modifying it to choose the action using the policy.\n",
    "    raise NotImplementedError\n",
    "\n",
    "    # Return the cumulative reward.\n",
    "    return cumulative_reward\n",
    "\n",
    "def sample_policy1(state):\n",
    "    return 0 if state[0] < 0 else 1\n",
    "\n",
    "def sample_policy2(state):\n",
    "    return 1 if state[0] < 0 else 0\n",
    "\n",
    "reward1 = np.mean([rollout_policy(env, sample_policy1) for _ in range(100)])\n",
    "reward2 = np.mean([rollout_policy(env, sample_policy2) for _ in range(100)])\n",
    "\n",
    "print('The first sample policy got an average reward of {}.'.format(reward1))\n",
    "print('The second sample policy got an average reward of {}.'.format(reward2))\n",
    "\n",
    "assert 5 < reward1 < 15, ('Make sure that rollout_policy computes the action '\n",
    "                          'by applying the policy to the state.')\n",
    "assert 25 < reward2 < 35, ('Make sure that rollout_policy computes the action '\n",
    "                           'by applying the policy to the state.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qXQ8hIB9Nlh0"
   },
   "source": [
    "## Proximal Policy Optimization\n",
    "\n",
    "> **GOAL:** The goal of this section is to demonstrate how to use the proximal policy optimization (PPO) algorithm, a popular way to develop a policy. \n",
    "\n",
    "We'll use **RLlib** this time with relatively little explanation for now, but explore it in greater depth in subsequent lessons. For more on RLlib, see the documentation at http://rllib.io."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qXQ8hIB9Nlh0"
   },
   "source": [
    "PPO is described in detail in https://arxiv.org/abs/1707.06347. It is a variant of Trust Region Policy Optimization (TRPO) described in https://arxiv.org/abs/1502.05477. [This OpenAI post](https://openai.com/blog/openai-baselines-ppo/) provides a more accessible introduction to PPO.\n",
    "\n",
    "PPO works in two phases. In one phase, a large number of rollouts are performed (in parallel). The rollouts are then aggregated on the driver and a surrogate optimization objective is defined based on those rollouts. We then use SGD (_stochastic gradient descent_) to find the policy that maximizes that objective with a penalty term for diverging too much from the current policy.\n",
    "\n",
    "![PPO](../images/ppo.png)\n",
    "\n",
    "([source](https://raw.githubusercontent.com/ucbrise/risecamp/risecamp2018/ray/tutorial/rllib_exercises/))\n",
    "\n",
    "> **NOTE:** The SGD optimization step is best performed in a data-parallel manner over multiple GPUs. This is exposed through the `num_gpus` field of the `config` dictionary. Hence, for this to work, you must be using a machine that has one or more GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XwPnR2ibNlh2"
   },
   "outputs": [],
   "source": [
    "# import gym  # imported above already, but listed here for completeness\n",
    "import ray\n",
    "from ray.rllib.agents.ppo import PPOTrainer, DEFAULT_CONFIG\n",
    "from ray.tune.logger import pretty_print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tQFzEX2BNlh3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-04 16:03:50,980\tINFO resource_spec.py:212 -- Starting Ray with 3.66 GiB memory available for workers and up to 1.85 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n",
      "2020-05-04 16:03:51,329\tINFO services.py:1148 -- View the Ray dashboard at \u001b[1m\u001b[32mlocalhost:8266\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '192.168.1.149',\n",
       " 'redis_address': '192.168.1.149:39687',\n",
       " 'object_store_address': '/tmp/ray/session_2020-05-04_16-03-50_968099_55589/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2020-05-04_16-03-50_968099_55589/sockets/raylet',\n",
       " 'webui_url': 'localhost:8266',\n",
       " 'session_dir': '/tmp/ray/session_2020-05-04_16-03-50_968099_55589'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start up Ray. This must be done before we instantiate any RL agents.\n",
    "ray.init(num_cpus=3, ignore_reinit_error=True, log_to_driver=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Ray Dashboard is useful for monitoring Ray:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dashboard URL: http://localhost:8266\n"
     ]
    }
   ],
   "source": [
    "print(f'Dashboard URL: http://{ray.get_webui_url()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f9yhpJZVNlh5"
   },
   "source": [
    "Instantiate a PPOTrainer object. We pass in a config object that specifies how the network and training procedure should be configured. Some of the parameters are the following.\n",
    "\n",
    "- `num_workers` is the number of actors that the agent will create. This determines the degree of parallelism that will be used.\n",
    "- `num_sgd_iter` is the number of epochs of SGD (passes through the data) that will be used to optimize the PPO surrogate objective at each iteration of PPO.\n",
    "- `sgd_minibatch_size` is the SGD batch size that will be used to optimize the PPO surrogate objective.\n",
    "- `model` contains a dictionary of parameters describing the neural net used to parameterize the policy. The `fcnet_hiddens` parameter is a list of the sizes of the hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ok210MCfNlh5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-04 16:04:39,321\tINFO trainer.py:428 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution\n",
      "2020-05-04 16:04:39,349\tINFO trainer.py:585 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "2020-05-04 16:04:41,272\tINFO trainable.py:217 -- Getting current IP.\n",
      "2020-05-04 16:04:41,273\tWARNING util.py:37 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "config = DEFAULT_CONFIG.copy()\n",
    "config['num_workers'] = 1\n",
    "config['num_sgd_iter'] = 30\n",
    "config['sgd_minibatch_size'] = 128\n",
    "config['model']['fcnet_hiddens'] = [100, 100]\n",
    "config['num_cpus_per_worker'] = 0  # This avoids running out of resources in the notebook environment when this cell is re-executed\n",
    "\n",
    "agent = PPOTrainer(config, 'CartPole-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ty1a6AWVNlh7"
   },
   "source": [
    "Train the policy on the `CartPole-v0` environment for 2 steps. The CartPole problem is described at https://gym.openai.com/envs/CartPole-v0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ty1a6AWVNlh7"
   },
   "source": [
    "### Exercise 2\n",
    "\n",
    "Inspect how well the policy is doing by looking for the lines that say something like the following:\n",
    "\n",
    "```\n",
    "episode_len_mean: 22.262569832402235\n",
    "episode_reward_mean: 22.262569832402235\n",
    "```\n",
    "\n",
    "This output indicates how much reward the policy is receiving and how many time steps of the environment the policy ran. The maximum possible reward for this problem is 200. The reward and trajectory length are very close because the agent receives a reward of one for every time step that it survives. However, this is specific to this environment.\n",
    "\n",
    "TODO: questions to answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3o0wjdZ3Nlh7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom_metrics: {}\n",
      "date: 2020-05-01_10-29-38\n",
      "done: false\n",
      "episode_len_mean: 22.844827586206897\n",
      "episode_reward_max: 90.0\n",
      "episode_reward_mean: 22.844827586206897\n",
      "episode_reward_min: 9.0\n",
      "episodes_this_iter: 174\n",
      "episodes_total: 174\n",
      "experiment_id: cfeec7c4aa004861b5d15ef47eb38fd7\n",
      "hostname: DWAnyscaleMBP.local\n",
      "info:\n",
      "  grad_time_ms: 1353.552\n",
      "  learner:\n",
      "    default_policy:\n",
      "      cur_kl_coeff: 0.20000000298023224\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6649100184440613\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.02841697819530964\n",
      "      model: {}\n",
      "      policy_loss: -0.03499114140868187\n",
      "      total_loss: 159.3486785888672\n",
      "      vf_explained_var: 0.03966745361685753\n",
      "      vf_loss: 159.37799072265625\n",
      "  load_time_ms: 49.631\n",
      "  num_steps_sampled: 4000\n",
      "  num_steps_trained: 3968\n",
      "  sample_time_ms: 2405.276\n",
      "  update_time_ms: 444.975\n",
      "iterations_since_restore: 1\n",
      "node_ip: 192.168.1.149\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 11.414583333333335\n",
      "  ram_util_percent: 65.43333333333334\n",
      "pid: 23456\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_env_wait_ms: 0.035216229702645145\n",
      "  mean_inference_ms: 0.44916481174429895\n",
      "  mean_processing_ms: 0.10136186227414706\n",
      "time_since_restore: 4.294661283493042\n",
      "time_this_iter_s: 4.294661283493042\n",
      "time_total_s: 4.294661283493042\n",
      "timestamp: 1588354178\n",
      "timesteps_since_restore: 4000\n",
      "timesteps_this_iter: 4000\n",
      "timesteps_total: 4000\n",
      "training_iteration: 1\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2020-05-01_10-29-41\n",
      "done: false\n",
      "episode_len_mean: 40.68\n",
      "episode_reward_max: 163.0\n",
      "episode_reward_mean: 40.68\n",
      "episode_reward_min: 10.0\n",
      "episodes_this_iter: 98\n",
      "episodes_total: 272\n",
      "experiment_id: cfeec7c4aa004861b5d15ef47eb38fd7\n",
      "hostname: DWAnyscaleMBP.local\n",
      "info:\n",
      "  grad_time_ms: 1233.189\n",
      "  learner:\n",
      "    default_policy:\n",
      "      cur_kl_coeff: 0.30000001192092896\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6222237944602966\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.015810487791895866\n",
      "      model: {}\n",
      "      policy_loss: -0.02175222709774971\n",
      "      total_loss: 355.9433898925781\n",
      "      vf_explained_var: 0.02135390415787697\n",
      "      vf_loss: 355.9604187011719\n",
      "  load_time_ms: 25.346\n",
      "  num_steps_sampled: 8000\n",
      "  num_steps_trained: 7936\n",
      "  sample_time_ms: 2363.266\n",
      "  update_time_ms: 223.541\n",
      "iterations_since_restore: 2\n",
      "node_ip: 192.168.1.149\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.96\n",
      "  ram_util_percent: 65.3\n",
      "pid: 23456\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_env_wait_ms: 0.035032157013781255\n",
      "  mean_inference_ms: 0.44576188625282365\n",
      "  mean_processing_ms: 0.09791528495190674\n",
      "time_since_restore: 7.7351062297821045\n",
      "time_this_iter_s: 3.4404449462890625\n",
      "time_total_s: 7.7351062297821045\n",
      "timestamp: 1588354181\n",
      "timesteps_since_restore: 8000\n",
      "timesteps_this_iter: 4000\n",
      "timesteps_total: 8000\n",
      "training_iteration: 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    result = agent.train()\n",
    "    print(pretty_print(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'episode_reward_max': 79.0,\n",
       " 'episode_reward_min': 9.0,\n",
       " 'episode_reward_mean': 22.46067415730337,\n",
       " 'episode_len_mean': 22.46067415730337,\n",
       " 'episodes_this_iter': 178,\n",
       " 'policy_reward_min': {},\n",
       " 'policy_reward_max': {},\n",
       " 'policy_reward_mean': {},\n",
       " 'custom_metrics': {},\n",
       " 'hist_stats': {'episode_reward': [27.0,\n",
       "   10.0,\n",
       "   28.0,\n",
       "   23.0,\n",
       "   12.0,\n",
       "   41.0,\n",
       "   21.0,\n",
       "   17.0,\n",
       "   17.0,\n",
       "   13.0,\n",
       "   14.0,\n",
       "   11.0,\n",
       "   20.0,\n",
       "   27.0,\n",
       "   40.0,\n",
       "   15.0,\n",
       "   21.0,\n",
       "   43.0,\n",
       "   11.0,\n",
       "   18.0,\n",
       "   14.0,\n",
       "   29.0,\n",
       "   14.0,\n",
       "   38.0,\n",
       "   13.0,\n",
       "   13.0,\n",
       "   15.0,\n",
       "   39.0,\n",
       "   12.0,\n",
       "   30.0,\n",
       "   34.0,\n",
       "   12.0,\n",
       "   21.0,\n",
       "   29.0,\n",
       "   18.0,\n",
       "   18.0,\n",
       "   32.0,\n",
       "   12.0,\n",
       "   14.0,\n",
       "   11.0,\n",
       "   20.0,\n",
       "   30.0,\n",
       "   28.0,\n",
       "   9.0,\n",
       "   18.0,\n",
       "   35.0,\n",
       "   10.0,\n",
       "   38.0,\n",
       "   13.0,\n",
       "   22.0,\n",
       "   20.0,\n",
       "   13.0,\n",
       "   30.0,\n",
       "   13.0,\n",
       "   18.0,\n",
       "   14.0,\n",
       "   13.0,\n",
       "   14.0,\n",
       "   14.0,\n",
       "   16.0,\n",
       "   16.0,\n",
       "   30.0,\n",
       "   13.0,\n",
       "   19.0,\n",
       "   26.0,\n",
       "   14.0,\n",
       "   18.0,\n",
       "   15.0,\n",
       "   19.0,\n",
       "   11.0,\n",
       "   58.0,\n",
       "   11.0,\n",
       "   44.0,\n",
       "   12.0,\n",
       "   19.0,\n",
       "   14.0,\n",
       "   12.0,\n",
       "   36.0,\n",
       "   25.0,\n",
       "   16.0,\n",
       "   30.0,\n",
       "   19.0,\n",
       "   14.0,\n",
       "   19.0,\n",
       "   45.0,\n",
       "   16.0,\n",
       "   25.0,\n",
       "   46.0,\n",
       "   21.0,\n",
       "   14.0,\n",
       "   23.0,\n",
       "   21.0,\n",
       "   14.0,\n",
       "   45.0,\n",
       "   46.0,\n",
       "   14.0,\n",
       "   20.0,\n",
       "   18.0,\n",
       "   11.0,\n",
       "   34.0,\n",
       "   43.0,\n",
       "   14.0,\n",
       "   19.0,\n",
       "   22.0,\n",
       "   27.0,\n",
       "   62.0,\n",
       "   33.0,\n",
       "   16.0,\n",
       "   12.0,\n",
       "   14.0,\n",
       "   26.0,\n",
       "   20.0,\n",
       "   29.0,\n",
       "   28.0,\n",
       "   15.0,\n",
       "   11.0,\n",
       "   16.0,\n",
       "   14.0,\n",
       "   12.0,\n",
       "   26.0,\n",
       "   21.0,\n",
       "   12.0,\n",
       "   46.0,\n",
       "   18.0,\n",
       "   23.0,\n",
       "   14.0,\n",
       "   28.0,\n",
       "   22.0,\n",
       "   14.0,\n",
       "   24.0,\n",
       "   14.0,\n",
       "   11.0,\n",
       "   21.0,\n",
       "   33.0,\n",
       "   12.0,\n",
       "   38.0,\n",
       "   21.0,\n",
       "   11.0,\n",
       "   19.0,\n",
       "   18.0,\n",
       "   16.0,\n",
       "   11.0,\n",
       "   15.0,\n",
       "   79.0,\n",
       "   69.0,\n",
       "   49.0,\n",
       "   20.0,\n",
       "   13.0,\n",
       "   33.0,\n",
       "   30.0,\n",
       "   20.0,\n",
       "   19.0,\n",
       "   40.0,\n",
       "   31.0,\n",
       "   23.0,\n",
       "   23.0,\n",
       "   22.0,\n",
       "   11.0,\n",
       "   26.0,\n",
       "   24.0,\n",
       "   15.0,\n",
       "   14.0,\n",
       "   15.0,\n",
       "   13.0,\n",
       "   21.0,\n",
       "   21.0,\n",
       "   23.0,\n",
       "   16.0,\n",
       "   20.0,\n",
       "   33.0,\n",
       "   37.0,\n",
       "   15.0,\n",
       "   27.0,\n",
       "   25.0,\n",
       "   19.0,\n",
       "   18.0,\n",
       "   21.0,\n",
       "   34.0],\n",
       "  'episode_lengths': [27,\n",
       "   10,\n",
       "   28,\n",
       "   23,\n",
       "   12,\n",
       "   41,\n",
       "   21,\n",
       "   17,\n",
       "   17,\n",
       "   13,\n",
       "   14,\n",
       "   11,\n",
       "   20,\n",
       "   27,\n",
       "   40,\n",
       "   15,\n",
       "   21,\n",
       "   43,\n",
       "   11,\n",
       "   18,\n",
       "   14,\n",
       "   29,\n",
       "   14,\n",
       "   38,\n",
       "   13,\n",
       "   13,\n",
       "   15,\n",
       "   39,\n",
       "   12,\n",
       "   30,\n",
       "   34,\n",
       "   12,\n",
       "   21,\n",
       "   29,\n",
       "   18,\n",
       "   18,\n",
       "   32,\n",
       "   12,\n",
       "   14,\n",
       "   11,\n",
       "   20,\n",
       "   30,\n",
       "   28,\n",
       "   9,\n",
       "   18,\n",
       "   35,\n",
       "   10,\n",
       "   38,\n",
       "   13,\n",
       "   22,\n",
       "   20,\n",
       "   13,\n",
       "   30,\n",
       "   13,\n",
       "   18,\n",
       "   14,\n",
       "   13,\n",
       "   14,\n",
       "   14,\n",
       "   16,\n",
       "   16,\n",
       "   30,\n",
       "   13,\n",
       "   19,\n",
       "   26,\n",
       "   14,\n",
       "   18,\n",
       "   15,\n",
       "   19,\n",
       "   11,\n",
       "   58,\n",
       "   11,\n",
       "   44,\n",
       "   12,\n",
       "   19,\n",
       "   14,\n",
       "   12,\n",
       "   36,\n",
       "   25,\n",
       "   16,\n",
       "   30,\n",
       "   19,\n",
       "   14,\n",
       "   19,\n",
       "   45,\n",
       "   16,\n",
       "   25,\n",
       "   46,\n",
       "   21,\n",
       "   14,\n",
       "   23,\n",
       "   21,\n",
       "   14,\n",
       "   45,\n",
       "   46,\n",
       "   14,\n",
       "   20,\n",
       "   18,\n",
       "   11,\n",
       "   34,\n",
       "   43,\n",
       "   14,\n",
       "   19,\n",
       "   22,\n",
       "   27,\n",
       "   62,\n",
       "   33,\n",
       "   16,\n",
       "   12,\n",
       "   14,\n",
       "   26,\n",
       "   20,\n",
       "   29,\n",
       "   28,\n",
       "   15,\n",
       "   11,\n",
       "   16,\n",
       "   14,\n",
       "   12,\n",
       "   26,\n",
       "   21,\n",
       "   12,\n",
       "   46,\n",
       "   18,\n",
       "   23,\n",
       "   14,\n",
       "   28,\n",
       "   22,\n",
       "   14,\n",
       "   24,\n",
       "   14,\n",
       "   11,\n",
       "   21,\n",
       "   33,\n",
       "   12,\n",
       "   38,\n",
       "   21,\n",
       "   11,\n",
       "   19,\n",
       "   18,\n",
       "   16,\n",
       "   11,\n",
       "   15,\n",
       "   79,\n",
       "   69,\n",
       "   49,\n",
       "   20,\n",
       "   13,\n",
       "   33,\n",
       "   30,\n",
       "   20,\n",
       "   19,\n",
       "   40,\n",
       "   31,\n",
       "   23,\n",
       "   23,\n",
       "   22,\n",
       "   11,\n",
       "   26,\n",
       "   24,\n",
       "   15,\n",
       "   14,\n",
       "   15,\n",
       "   13,\n",
       "   21,\n",
       "   21,\n",
       "   23,\n",
       "   16,\n",
       "   20,\n",
       "   33,\n",
       "   37,\n",
       "   15,\n",
       "   27,\n",
       "   25,\n",
       "   19,\n",
       "   18,\n",
       "   21,\n",
       "   34]},\n",
       " 'sampler_perf': {'mean_env_wait_ms': 0.0369576804788671,\n",
       "  'mean_processing_ms': 0.10702545063044541,\n",
       "  'mean_inference_ms': 0.4561423540294126},\n",
       " 'off_policy_estimator': {},\n",
       " 'info': {'num_steps_trained': 3968,\n",
       "  'num_steps_sampled': 4000,\n",
       "  'sample_time_ms': 2463.111,\n",
       "  'load_time_ms': 50.743,\n",
       "  'grad_time_ms': 1403.649,\n",
       "  'update_time_ms': 445.991,\n",
       "  'learner': {'default_policy': {'cur_kl_coeff': 0.20000000298023224,\n",
       "    'cur_lr': 4.999999873689376e-05,\n",
       "    'total_loss': 172.1385,\n",
       "    'policy_loss': -0.03654765,\n",
       "    'vf_loss': 172.16942,\n",
       "    'vf_explained_var': 0.016607448,\n",
       "    'kl': 0.028212082,\n",
       "    'entropy': 0.6657186,\n",
       "    'entropy_coeff': 0.0,\n",
       "    'model': {}}}},\n",
       " 'timesteps_this_iter': 4000,\n",
       " 'done': False,\n",
       " 'timesteps_total': 4000,\n",
       " 'episodes_total': 178,\n",
       " 'training_iteration': 1,\n",
       " 'experiment_id': 'be7016fcfa3945b3848d57a45ee12056',\n",
       " 'date': '2020-05-04_16-05-11',\n",
       " 'timestamp': 1588633511,\n",
       " 'time_this_iter_s': 4.405592918395996,\n",
       " 'time_total_s': 4.405592918395996,\n",
       " 'pid': 55589,\n",
       " 'hostname': 'DWAnyscaleMBP.local',\n",
       " 'node_ip': '192.168.1.149',\n",
       " 'config': {'num_workers': 1,\n",
       "  'num_envs_per_worker': 1,\n",
       "  'rollout_fragment_length': 200,\n",
       "  'sample_batch_size': -1,\n",
       "  'batch_mode': 'truncate_episodes',\n",
       "  'num_gpus': 0,\n",
       "  'train_batch_size': 4000,\n",
       "  'model': {'conv_filters': None,\n",
       "   'conv_activation': 'relu',\n",
       "   'fcnet_activation': 'tanh',\n",
       "   'fcnet_hiddens': [100, 100],\n",
       "   'free_log_std': False,\n",
       "   'no_final_linear': False,\n",
       "   'vf_share_layers': True,\n",
       "   'use_lstm': False,\n",
       "   'max_seq_len': 20,\n",
       "   'lstm_cell_size': 256,\n",
       "   'lstm_use_prev_action_reward': False,\n",
       "   'state_shape': None,\n",
       "   'framestack': True,\n",
       "   'dim': 84,\n",
       "   'grayscale': False,\n",
       "   'zero_mean': True,\n",
       "   'custom_model': None,\n",
       "   'custom_action_dist': None,\n",
       "   'custom_options': {},\n",
       "   'custom_preprocessor': None},\n",
       "  'optimizer': {},\n",
       "  'gamma': 0.99,\n",
       "  'horizon': None,\n",
       "  'soft_horizon': False,\n",
       "  'no_done_at_end': False,\n",
       "  'env_config': {},\n",
       "  'env': 'CartPole-v0',\n",
       "  'normalize_actions': False,\n",
       "  'clip_rewards': None,\n",
       "  'clip_actions': True,\n",
       "  'preprocessor_pref': 'deepmind',\n",
       "  'lr': 5e-05,\n",
       "  'monitor': False,\n",
       "  'log_level': 'WARN',\n",
       "  'callbacks': {'on_episode_start': None,\n",
       "   'on_episode_step': None,\n",
       "   'on_episode_end': None,\n",
       "   'on_sample_end': None,\n",
       "   'on_train_result': None,\n",
       "   'on_postprocess_traj': None},\n",
       "  'ignore_worker_failures': False,\n",
       "  'log_sys_usage': True,\n",
       "  'use_pytorch': False,\n",
       "  'eager': False,\n",
       "  'eager_tracing': False,\n",
       "  'no_eager_on_workers': False,\n",
       "  'explore': True,\n",
       "  'exploration_config': {'type': 'StochasticSampling'},\n",
       "  'evaluation_interval': None,\n",
       "  'evaluation_num_episodes': 10,\n",
       "  'in_evaluation': False,\n",
       "  'evaluation_config': {},\n",
       "  'evaluation_num_workers': 0,\n",
       "  'custom_eval_function': None,\n",
       "  'use_exec_api': False,\n",
       "  'sample_async': False,\n",
       "  'observation_filter': 'NoFilter',\n",
       "  'synchronize_filters': True,\n",
       "  'tf_session_args': {'intra_op_parallelism_threads': 2,\n",
       "   'inter_op_parallelism_threads': 2,\n",
       "   'gpu_options': {'allow_growth': True},\n",
       "   'log_device_placement': False,\n",
       "   'device_count': {'CPU': 1},\n",
       "   'allow_soft_placement': True},\n",
       "  'local_tf_session_args': {'intra_op_parallelism_threads': 8,\n",
       "   'inter_op_parallelism_threads': 8},\n",
       "  'compress_observations': False,\n",
       "  'collect_metrics_timeout': 180,\n",
       "  'metrics_smoothing_episodes': 100,\n",
       "  'remote_worker_envs': False,\n",
       "  'remote_env_batch_wait_ms': 0,\n",
       "  'min_iter_time_s': 0,\n",
       "  'timesteps_per_iteration': 0,\n",
       "  'seed': None,\n",
       "  'num_cpus_per_worker': 0,\n",
       "  'num_gpus_per_worker': 0,\n",
       "  'custom_resources_per_worker': {},\n",
       "  'num_cpus_for_driver': 1,\n",
       "  'memory': 0,\n",
       "  'object_store_memory': 0,\n",
       "  'memory_per_worker': 0,\n",
       "  'object_store_memory_per_worker': 0,\n",
       "  'input': 'sampler',\n",
       "  'input_evaluation': ['is', 'wis'],\n",
       "  'postprocess_inputs': False,\n",
       "  'shuffle_buffer_size': 0,\n",
       "  'output': None,\n",
       "  'output_compress_columns': ['obs', 'new_obs'],\n",
       "  'output_max_file_size': 67108864,\n",
       "  'multiagent': {'policies': {},\n",
       "   'policy_mapping_fn': None,\n",
       "   'policies_to_train': None},\n",
       "  'use_critic': True,\n",
       "  'use_gae': True,\n",
       "  'lambda': 1.0,\n",
       "  'kl_coeff': 0.2,\n",
       "  'sgd_minibatch_size': 128,\n",
       "  'shuffle_sequences': True,\n",
       "  'num_sgd_iter': 30,\n",
       "  'lr_schedule': None,\n",
       "  'vf_share_layers': False,\n",
       "  'vf_loss_coeff': 1.0,\n",
       "  'entropy_coeff': 0.0,\n",
       "  'entropy_coeff_schedule': None,\n",
       "  'clip_param': 0.3,\n",
       "  'vf_clip_param': 10.0,\n",
       "  'grad_clip': None,\n",
       "  'kl_target': 0.01,\n",
       "  'simple_optimizer': False},\n",
       " 'time_since_restore': 4.405592918395996,\n",
       " 'timesteps_since_restore': 4000,\n",
       " 'iterations_since_restore': 1,\n",
       " 'perf': {'cpu_util_percent': 18.939534883720928,\n",
       "  'ram_util_percent': 70.25581395348838},\n",
       " 'num_healthy_workers': 1}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = agent.train()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FPdkWLrENlh9"
   },
   "source": [
    "### Exercise 3\n",
    "\n",
    "The current network and training configuration are too large and heavy-duty for a simple problem like CartPole. Modify the configuration to use a smaller network and to speed up the optimization of the surrogate objective. (Fewer SGD iterations and a larger batch size should help.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3lp6tqkNNlh9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-01 14:15:04,808\tINFO trainable.py:217 -- Getting current IP.\n",
      "2020-05-01 14:15:04,811\tWARNING util.py:37 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "config = DEFAULT_CONFIG.copy()\n",
    "config['num_workers'] = 3\n",
    "config['num_sgd_iter'] = 30\n",
    "config['sgd_minibatch_size'] = 128\n",
    "config['model']['fcnet_hiddens'] = [100, 100]\n",
    "config['num_cpus_per_worker'] = 0\n",
    "\n",
    "agent = PPOTrainer(config, 'CartPole-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "64FmVP7kNlh_"
   },
   "source": [
    "Train the agent and try to get a reward of 200. If it's training too slowly you may need to modify the config above to use fewer hidden units, a larger `sgd_minibatch_size`, a smaller `num_sgd_iter`, or a larger `num_workers`.\n",
    "\n",
    "This should take around 20 or 30 training iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XB7sdKUzNliA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom_metrics: {}\n",
      "date: 2020-05-01_14-15-14\n",
      "done: false\n",
      "episode_len_mean: 23.204678362573098\n",
      "episode_reward_max: 73.0\n",
      "episode_reward_mean: 23.204678362573098\n",
      "episode_reward_min: 9.0\n",
      "episodes_this_iter: 171\n",
      "episodes_total: 171\n",
      "experiment_id: b87ee6a868904eb3b2339b413a6b33e6\n",
      "hostname: DWAnyscaleMBP.local\n",
      "info:\n",
      "  grad_time_ms: 2558.961\n",
      "  learner:\n",
      "    default_policy:\n",
      "      cur_kl_coeff: 0.20000000298023224\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.663716733455658\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.029719049111008644\n",
      "      model: {}\n",
      "      policy_loss: -0.04187756031751633\n",
      "      total_loss: 198.08648681640625\n",
      "      vf_explained_var: 0.008818145841360092\n",
      "      vf_loss: 198.1223907470703\n",
      "  load_time_ms: 93.352\n",
      "  num_steps_sampled: 4000\n",
      "  num_steps_trained: 3968\n",
      "  sample_time_ms: 5255.472\n",
      "  update_time_ms: 1343.73\n",
      "iterations_since_restore: 1\n",
      "node_ip: 192.168.1.149\n",
      "num_healthy_workers: 3\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 64.64999999999999\n",
      "  ram_util_percent: 64.35714285714285\n",
      "pid: 30398\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_env_wait_ms: 0.09012538013664789\n",
      "  mean_inference_ms: 1.1841216948297306\n",
      "  mean_processing_ms: 0.2556961380168783\n",
      "time_since_restore: 9.307942152023315\n",
      "time_this_iter_s: 9.307942152023315\n",
      "time_total_s: 9.307942152023315\n",
      "timestamp: 1588367714\n",
      "timesteps_since_restore: 4000\n",
      "timesteps_this_iter: 4000\n",
      "timesteps_total: 4000\n",
      "training_iteration: 1\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2020-05-01_14-15-17\n",
      "done: false\n",
      "episode_len_mean: 38.56310679611651\n",
      "episode_reward_max: 185.0\n",
      "episode_reward_mean: 38.56310679611651\n",
      "episode_reward_min: 12.0\n",
      "episodes_this_iter: 103\n",
      "episodes_total: 274\n",
      "experiment_id: b87ee6a868904eb3b2339b413a6b33e6\n",
      "hostname: DWAnyscaleMBP.local\n",
      "info:\n",
      "  grad_time_ms: 2260.384\n",
      "  learner:\n",
      "    default_policy:\n",
      "      cur_kl_coeff: 0.30000001192092896\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6134884357452393\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.01848619617521763\n",
      "      model: {}\n",
      "      policy_loss: -0.030175646767020226\n",
      "      total_loss: 277.9844055175781\n",
      "      vf_explained_var: 0.03403020277619362\n",
      "      vf_loss: 278.009033203125\n",
      "  load_time_ms: 47.335\n",
      "  num_steps_sampled: 8000\n",
      "  num_steps_trained: 7936\n",
      "  sample_time_ms: 3404.191\n",
      "  update_time_ms: 674.802\n",
      "iterations_since_restore: 2\n",
      "node_ip: 192.168.1.149\n",
      "num_healthy_workers: 3\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 67.64000000000001\n",
      "  ram_util_percent: 64.4\n",
      "pid: 30398\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_env_wait_ms: 0.07872465384934171\n",
      "  mean_inference_ms: 1.0187536786867875\n",
      "  mean_processing_ms: 0.21631307881795864\n",
      "time_since_restore: 12.834509372711182\n",
      "time_this_iter_s: 3.526567220687866\n",
      "time_total_s: 12.834509372711182\n",
      "timestamp: 1588367717\n",
      "timesteps_since_restore: 8000\n",
      "timesteps_this_iter: 4000\n",
      "timesteps_total: 8000\n",
      "training_iteration: 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    result = agent.train()\n",
    "    print(pretty_print(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PW6bN9CYNliB"
   },
   "source": [
    "Checkpoint the current model. The call to `agent.save()` returns the path to the checkpointed model and can be used later to restore the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6uf808LMNliC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/deanwampler/ray_results/PPO_CartPole-v0_2020-05-01_14-15-00c1v_vfaw/checkpoint_2/checkpoint-2\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = agent.save()\n",
    "print(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "05icI8bfNliD"
   },
   "source": [
    "Now let's use the trained policy to make predictions.\n",
    "\n",
    "> **Note:** Here we are loading the trained policy in the same process, but in practice, this would often be done in a different process (probably on a different machine)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8Qq2_AYVNliE"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-01 14:15:21,909\tINFO trainable.py:217 -- Getting current IP.\n",
      "2020-05-01 14:15:21,912\tWARNING util.py:37 -- Install gputil for GPU system monitoring.\n",
      "2020-05-01 14:15:22,010\tINFO trainable.py:217 -- Getting current IP.\n",
      "2020-05-01 14:15:22,012\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: /Users/deanwampler/ray_results/PPO_CartPole-v0_2020-05-01_14-15-00c1v_vfaw/checkpoint_2/checkpoint-2\n",
      "2020-05-01 14:15:22,013\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 2, '_timesteps_total': 8000, '_time_total': 12.834509372711182, '_episodes_total': 274}\n"
     ]
    }
   ],
   "source": [
    "trained_config = config.copy()\n",
    "\n",
    "test_agent = PPOTrainer(trained_config, 'CartPole-v0')\n",
    "test_agent.restore(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c2gUUlqkNliG"
   },
   "source": [
    "Now use the trained policy to act in an environment. The key line is the call to `test_agent.compute_action(state)` which uses the trained policy to choose an action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c2gUUlqkNliG"
   },
   "source": [
    "### Exercise 4\n",
    "\n",
    "Verify that the reward received roughly matches up with the reward printed in the training logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9asL5Z5lNliH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "state = env.reset()\n",
    "done = False\n",
    "cumulative_reward = 0\n",
    "\n",
    "while not done:\n",
    "    action = test_agent.compute_action(state)\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    cumulative_reward += reward\n",
    "\n",
    "print(cumulative_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next lesson, [02: About RLlib](02-About-RLlib.ipynb) steps back to introduce to RLlib, its goals and the capabilities it provides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Copy of RLlib Tutorial",
   "provenance": []
  },
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
